---------------------------------------------------------------------------
                          Pengelaan Deep Learning
---------------------------------------------------------------------------

Neural Network adalah fondasi dari deep learning. Bagian ini adalah sebuah model matematis yang terinspirasi dari cara kerja jaringan saraf manusia, di mana komputer dapat belajar mengenali pola kompleks dari data.

NN adalah representasi sederhana dari neural network. Di dalamnya, ada neuron-neuron buatan yang terhubung dalam lapisan-lapisan. Setiap neuron menerima input, melakukan perhitungan matematis, dan menghasilkan output. Dengan banyaknya neuron dan lapisan yang saling terhubung, neural network dapat mempelajari pola-pola yang rumit dari data.

Taksonomi AI:
1. Artificial Intelligence (AI)
Artificial intelligence adalah konsep yang mendasari seluruh bidang kecerdasan buatan. Pada tingkat paling dasar, AI mencakup penggunaan komputer atau mesin untuk melakukan tugas yang membutuhkan kecerdasan manusia, seperti pengambilan keputusan, pengenalan pola, dan pemecahan masalah.

2. Machine Learning (ML)
Machine learning adalah cabang dari AI, ketika komputer dapat belajar dari data tanpa perlu diprogram secara eksplisit. Teknik-teknik ML memungkinkan komputer untuk mengenali pola dalam data, membuat prediksi, dan mengambil keputusan berdasarkan informasi yang dipelajari dari pengalaman atau data latihan.

3. Neural Network (NN)
Neural network adalah model matematis yang terinspirasi dari struktur jaringan saraf manusia. Dalam konteks ML, NN digunakan untuk memproses informasi dan belajar dari data. Model ini terdiri atas neuron-neuron buatan yang saling terhubung dalam lapisan-lapisan dan mampu mempelajari representasi yang semakin abstrak dari data.

4. Deep Learning (DL)
Deep learning adalah sub-bidang dari ML yang menggunakan NN dengan banyak lapisan atau deep neural network (DNN) untuk memahami representasi data yang abstrak dan kompleks. DL telah menghasilkan kemajuan besar dalam bidang pengenalan gambar, pemrosesan bahasa alami, dan berbagai aplikasi AI lainnya.

5. Gen AI
Generative AI adalah cabang dari AI yang berfokus pada penciptaan konten baru dan original. Berbeda dengan AI tradisional yang biasanya beroperasi berdasarkan aturan yang telah ditetapkan dan data yang ada, generative AI memiliki kemampuan untuk menghasilkan teks, gambar, musik, video, dan bentuk konten lainnya yang belum pernah ada sebelumnya.



Konsep Dasar Neural Network:
Neural network (NN) atau jaringan saraf tiruan adalah model matematis yang terinspirasi dari struktur jaringan saraf manusia. NN dirancang untuk meniru cara otak manusia bekerja dengan menggunakan unit-unit pemrosesan sederhana yang disebut neuron. Jaringan saraf tiruan terdiri dari beberapa lapisan neuron yang saling terhubung. Informasi yang dibawa akan mengalir melalui jaringan ini untuk melakukan tugas-tugas tertentu, seperti pengenalan pola, klasifikasi, atau prediksi berdasarkan data yang diberikan.


Saraf Biologis vs Saraf Tiruan:
Sebelum kita belajar mengenai saraf tiruan, kita akan mengenal lebih dahulu saraf biologis (neuron). National Institute of Neurological Disorders and Stroke dalam tulisannya yang berjudul “Brain Basics: The Life and Death of a Neuron” menyatakan bahwa neuron atau saraf adalah pembawa pesan atau informasi. Mereka menggunakan impuls listrik dan sinyal kimiawi untuk mengirimkan informasi di antara area otak yang berbeda, serta antara otak dan seluruh sistem saraf.

Sebuah saraf terdiri dari tiga bagian utama, yaitu akson, dendrit, dan badan sel yang di dalamnya terdapat nukleus. Nukleus berisi materi genetik dan bertugas mengontrol seluruh aktivitas sel. Akson adalah cabang yang terlihat seperti ekor panjang dan bertugas mengirimkan pesan dari sel. Panjang akson berkisar antara beberapa kali lebih panjang dari badan sel, sampai 10 ribu kali. Berikutnya, dendrit adalah cabang-cabang pendek yang terlihat seperti cabang pohon dan bertugas menerima pesan untuk sel. 

Setiap ujung akson dari sebuah neuron terhubung dengan dendrit dari neuron lainnya. Neuron berkomunikasi satu sama lain dengan mengirimkan senyawa kimia yang disebut neurotransmitter, melintasi ruang kecil (synapse) antara akson dan dendrit neuron yang berdekatan. 

Tahukah Anda? Konsep kerja dan struktur NN ternyata terinspirasi dari saraf biologis kita, lo!  Pada NN, ada unit-unit pemrosesan disebut neuron buatan dan saling terhubung dalam struktur yang mirip dengan jaringan saraf biologis. Setiap neuron buatan menerima sejumlah input, menghitung hasil berdasarkan bobot yang ditetapkan, dan mengirimkan output ke neuron-neuron lainnya.  

Pengaturan bobot ini adalah kunci dalam NN. Selama proses pelatihan, NN belajar untuk mengubah bobot-bobot ini berdasarkan data latihan yang diberikan. Jadi, NN mampu mengenali pola-pola kompleks dan membuat prediksi atau keputusan berdasarkan informasi yang diterima. 



Struktur Artificial Neural Network:
Neural network yang paling sederhana atau minimal terdiri dari satu unit perceptron tunggal, juga dikenal sebagai single perceptron. Perceptron ini memproses input dengan mengalikan nilai input (x) dengan bobot (w), kemudian menjumlahkannya dengan bias (b). Hasil dari operasi ini disebut net input (z) dan kemudian akan diproses melalui fungsi aktivasi (f) untuk menghasilkan output (y) dari perceptron tersebut. 

Rumus Single Perceptron:

     m
Z = EEE   Wi Xi + b
    i = 1

- x  : x1, x2, …, xn; xi adalah nilai input ke-i.
- w : w1, w2, …, wn; wi adalah bobot terkait dengan input xi.
- b : nilai bias (konstanta tambahan) yang dimiliki oleh perceptron.
- z : fungsi linear.
- f : fungsi aktivasi untuk menghasilkan output y.
- y : output.

Jadi, secara matematis, single perceptron dapat dijelaskan sebagai sebuah fungsi matematis yang mengambil input, mengalikannya dengan bobot, menambahkan bias, dan kemudian menerapkan fungsi aktivasi pada hasilnya untuk menghasilkan output. Prosedur ini adalah dasar dari komputasi dalam NN yang lebih kompleks.


Cara Kerja Artificial Neural Network:
Cara kerja artificial neural network (ANN) mengambil inspirasi dari struktur jaringan saraf manusia. Pada sebuah ANN, ada jutaan atau bahkan miliaran "neuron" buatan yang terorganisir dalam lapisan-lapisan. Setiap neuron menerima input dari neuron-neuron dalam lapisan sebelumnya, melakukan operasi matematika pada input ini, dan menghasilkan output yang akan diteruskan ke neuron-neuron dalam lapisan berikutnya.

Mirip dengan kerja jaringan saraf manusia, setiap koneksi antar neuron dalam ANN memiliki bobot atau nilai yang memengaruhi pentingnya informasi dari neuron sebelumnya. Selama proses pembelajaran, bobot-bobot ini diatur ulang agar ANN dapat belajar memahami pola-pola yang ada dalam data.

Saat ANN diberikan masukan (misalnya gambar, teks, atau data numerik), sinyal-sinyal ini bergerak melalui jaringan neuron, diolah, dan menghasilkan keluaran. Proses ini memungkinkan ANN untuk mengenali pola-pola kompleks pada data, seperti pengenalan wajah dalam gambar, klasifikasi teks, atau prediksi harga saham berdasarkan data historis.


Perceptron:
Perceptron adalah komponen dasar pembangun jaringan saraf tiruan. Frank Rosenblatt dari Cornell Aeronautical Library adalah ilmuwan yang pertama kali menemukan perceptron pada tahun 1957. Perceptron terinspirasi dari neuron pada jaringan saraf dalam otak manusia. Dalam jaringan saraf tiruan, perceptron dan neuron merujuk pada hal yang sama. 

Lantas, bagaimana perceptron bekerja pada jaringan saraf tiruan? 
Sebuah perceptron menerima masukan berupa bilangan numerik. Perceptron kemudian memproses masukan tersebut untuk menghasilkan sebuah keluaran. Agar lebih memahami cara kerja perceptron, kita akan menggunakan diagram di bawah.

Sebuah perceptron terdiri dari 5 komponen, yaitu:
- input (xi),
- bobot atau weights (wi) dan bias (w0),
- penjumlahan atau sum (sigma),
- fungsi aktivasi atau non linearity function, dan
- output (y).


Sebuah perceptron tunggal saat berdiri sendiri mungkin tidak memberikan hasil yang signifikan dalam konteks pemrosesan data kompleks. Namun, ketika perceptron ini dihubungkan dengan ratusan atau ribuan perceptron lain dalam sebuah jaringan yang lebih besar, kemampuannya untuk memproses informasi dan memberikan hasil akan jauh lebih kuat. 

Banyak perceptron yang saling terhubung dalam neural network. Setiap perceptron melakukan komputasi terhadap inputnya sendiri dengan bobot dan bias yang spesifik. Kemudian, informasi hasil komputasi dari satu perceptron akan mengalir ke perceptron-perceptron lainnya melalui koneksi yang terbentuk di antara mereka. Proses ini memungkinkan jaringan untuk mempelajari pola kompleks dalam data dan membuat prediksi atau keputusan yang akurat.

Dengan koneksi yang kuat antar-perceptron serta kemampuan untuk menyesuaikan bobot dan bias selama proses pelatihan, neural network dapat mengungguli banyak metode machine learning lainnya dalam hal keakuratan, begitu pun kemampuan adaptasi terhadap masalah yang rumit. 

Seiring dengan meningkatnya ukuran dan kompleksitas jaringan, performa neural network dapat semakin ditingkatkan. Ini menjadikannya alat yang sangat efektif dalam berbagai aplikasi, termasuk pengenalan pola, klasifikasi, prediksi, dan banyak lagi.

Pertama, input menerima masukan berupa angka-angka. Setiap input memiliki bobot masing-masing. Bobot adalah parameter yang akan dipelajari oleh sebuah perceptron dan menunjukkan kekuatan node tertentu. 

Selanjutnya adalah tahap penjumlahan input. Pada tahap ini, setiap input akan dikalikan dengan bobotnya masing-masing, lalu hasilnya ditambahkan dengan bias yang berupa sebuah konstanta atau angka. Nilai bias memungkinkan Anda untuk mengubah kurva fungsi aktivasi ke atas atau bawah sehingga bisa lebih fleksibel dalam meminimalkan error. Hasil penjumlahan pada tahap ini biasanya disebut weighted sum.

Langkah berikutnya, aplikasikan weighted sum pada fungsi aktivasi atau disebut juga non-linearity function. Fungsi aktivasi digunakan untuk memetakan nilai hasil menjadi nilai yang diperlukan, misalnya antara (0, 1) atau (-1, 1). Fungsi ini memungkinkan perceptron dapat menyesuaikan pola untuk data yang non-linier. Penjelasan lebih lanjut tentang fungsi aktivasi akan diulas pada paragraf di bawah.


      (     m        )
y =g ( W0 + EEE Wi Xi )
      (   i = 1      )

- y = output
- g = non-linear activation function

Fungsi matematis dari perceptron dapat kita lihat di bawah. Rumus di bawah adalah notasi matematis yang menjelaskan proses sebelumnya. Keluaran (ŷ) dari perceptron adalah bias (w0) ditambah dengan jumlah setiap input (xi) yang dikali dengan bobot masing-masing (wi) sehingga menghasilkan weighted sum, kemudian dimasukkan dalam fungsi aktivasi (g). 



Multilayer Perceptron (MLP):
Multilayer perceptron (MLP) atau feedforward neural network adalah jenis neural network yang terdiri dari banyak perceptron (neuron) yang saling terhubung dalam beberapa lapisan. MLP memiliki struktur yang terdiri dari tiga jenis layer utama.

1. Input Layer
Layer pertama dari MLP adalah input layer. Ini berfungsi untuk menerima data atau input dari luar. Setiap neuron dalam input layer mewakili satu fitur atau variabel dari data yang masuk ke jaringan. Misalnya, dalam aplikasi pengenalan gambar, setiap neuron dapat mewakili nilai intensitas piksel dari gambar.

2. Hidden Layer
Setelah menerima input, data akan diteruskan ke hidden layer (lapisan tersembunyi) dalam MLP. Hidden layer terdiri dari satu atau lebih lapisan antara input layer dan output layer. Neuron-neuron dalam hidden layer memproses input yang diterima dari layer sebelumnya dengan melakukan operasi matematika menggunakan bobot (weights) dan fungsi aktivasi tertentu. Hidden layer berfungsi sebagai penyaring atau extractor fitur yang membantu jaringan dalam mempelajari pola-pola kompleks pada data.

3. Output Layer
Output layer adalah layer terakhir dalam MLP yang menghasilkan output berdasarkan hasil pemrosesan oleh hidden layer. Jumlah neuron dalam output layer bergantung pada tipe tugas yang ingin diselesaikan oleh jaringan. Misalnya, untuk tugas klasifikasi biner, output layer dapat memiliki satu neuron yang menghasilkan nilai antara 0 dan 1 (mengindikasikan probabilitas kelas), sedangkan untuk klasifikasi multiclass, setiap neuron mungkin mewakili probabilitas dalam kelas tertentu.



Terms pada Neural Network:
1. Activation Function (Fungsi Aktivasi)
Fungsi aktivasi adalah sebuah fungsi matematika yang menentukan bahwa neuron dalam jaringan saraf tiruan akan menghasilkan output atau tidak berdasarkan inputnya. Analoginya, fungsi ini meniru cara neuron biologis "aktif" atau "tidak aktif" berdasarkan sinyal masukan yang diterima.

Fungsi aktivasi pada perceptron bertugas untuk membuat jaringan saraf mampu menyesuaikan pola dengan data non linier. Sebagaimana yang sudah pernah dibahas sebelumnya, mayoritas data di dunia nyata adalah data non linier seperti di bawah.

Fungsi aktivasilah yang memungkinkan jaringan saraf dapat mengenali pola non-linier seperti di bawah. Tanpa fungsi aktivasi, jaringan saraf hanya bisa mengenali pola linier layaknya garis pada regresi linier.


Secara umum, ada dua jenis activation function, linear, dan non-linear activation function. Ada beberapa jenis fungsi aktivasi yang umum digunakan dalam jaringan saraf sebagai berikut.

a. Linear [f(x) = x]
Linear activation function (fungsi aktivasi linear) dalam konteks jaringan saraf tiruan adalah sebuah fungsi dengan keluaran yang proporsional secara linear terhadap input. Dengan kata lain, fungsi ini hanya melakukan transformasi linear sederhana dari input ke output tanpa memperkenalkan non-linearitas.

Pada rumus linear, x adalah input ke neuron atau lapisan jaringan. Dalam hal ini, keluaran dari neuron atau lapisan tersebut sama dengan inputnya sendiri. Oleh karena itu, tidak ada transformasi non-linear yang terjadi.

Sebagai tambahan informasi, fungsi aktivasi linear jarang digunakan dalam lapisan tersembunyi (hidden layers) karena tidak mampu memodelkan hubungan yang kompleks antara variabel input dan output. Namun, fungsi ini dapat digunakan pada lapisan output untuk masalah regresi.


b. ReLU (Rectified Linear Activation) [f(x) = max(0, x)]
ReLU (rectified linear activation) adalah jenis fungsi aktivasi yang umum digunakan dalam jaringan saraf tiruan. Fungsi ReLU didefinisikan sebagai f(x) = max(0, x). Ini berarti output dari ReLU adalah nilai inputnya jika nilai input tersebut lebih besar dari atau sama dengan nol, dan output-nya adalah nol jika nilai inputnya kurang dari nol.

Keuntungan utama dari ReLU adalah sederhana pada komputasi dan memperkenalkan non-linearitas ke dalam jaringan. ReLU umumnya digunakan sebagai fungsi aktivasi pada lapisan tersembunyi karena kemampuannya untuk mempercepat konvergensi pembelajaran dan mengurangi risiko overfitting.

ReLU adalah pilihan populer dan efektif untuk fungsi aktivasi dalam jaringan saraf modern karena sederhana, efisien, serta mampu membantu jaringan mempelajari representasi yang kompleks dari data dengan baik.


c. Leaky ReLU [f(x) = max(0.1 x, x)
Kekurangan pada ReLU adalah beberapa gradien dapat menjadi sangat rapuh selama proses pelatihan, yang dapat menyebabkan fenomena "neuron mati". Ini berarti bahwa selama pelatihan, beberapa neuron dapat terkunci dalam keadaan mereka tidak akan pernah diaktifkan lagi pada sebagian besar atau semua data yang diberikan. Hal itu terjadi karena gradien (turunan) dari fungsi ReLU menjadi nol pada bagian negatifnya.

Untuk mengatasi masalah ini, modifikasi lain dari fungsi ReLU diperkenalkan, dikenal sebagai leaky ReLU. Ia memperkenalkan kemiringan kecil (biasanya nilai tetap yang sangat kecil, seperti 0.01) pada bagian negatif dari fungsi ReLU.

Hal ini membantu menjaga agar neuron tetap aktif selama pelatihan, bahkan jika gradien pada bagian negatifnya mendekati nol. Dengan demikian, leaky ReLU dapat mencegah "kematian" neuron dan membantu meningkatkan stabilitas serta kecepatan konvergensi dalam pelatihan jaringan saraf.


d. Sigmoid
Sigmoid akan menerima angka tunggal dan mengubah x menjadi sebuah nilai yang memiliki rentang mulai dari 0 sampai 1. Fungsi ini biasanya dapat diinterpretasikan sebagai probabilitas dalam konteks klasifikasi biner.

Keuntungan utama dari sigmoid adalah kemampuannya dalam menghasilkan keluaran dengan batas antara 0 dan 1, yang berguna untuk tugas klasifikasi ketika kita ingin memprediksi probabilitas keanggotaan pada kelas tertentu.

Namun, ada beberapa perhatian terkait penggunaan sigmoid.
a. Gradien yang Menghilang
Ketika nilai input sangat besar (positif atau negatif), gradien sigmoid cenderung mendekati nol, yang dapat menyebabkan masalah lambatnya konvergensi selama pelatihan jaringan. ?

b. Output yang Tidak Seimbang
Sigmoid memiliki kecenderungan untuk menghasilkan output yang condong ke nilai 0 atau 1 dengan cepat; ini dapat menghambat pembelajaran dalam beberapa kasus.


e. Tanh [f(x) = tanh(x)]
Tanh (hyperbolic tangent) adalah jenis fungsi aktivasi lain yang umum digunakan dalam jaringan saraf tiruan. Tanh akan mengubah nilai input x-nya menjadi sebuah nilai yang memiliki rentang mulai dari -1 hingga 1.

Namun, serupa halnya dengan sigmoid, tanh juga memiliki beberapa masalah, seperti rentang output yang terbatas (-1 sampai 1). Ini juga bisa menyebabkan gradien menghilang pada jaringan. Meskipun demikian, tanh masih menjadi pilihan populer untuk fungsi aktivasi dalam beberapa kasus, terutama ketika rentang output yang simetris di sekitar nol diinginkan atau saat sigmoid tidak memberikan hasil yang memuaskan.

Beberapa karakteristik dari fungsi tanh adalah berikut.
a. Rentang Output
Fungsi tanh menghasilkan output antara -1 dan 1. Ini membuatnya lebih simetris di sekitar titik nol dibandingkan sigmoid yang memiliki rentang antara 0 dan 1.

b. Non-linearitas
Layaknya sigmoid, tanh juga memperkenalkan non-linearitas ke jaringan. Ini memungkinkan jaringan untuk mempelajari hubungan yang lebih kompleks antara input dan output.

c. Penggunaan di Lapisan Tersembunyi
Tanh sering digunakan sebagai alternatif sigmoid pada lapisan tersembunyi karena rentangnya simetris dan kemampuannya untuk menangani gradien yang menghilang lebih baik daripada sigmoid.


f. Softmax
Softmax adalah jenis fungsi aktivasi yang umum digunakan pada lapisan output dari jaringan saraf, terutama untuk tugas klasifikasi multiclass. Fungsi softmax mengubah nilai input menjadi distribusi probabilitas yang memetakan output pada rentang (0, 1) sehingga total probabilitas output menjadi 1.

Berikut adalah beberapa karakteristik dari fungsi softmax.
a. Interpretasi Probabilitas
Output dari softmax dapat diinterpretasikan sebagai probabilitas bahwa input termasuk dalam setiap kelas yang mungkin karena nilai-nilainya berada pada rentang (0, 1) dan total probabilitasnya adalah 1.

b. Penanganan Masalah Multikelas
Softmax sangat berguna dalam tugas klasifikasi, yakni ketika ada lebih dari dua kelas yang mungkin. Ini memungkinkan model dalam menghasilkan prediksi probabilitas untuk setiap kelas.

c. Loss Function / Cost Function
Softmax sering digunakan bersama dengan cross-entropy loss function sebagai fungsi kerugian (loss function) dalam jaringan saraf untuk tugas klasifikasi multiclass. Ini karena softmax menghasilkan distribusi probabilitas dan cross-entropy dapat digunakan untuk mengukur kesalahan antara distribusi prediksi dan distribusi target.

Penggunaan softmax biasa ditemukan dalam lapisan output jaringan saraf, sedangkan pada lapisan tersembunyi, fungsi aktivasi lainnya, seperti ReLU atau tanh, lebih umum digunakan. Kombinasi softmax dengan fungsi aktivasi yang sesuai pada lapisan tersembunyi membentuk arsitektur jaringan saraf yang efektif untuk tugas klasifikasi multi kelas.


2. Loss Function

Loss function (fungsi kerugian) dalam konteks jaringan saraf adalah algoritma matematis yang digunakan untuk mengukur seberapa baik atau buruk kinerja model neural network pada data pelatihan. Fungsi kerugian memberikan representasi numerik tentang seberapa besar kesalahan atau perbedaan antara prediksi model dan nilai yang sebenarnya dalam data pelatihan.

Tujuan utama dari loss function untuk membimbing proses pelatihan jaringan saraf agar dapat meminimalkan kesalahan prediksi. Saat model melakukan prediksi pada data pelatihan, loss function menghitung seberapa jauh hasil prediksi dari nilai yang sebenarnya. Semakin kecil nilai loss function, semakin baik kinerja model.
 
Contoh loss function yang umum digunakan adalah mean square error (MSE). MSE menghitung rata-rata dari kuadrat selisih antara prediksi model dan nilai yang sebenarnya pada setiap titik data. Nilai MSE yang lebih kecil menunjukkan bahwa model memiliki kesalahan prediksi lebih rendah. 

Selain MSE, ada juga berbagai jenis loss function lainnya, seperti cross entropy loss yang umum digunakan untuk masalah klasifikasi. Setiap jenis loss function memiliki karakteristik dan aplikasi berbeda tergantung pada tipe masalah yang dihadapi dalam pembelajaran mesin. 

Pada dasarnya, loss function berperan penting dalam membantu model jaringan saraf untuk belajar dari data pelatihan. Dengan mengukur kesalahan prediksi secara objektif, loss function memungkinkan kita untuk menyesuaikan parameter model. Jadi, model dapat menghasilkan prediksi lebih akurat dan sesuai dengan data yang diberikan.

3. Optimizer

Optimizer (pengoptimal) adalah komponen kunci dalam pelatihan jaringan saraf tiruan yang bertanggung jawab untuk mengoptimalkan atau menyesuaikan bobot dan bias pada jaringan agar dapat mengurangi kesalahan prediksi. Tujuan utama dari pengoptimal adalah untuk menemukan nilai bobot dan bias dengan hasil prediksi paling akurat dan generalisasi yang baik terhadap data baru.

Beberapa fungsi utama dari optimizer dalam konteks jaringan saraf adalah berikut.
a. Menghitung Gradien
Optimizer menghitung gradien dari loss function terhadap parameter (weights dan biases) dalam jaringan. Gradien ini memberikan informasi tentang arah dan seberapa besar perubahan yang harus dilakukan pada parameter untuk mengurangi nilai fungsi kerugian.

b. Memperbarui Parameter
Berdasarkan gradien yang dihitung, optimizer akan memperbarui nilai parameter (weights dan biases) jaringan. Update ini dilakukan dengan cara menggerakkan nilai parameter ke arah yang mengurangi nilai fungsi kerugian. Cara update ini biasanya menggunakan metode gradient descent atau varian-varian lainnya, yakni momentum, RMSprop, Adam, dan lainnya.

c. Menangani Masalah Optimalisasi
Optimizer berusaha menangani masalah, seperti lambatnya konvergensi, kemungkinan terjebak dalam optimum lokal, ataupun masalah gradien yang meledak atau menghilang. Beberapa optimizer menggunakan berbagai teknik, seperti momentum, learning rate scheduling, atau adaptive learning rate untuk mengatasi masalah-masalah ini.

Beberapa contoh optimizer yang umum digunakan dalam pelatihan jaringan saraf seperti berikut.

a. Stochastic Gradient Descent (SGD): Optimizer klasik yang menggunakan gradien dari subset data (batch) untuk memperbarui parameter.

b. RMSprop: Optimizer yang menyesuaikan learning rate untuk setiap parameter berdasarkan perbedaan antara gradien saat ini dan sejarah gradien.

c. Adam: Optimizer adaptif yang menggabungkan konsep dari momentum dan RMSprop untuk mengatur learning rate secara adaptif berdasarkan estimasi momen gradien.

Pemilihan optimizer sangat penting dalam pelatihan jaringan saraf karena dapat memengaruhi kecepatan konvergensi, kualitas model yang dihasilkan, dan kemampuan jaringan untuk menghindari masalah-masalah optimisasi. Berbagai faktor, seperti jenis tugas, ukuran dataset, dan arsitektur jaringan, dapat memengaruhi pemilihan optimizer yang paling sesuai untuk suatu proyek.


Konsep Metode Forward Propagation dan Backpropagation:
1. Forward Propagation
Forward propagation adalah langkah pertama dalam proses penggunaan neural network untuk membuat prediksi berdasarkan masukan data.

a. Input Data
Langkah pertama dalam penggunaan neural network adalah memberikan data sebagai input pada jaringan. Data ini bisa berupa berbagai jenis informasi, seperti gambar (dalam bentuk piksel), teks (dalam bentuk token atau urutan kata), atau nilai numerik (seperti atribut atau fitur yang menggambarkan suatu objek atau fenomena).

Misalnya, jika kita ingin menggunakan neural network untuk klasifikasi gambar, data input dapat berupa gambar-gambar digital yang direpresentasikan dalam bentuk matriks piksel. Setiap piksel memiliki nilai intensitas yang mewakili warna pada posisi tertentu dalam gambar. Gambar-gambar ini kemudian dijadikan input pada neural network untuk memungkinkan model mempelajari pola dan fitur yang terkandung dalam gambar.

Contoh lainnya, jika kita ingin melakukan analisis sentimen terhadap teks, data input bisa berupa urutan kata atau kalimat. Setiap kata dalam teks direpresentasikan dengan token atau angka yang menunjukkan kata tersebut. Data tersebut kemudian diproses dan dimasukkan ke neural network untuk memprediksi sentimen atau makna dari teks tersebut.

Selain itu, data numerik, seperti atribut dari suatu objek, juga dapat digunakan sebagai input. Misalnya, jika kita ingin memprediksi harga rumah berdasarkan fitur-fitur, seperti luas tanah, jumlah kamar, lokasi, dan sebagainya, atribut-atribut ini dapat dimasukkan sebagai input pada neural network.

Dengan pemberian data sebagai input, neural network dapat memproses informasi ini melalui serangkaian langkah komputasi yang kompleks. Itu bertujuan untuk mempelajari pola, menerapkan transformasi, dan menghasilkan output yang berguna atau prediksi sesuai dengan keinginan berdasarkan tugas atau tujuan.


b. Neuron dan Bobot
Setelah data dimasukkan dalam neural network, data tersebut melewati serangkaian neuron pada berbagai lapisan jaringan tersebut. Setiap neuron dalam jaringan menerima input data dari neuron-neuron pada lapisan sebelumnya atau data asli, seperti gambar atau teks.

Setiap neuron memiliki parameter "bobot" (weights) yang digunakan untuk mengalikan input tersebut. Bobot ini mewakili kekuatan atau pentingnya setiap input terhadap aktivasi neuron. Misalnya, jika kita memiliki neuron pada lapisan pertama yang menerima gambar sebagai input, setiap piksel dalam gambar akan dikalikan dengan bobot yang sesuai. Bobot ini adalah angka-angka yang dipelajari oleh model selama proses pelatihan untuk mengoptimalkan kinerja jaringan.

Selain bobot, setiap neuron juga memiliki bias sebagai nilai tambahan yang ditambahkan ke hasil perkalian input dengan bobot. Bias membantu neuron untuk belajar membedakan pola yang kompleks dalam data. Misalnya, jika semua input bernilai nol, bias dapat memungkinkan neuron untuk tetap aktif dan belajar dari situasi-situasi yang tidak biasa.

Proses perkalian input dengan bobot dan penambahan bias pada setiap neuron menghasilkan nilai yang diteruskan ke fungsi aktivasi. **Fungsi aktivasi bertujuan untuk** memutuskan bahwa neuron tersebut harus "aktif" atau "tidak aktif" berdasarkan hasil perhitungan tersebut. Fungsi aktivasi yang umum digunakan adalah ReLU untuk lapisan tersembunyi dan softmax dalam lapisan output pada masalah klasifikasi.

Selanjutnya, hasil aktivasi dari setiap neuron akan menjadi input bagi neuron-neuron pada lapisan berikutnya dalam jaringan. Proses ini berulang terus menerus melalui seluruh jaringan dan setiap lapisan akan menghasilkan representasi yang semakin abstrak dari data asli. Hasil akhir dari neural network adalah prediksi atau output berdasarkan input data yang dipelajari oleh model selama proses pelatihan.


c. Perhitungan di Neuron
Pada sebuah neural network, setiap neuron memiliki peran penting dalam menghitung output berdasarkan input yang diterimanya. Proses perhitungan dalam neuron dimulai dengan menerima input, yang bisa berasal dari neuron-neuron pada lapisan sebelumnya atau data asli, seperti gambar, teks, atau nilai numerik. Tiap input ini kemudian dikalikan dengan bobot yang telah dipelajari oleh jaringan selama proses pelatihan. **Bobot ini menentukan seberapa penting setiap input terhadap aktivasi neuron.**

Selain itu, neuron memiliki bias yang ditambahkan ke hasil perkalian input dan bobot sebelum memasuki fungsi aktivasi. Bias ini membantu neuron mempelajari pola-pola yang kompleks dalam data. Setelah itu, hasil penjumlahan input dan bobot dengan bias dimasukkan ke fungsi aktivasi. Fungsi aktivasi, seperti ReLU atau sigmoid, bertujuan untuk menentukan output neuron berdasarkan ambang batas tertentu.

Output ini kemudian menjadi input bagi neuron-neuron pada lapisan berikutnya dalam jaringan. Proses ini terjadi secara berulang melalui seluruh jaringan, yakni setiap neuron belajar mengoptimalkan bobot dan biasnya untuk menghasilkan representasi yang semakin abstrak dari data.

Hasil akhir dari neural network adalah prediksi atau output sesuai dengan keinginan, yang digunakan untuk menyelesaikan tugas spesifik, seperti klasifikasi, regresi, atau pengenalan pola.


2. Backpropagation
Backpropagation adalah langkah kedua dalam proses penggunaan neural network untuk menghitung nilai error dari output hasil prediksi.

a. Perhitungan Error
Setelah mendapatkan output dari neural network, kita bandingkan output tersebut dengan nilai target yang sebenarnya (ground truth) dari data latih. Perhitungan error dilakukan dengan menggunakan loss function, seperti mean squared error (MSE) atau cross-entropy loss. Tujuan kita adalah untuk mengukur seberapa jauh prediksi neural network dari target sebenarnya.

b. Backward Pass
Setelah menghitung error, langkah berikutnya adalah melakukan backward pass. Ide dasar dari **backpropagation adalah menghitung seberapa besar setiap bobot harus diubah agar mengurangi kesalahan total neural network.**

Pertama, kita hitung gradien (turunan) dari loss function terhadap setiap bobot dalam neural network menggunakan aturan rantai (chain rule) dari kalkulus. Tujuannya untuk memberi tahu kita seberapa besar setiap bobot berkontribusi terhadap kesalahan total.

Gradien ini kemudian digunakan untuk mengubah setiap bobot dalam arah yang mengurangi kesalahan. Dalam hal ini, kita menggunakan algoritma optimasi, seperti gradient descent, ketika bobot diperbarui dengan menggerakkannya melawan arah gradien dengan suatu laju pembelajaran (learning rate).

c. Perubahan Bobot
Bobot-bobot dalam neural network diperbarui menggunakan hasil gradien yang telah dihitung sebelumnya. Proses ini bertujuan untuk menggerakkan bobot ke arah yang mengurangi nilai fungsi kerugian sehingga prediksi neural network menjadi lebih akurat.

d. Iterasi
Langkah-langkah forward propagation, perhitungan error, backward propagation, dan optimisasi bobot diulang untuk setiap batch data latihan (mini-batch) hingga bobot-bobot dalam neural network konvergen bernilai optimal. Proses ini dapat melibatkan banyak iterasi (epochs) tergantung pada kompleksitas model dan jumlah data latihan.






Pengantar Deep Learning:


Sejarah Perkembangan Deep Learning:

Deep learning memiliki akar yang dapat ditelusuri kembali ke konsep dasar jaringan saraf buatan, mulai pada tahun 1943 ketika Warren McCulloch dan Walter Pitts memperkenalkan model awal jaringan saraf. Konsep ini menginspirasi pengembangan selanjutnya dalam bidang kecerdasan buatan.

Pada tahun 1950-an dan 1960-an, ilmuwan seperti Frank Rosenblatt mengembangkan perceptron, bentuk awal dari jaringan saraf buatan yang mampu mengenali pola sederhana. Meskipun demikian, kemajuan signifikan dalam deep learning baru terjadi pada tahun 1980-an dan 1990-an, ketika muncul metode pembelajaran yang lebih efisien dan algoritma pelatihan yang lebih baik.

Pada tahun 2006, Geoffrey Hinton, Yoshua Bengio, dan Yann LeCun berhasil mengembangkan algoritma backpropagation; itu memungkinkan pelatihan jaringan saraf yang lebih dalam secara efisien. Backpropagation memungkinkan penyesuaian bobot jaringan berdasarkan kesalahan prediksi, memungkinkan pembelajaran yang lebih baik dari data.

Perkembangan lebih lanjut terjadi dengan penggunaan unit pemrosesan grafis (GPU) yang kuat untuk mempercepat pelatihan jaringan saraf kompleks. Selain itu, perkembangan perangkat keras yang lebih kuat dan tersedia secara luas mendukung kemajuan deep learning.

Titik balik terjadi pada tahun 2012 ketika jaringan saraf konvensional yang dalam, dikenal sebagai AlexNet, memenangkan kompetisi ImageNet dengan hasil mengesankan. Kemenangan ini menunjukkan kemampuan deep learning dalam pengenalan gambar.

Sejak itu, deep learning telah mencapai kemajuan signifikan dalam berbagai bidang. Dalam pengenalan gambar, deep learning digunakan untuk mendeteksi objek, mengklasifikasikan citra, dan menghasilkan deskripsi otomatis.

Pada pemrosesan bahasa alami, deep learning mempercepat kemajuan dalam pemahaman teks, terjemahan mesin, dan pengenalan suara.

Perkembangan deep learning didorong oleh hadirnya kerangka kerja (framework), seperti TensorFlow dan PyTorch, yang mempermudah pengembangan dan implementasi jaringan saraf yang dalam.


Definisi Deep Learning:

Deep learning adalah bagian dari bidang kecerdasan buatan dengan menggunakan algoritma yang terinspirasi dari cara otak manusia bekerja. Metode ini diimplementasikan melalui jaringan saraf tiruan yang disebut artificial neural networks atau disingkat sebagai ANN. 

ANN adalah model matematis yang terdiri dari tiga atau lebih lapisan neuron yang saling terhubung. Dengan struktur ini, ANN dapat memproses dan mempelajari pola kompleks dari data sehingga mampu mengatasi berbagai masalah yang sulit diselesaikan dengan algoritma machine learning konvensional.

Deep learning menggunakan konsep ANN dengan banyak lapisan (deep neural networks) untuk melakukan beberapa tugas, seperti pengenalan gambar, pengenalan suara, atau bahkan penerjemahan bahasa. Keunggulan utama deep learning terletak pada kemampuannya untuk belajar secara mandiri dari data yang besar dan kompleks. Proses pembelajaran ini melibatkan penyesuaian bobot dan parameter jaringan secara iteratif melalui proses yang disebut pelatihan (training), ketika model diuji dan diperbaiki berdasarkan umpan balik dari data.

Pada konteks machine learning, deep learning telah membuka kemungkinan baru dalam pemrosesan data, terutama perihal analisis gambar dan data berurutan. Metode-metode, seperti convolutional neural networks (CNNs) dan recurrent neural networks (RNNs), adalah contoh dari deep learning yang sangat sukses dalam domain ini. Meskipun demikian, deep learning juga menghadapi tantangan, termasuk sulitnya interpretasi dari keputusan model, kebutuhan data yang besar untuk pelatihan, serta kompleksitas dalam pengaturan parameter model.

Jadi, deep learning bisa dianggap seperti "membuat komputer belajar layaknya manusia", ketika model-modelnya bisa belajar sendiri dari data besar untuk membuat prediksi yang lebih cerdas dan akurat. Hal ini telah membawa terobosan besar dalam berbagai bidang, yaitu pengenalan wajah, mobil otonom, penelitian medis, dan banyak lagi, lo




Arsitektur Deep Learning:

Arsitektur deep learning adalah struktur atau tata letak jaringan saraf buatan yang kompleks. Arsitektur ini terdiri dari berbagai lapisan yang bertugas untuk memproses dan mentransformasikan data masukan menjadi hasil keluaran sesuai dengan keinginan.

Setiap lapisan dalam arsitektur memiliki fungsi khusus serta bertanggung jawab untuk mengekstrak fitur-fitur yang semakin abstrak dan kompleks seiring dengan meningkatnya kedalaman jaringan. Dengan kedalaman lebih besar, arsitektur deep learning dapat mempelajari pola yang lebih kompleks dari data dan menunjukkan hasil lebih baik.


Input Layer:
Input layer adalah bagian pertama dari jaringan neural yang bertanggung jawab untuk menerima data masukan, yaitu gambar, teks, atau data numerik lainnya. Fungsi utama dari input layer adalah meneruskan data masukan ke lapisan-lapisan selanjutnya dalam jaringan, yang disebut sebagai hidden layers.

Karakteristik:

- Input layer tidak melakukan komputasi yang kompleks, seperti aktivasi atau transformasi data. Tugasnya hanyalah untuk menerima data masukan dan meneruskannya ke hidden layers.
- **Jumlah neuron atau unit dalam input layer ditentukan oleh dimensi atau jumlah fitur pada data masukan**. Misalnya, jika data masukan berupa gambar berwarna dengan resolusi 32 × 32 piksel dan tiga saluran warna RGB, input layer akan memiliki 32 × 32 × 3 neuron.

Input layer berperan penting dalam proses pembelajaran jaringan neural. Ini karena data masukan yang disampaikan ke jaringan akan diproses dan dipelajari oleh hidden layers. Fungsinya untuk menghasilkan output yang diinginkan, seperti klasifikasi gambar, prediksi teks, atau regresi numerik.


Hidden Layer:
Hidden layer adalah lapisan-lapisan di antara input layer dan output layer dalam jaringan neural. Tugas utama dari hidden layers adalah untuk mengekstrak fitur-fitur yang semakin abstrak dan kompleks dari data masukan yang telah diteruskan oleh input layer.

Jenis jenis hidden layer:
1. Fully Connected Layer (Dense Layer)
Deskripsi: Setiap neuron pada lapisan ini terhubung dengan setiap neuron dalam lapisan sebelumnya dan sesudahnya.
Penggunaan Umum: Fully connected layer paling umum digunakan dalam jaringan saraf multi-layer perceptron.
Fungsi: Layer ini membantu dalam mempelajari hubungan yang kompleks antara fitur-fitur input.

2. Convolutional Layer (Conv Layer)
Deskripsi: Ini digunakan khusus untuk memproses data spasial, seperti gambar.
Karakteristik: Convolutional layer menggunakan filter atau kernel yang bergerak pada seluruh gambar untuk mengekstrak fitur lokal, seperti tepi, sudut, atau tekstur.
Penggunaan Umum: Lapisan ini digunakan dalam convolutional neural networks untuk tugas-tugas tertentu, seperti pengenalan gambar atau segmentasi objek

3. Batch Normalization Layer
Deskripsi: Batch normalization adalah teknik yang digunakan untuk mempercepat dan stabilisasi pelatihan jaringan neural dengan normalisasi batch. Umumnya ditempatkan setelah lapisan aktivasi (misalnya setelah convolutional layer atau fully connected layer) sebelum lapisan selanjutnya

karakteristik :
- Menghitung mean dan variance dari setiap batch data input.
- Normalisasi input dengan menggunakan mean dan variance untuk mengurangi internal covariate shift.
- Menerapkan transformasi linier pada setiap mini-batch untuk memperbaiki distribusi input ke setiap lapisan.

Penggunaan Umum: Lapisan ini digunakan untuk mempercepat konvergensi pelatihan dengan memungkinkan penggunaan learning rate yang lebih tinggi.


4. Recurrent Layer (RNN, LSTM, GRU)
Deskripsi: Ini digunakan untuk memproses data berurutan, seperti teks, audio, atau video.
Karakteristik: Lapisan ini mempertahankan informasi state (keadaan) dari waktu ke waktu sehingga mampu mengatasi masalah dependensi jarak jauh.
Penggunaan Umum: Recurrent layers, termasuk long short-term memory (LSTM) dan gated recurrent units (GRU), digunakan dalam aplikasi, seperti pemrosesan bahasa alami dan pengenalan suara.

5. Dropout Layer:
Deskripsi: Dropout layer adalah teknik regularisasi yang digunakan untuk mencegah overfitting dalam jaringan neural dengan secara acak "menonaktifkan" sebagian neuron pada setiap iterasi pelatihan.
Karakteristik: Ini menonaktifkan secara acak sebagian neuron dengan probabilitas tertentu selama proses pelatihan dan mencegah neuron menjadi terlalu bergantung pada subset tertentu dari neuron lainnya.
Penggunaan Umum: Ini digunakan di antara lapisan-lapisan tersembunyi pada jaringan dan membuatnya lebih robust dan umumnya efektif dalam mengatasi overfitting

6. Pooling Layer:
Deskripsi: Lapisan ini digunakan untuk mengurangi dimensi spasial dari feature map.
Karakteristik: Pooling layer menggabungkan informasi dari beberapa neuron tetangga untuk mengurangi ukuran representasi data.
Penggunaan Umum: Umumnya, ini digunakan setelah convolutional layers dalam CNN untuk mengurangi overfitting dan menghemat komputasi

7. Flatten Layer:
Deskripsi: Ini adalah lapisan yang mengubah tensor multidimensi (seperti hasil dari convolutional layers) menjadi vektor satu dimensi agar dapat diproses oleh lapisan fully connected layer.
Karakteristik: Flatten layer mengubah representasi data spasial menjadi representasi linear.
Penggunaan Umum: 
Biasanya ditempatkan sebelum masuk ke lapisan fully connected layer (dense layer) di akhir arsitektur.
Ini memungkinkan hasil dari feature extraction (misalnya convolutional layer) dapat dijadikan input untuk proses klasifikasi atau regresi pada lapisan fully connected.


Output Layer:
Output layer adalah layer terakhir dalam deep learning yang menghasilkan output berdasarkan hasil pemrosesan oleh hidden layer. Jumlah neuron dalam output layer bergantung pada tipe tugas yang ingin diselesaikan oleh jaringan. Misalnya, untuk tugas klasifikasi biner, output layer dapat memiliki satu neuron yang menghasilkan nilai antara 0 dan 1 (mengindikasikan probabilitas kelas), sedangkan untuk klasifikasi multiclass, setiap neuron mungkin mewakili probabilitas dalam kelas tertentu. 




Pengenalan Arsitektur Deep Learning yang Populer:


Convolutional Neural Network:
Convolutional Neural Network (CNN) adalah jenis arsitektur deep learning yang dirancang khusus untuk memproses data gambar dan visual dengan efisien. CNN terdiri atas serangkaian lapisan yang dapat mengekstrak fitur-fitur hierarkis dari gambar secara bertahap.

Karakteristik CNN:
- Lapisan konvolusi (convolutional layer): Lapisan ini menggunakan filter konvolusi untuk mengekstrak fitur-fitur lokal dari gambar, seperti tepi, sudut, atau tekstur. Filter ini bergerak di seluruh gambar untuk menghasilkan feature map.
- Lapisan pooling (pooling layer): Lapisan ini mengurangi dimensi spasial dari feature map dengan memilih nilai maksimum (max pooling) atau rata-rata (average pooling) dalam suatu area.
- Lapisan aktivasi (activation layer): Umumnya menggunakan ReLU (Rectified Linear Unit) sebagai fungsi aktivasi untuk memperkenalkan non-linearitas ke dalam jaringan.
- Lapisan fully connected (dense layer): Ini digunakan untuk klasifikasi akhir berdasarkan fitur-fitur yang diekstraksi.


Recurrent Neural Network (RNN):
Recurrent neural network (RNN) adalah arsitektur deep learning yang dirancang untuk memproses data berurutan, seperti teks, audio, atau time series data. RNN memiliki kemampuan untuk "mengingat" informasi dari iterasi sebelumnya melalui loop rekursif

Karakteristik RNN:
- Recurrent loop (loop rekursif): Ini memungkinkan RNN untuk mengolah data berurutan dan mempertahankan konteks dari waktu ke waktu

- Memory cell (sel memori): Pada LSTM dan GRU (variasi RNN yang lebih canggih), ini memungkinkan jaringan untuk mengingat informasi dalam jangka panjang.

Implementasi:
- Pemrosesan bahasa alami, seperti penerjemahan mesin, pembangkitan teks, dan analisis sentimen.
- Pengenalan suara dan pengolahan sinyal audio.
- Prediksi time series, misalnya peramalan harga saham atau cuaca.


Long Short-Term Memory (LSTM): 
Long short-term memory (LSTM) adalah jenis RNN yang ditingkatkan dengan mekanisme gate untuk mengatasi masalah hilangnya informasi jangka panjang dalam pembelajaran berurutan. 

Karakteristik LSTM:
- Forget gate: LSTM dapat "melupakan" informasi yang tidak relevan atau usang dengan menggunakan gate ini 

- Input gate: Ini mengontrol aliran informasi baru yang akan disimpan dalam memori jangka panjang.

Implementasi :
- Pemrosesan bahasa alami yang memerlukan pemahaman konteks lebih dalam, seperti generasi teks yang alami dan jelas
- Pengenalan wicara dan pemrosesan sinyal audio yang membutuhkan pengertian konteks temporal


Generative Adversarial Network:
Generative adversarial network (GAN) adalah arsitektur deep learning terdiri dari dua jaringan neural berlawanan yang saling bersaing, yaitu generator dan diskriminator.

Karakteristik GAN:

- Generator: Ini menghasilkan data sintetis, seperti gambar, dari distribusi laten.
- Diskriminator: Ini mencoba membedakan antara data asli dan data sintetis yang dihasilkan oleh generator.

Implementasi:
- Menghasilkan gambar sintetis untuk aplikasi kreatif, seperti pembuatan gambar wajah palsu atau pembingkaian ulang data.
- Pemberdayaan data dan augmentasi dataset untuk melatih model yang lebih baik 


Transformer:
Transformer adalah arsitektur deep learning berbasis atensi (attention-based) yang digunakan, terutama dalam pemrosesan bahasa alami untuk mengatasi masalah ketergantungan jarak panjang 

Karakteristik Transformer:
- Mekanisme self-attention: Ini memungkinkan model untuk memberikan bobot yang tepat pada kata-kata penting dalam teks 
- Encoder block dan decoder block: Ini digunakan untuk tugas seperti penerjemahan mesin dan pemodelan bahasa

Implementasi:
- Penerjemahan mesin lintas bahasa
- Pemodelan bahasa untuk tugas NLP, seperti analisis teks atau penghasilan teks yang alami.


Autoencoder:
Autoencoder adalah arsitektur deep learning yang digunakan untuk melakukan reduksi dimensi atau generasi data sintetis 

Karakteristik:
- Encoder: Ini menghasilkan representasi tersembunyi dari data input 
- Decoder: Ini memulihkan data asli dari representasi tersembunyi yang dihasilkan oleh encoder 


Implementasi:
- Reduksi dimensi data untuk mempercepat proses pelatihan
- Denoising dan menghasilkan data sintetis untuk penggunaan tertentu


Pengenalan Convolutional Neural Network:
Convolutional neural networks (CNN) pertama kali dikenalkan oleh Yann LeCun dkk., pada tahun 1998 dalam makalahnya “Gradient-Based Learning Applied to Document Recognition”. LeCun mengenalkan versi awal CNN, yaitu LeNet (berasal dari nama LeCun), yang berhasil mengenali karakter tulisan tangan. Saat itu, LeNet hanya mampu bekerja dengan baik pada gambar dengan resolusi rendah.

Database yang digunakan dalam LeCun adalah MNIST Database of Handwritten Digits, terdiri dari pasangan angka 0 hingga 9 dengan labelnya. Dataset MNIST dikenal luas hingga saat ini dan banyak digunakan terutama oleh para pemula untuk melatih model machine learning.

Sejak ditemukannya LeNet, para peneliti terus melakukan riset untuk mengembangkan model CNN. Hingga pada tahun 2012, Alex Krizhevsky memperkenalkan AlexNet, versi lebih canggih dari CNN yang memenangkan perlombaan terkenal: ImageNet. AlexNet ini adalah cikal bakal deep learning, salah satu cabang AI yang menggunakan multi-layer neural networks 

Selain deep learning, salah satu bidang menarik yang muncul dari perkembangan machine learning adalah computer vision. Itu adalah bidang yang memberi komputer kemampuan untuk ‘melihat’ seperti manusia.

Convolutional neural network (CNN) adalah tipe jaringan saraf yang umum digunakan untuk memproses data gambar. CNN digunakan untuk mendeteksi dan mengenali objek dalam gambar, terinspirasi dari cara manusia memproses informasi visual. 

Secara umum, CNN memiliki struktur mirip seperti jaringan saraf biasa dengan neuron yang memiliki bobot, bias, dan fungsi aktivasi. Namun, pembeda CNN dengan lainnya adalah lapisan konvolusi, terdiri dari neuron-neuron yang tersusun dalam filter dengan dimensi panjang dan tinggi (piksel).


Bagaimana CNN Bekerja:
Secara garis besar, CNN memanfaatkan proses konvolusi untuk mengolah gambar. Proses konvolusi melibatkan penggunaan sebuah kernel konvolusi (filter) berukuran tertentu yang digerakkan melintasi seluruh gambar. Pada setiap lokasi dalam gambar, filter ini berinteraksi dengan piksel-piksel di sekitarnya.

Langkah 1: Memecah gambar menjadi bagian-bagian lebih kecil atau jendela-jendela gambar yang disebut "patches”.

Dari gambar seorang anak kecil yang menaiki kuda mainan, hasil dari proses konvolusi dapat diilustrasikan dengan membagi gambar tersebut menjadi bagian-bagian kecil atau "patches".

Dengan menggunakan konvolusi, gambar asli seorang anak kecil di atas dibagi menjadi 77 gambar yang lebih kecil.

Langkah 2: Memasukkan setiap gambar yang lebih kecil ke jaringan saraf yang lebih sederhana (small neural network).

Setiap gambar kecil hasil konvolusi digunakan sebagai input untuk menghasilkan representasi fitur melalui sebuah proses yang memberikan kemampuan kepada convolutional neural network (CNN) dalam mengenali objek, tidak peduli bahwa posisi objek tersebut muncul pada gambar. Proses ini diulang sebanyak 77 kali untuk setiap gambar kecil dengan menggunakan filter yang sama untuk setiap iterasi. 

Dengan demikian, setiap bagian dari gambar kecil akan mengalami transformasi yang sama menggunakan faktor pengali yang sama. Dalam konteks jaringan saraf, hal itu disebut sebagai weights sharing. Jika ada hal menarik atau penting dalam setiap gambar kecil, fitur tersebut akan ditemukan dan diidentifikasi sebagai objek yang relevan (object of interest). 

Proses ini memungkinkan CNN untuk memahami berbagai aspek visual pada gambar secara komprehensif dan memperoleh pemahaman yang lebih dalam tentang objek-objek pada gambar.

Langkah 3: Menyimpan hasil dari masing-masing gambar kecil yang telah melewati proses konvolusi pada sebuah array baru. 

Langkah ketiga dalam pengolahan gambar menggunakan convolutional neural network (CNN) adalah menyimpan hasil dari masing-masing gambar kecil yang telah melewati proses konvolusi pada sebuah array baru.

Setelah setiap gambar kecil melewati lapisan konvolusi dan lapisan-lapisan jaringan saraf (neural network) lainnya, gambar-gambar tersebut akan menghasilkan representasi fitur yang terdiri dari nilai-nilai numerik. Representasi fitur ini mencerminkan informasi penting yang diekstrak dari setiap bagian gambar.

Proses penyimpanan dilakukan dengan mengumpulkan semua representasi fitur dari gambar-gambar kecil dan mengaturnya pada sebuah array data. Array ini akan memiliki dimensi sesuai dengan jumlah gambar kecil yang dihasilkan oleh konvolusi.

Array baru yang berisi representasi fitur dari gambar-gambar kecil tersebut akan digunakan sebagai input untuk langkah-langkah selanjutnya dalam pengolahan data, seperti klasifikasi, deteksi objek, atau segmentasi. Representasi fitur ini menjadi landasan penting pada pengambilan keputusan oleh CNN terkait dengan informasi dalam gambar asli.

Langkah 4: Downsampling.

Langkah keempat dalam convolutional neural network (CNN) setelah representasi fitur dari gambar-gambar kecil disimpan pada array baru adalah melakukan downsampling. Ini adalah proses pengurangan dimensi atau resolusi dari representasi fitur yang telah diperoleh.

Proses downsampling umumnya dilakukan menggunakan lapisan pooling, seperti max pooling atau average pooling. Lapisan pooling mengambil bagian tertentu dari representasi fitur (misalnya, area 2 × 2 atau 3 × 3) dan melakukan operasi statistik, seperti mengambil nilai maksimum atau max pooling maupun rata-rata atau average pooling dari bagian tersebut.

Tujuan dari downsampling adalah mengurangi jumlah parameter dalam jaringan, mengurangi overfitting, dan meningkatkan invariansi terhadap pergeseran spasial. Dengan mengurangi resolusi representasi fitur, CNN dapat mempertahankan informasi penting sambil mempercepat komputasi dan mengurangi kompleksitas model.

Setelah proses downsampling selesai, representasi fitur yang sudah di-downsample akan menjadi input untuk lapisan-lapisan selanjutnya dalam CNN, seperti lapisan konvolusi tambahan, lapisan aktivasi, atau lapisan fully connected. Lapisan-lapisan itu akan terus memproses dan mempelajari fitur-fitur dari gambar secara hierarkis untuk tujuan tertentu, seperti klasifikasi atau deteksi objek. 

Langkah kelima dalam CNN setelah melakukan proses downsampling adalah membuat prediksi berdasarkan representasi fitur yang telah diproses oleh jaringan. Proses ini terjadi di bagian akhir dari arsitektur CNN, setelah representasi fitur melewati beberapa lapisan, yakni konvolusi, aktivasi, dan pooling.

Untuk membuat prediksi, representasi fitur yang telah dihasilkan dari gambar-gambar masukan akan dimasukkan ke lapisan fully connected di akhir jaringan. Lapisan ini akan mengubah representasi fitur menjadi vektor satu dimensi yang kemudian diumpankan ke jaringan saraf terakhir (biasanya berupa softmax) untuk menghasilkan output klasifikas.

Dalam lapisan softmax, vektor fitur akan diubah menjadi distribusi probabilitas yang menunjukkan kemungkinan kelas-kelas berbeda berdasarkan representasi fitur yang diterima. Kelas dengan probabilitas tertinggi akan menjadi prediksi akhir dari CNN untuk gambar yang diberikan.

Proses pembuatan prediksi ini adalah langkah terakhir dalam CNN dan biasanya dilakukan ketika jaringan telah menjalani proses pelatihan untuk mempelajari hubungan antara representasi fitur dengan label atau kelas yang benar dari gambar-gambar pelatihan. Dengan menggunakan prediksi yang dihasilkan oleh CNN, kita dapat mengidentifikasi atau mengklasifikasikan objek atau pola dalam gambar. Tingkat akurasinya pun tinggi berdasarkan pembelajaran dari data pelatihan.



Pengenalan Recurrent Neural Network:
Recurrent neural network (RNN) adalah jenis arsitektur jaringan saraf yang dirancang untuk memproses data berurutan, yakni ketika hubungan antar elemen dalam urutan memiliki arti atau konteks temporal. RNN biasanya digunakan pada data teks (sekuens kata), audio (gelombang suara), deret waktu (data berurutan terkait waktu), dan lainnya. Keunggulan utama RNN adalah kemampuannya untuk "mengingat" atau mempertahankan informasi tentang sejarah (konteks) dari urutan data yang diproses.

Anda dapat menemukan RNN digunakan dalam aplikasi populer, seperti Siri, pencarian suara, dan Google Translate. Sebagaimana halnya jaringan saraf feedforward dan convolutional neural networks (CNNs), RNN juga belajar dari data pelatihan. **RNN mengambil informasi dari input dan output saat ini yang memungkinkannya untuk "mengingat" konteks sebelumnya.**

Tujuan utama RNN adalah memahami dan memproses data yang memiliki struktur urutan atau jangka waktu. Hal ini membuat RNN sangat berguna dalam aplikasi-aplikasi yang melibatkan data berurutan, yaitu pemrosesan bahasa alami atau natural language processing (NLP), pemodelan bahasa, prediksi deret waktu (time series), pengenalan suara, dan lainnya. RNN memungkinkan jaringan saraf untuk memahami konteks temporal dan menghasilkan prediksi atau output berdasarkan urutan masukan yang diberikan.


Tipe-tipe RNN:
Ada empat tipe RNN berdasarkan panjang input dan output yang berbeda.

1. One-to-one adalah jaringan neural sederhana yang umum digunakan untuk masalah pembelajaran mesin dengan satu input dan satu output.

2. One-to-many memiliki satu input dan banyak output. Ini biasanya digunakan untuk menghasilkan deskripsi gambar.

3. Many-to-one mengambil urutan multiple inputs dan memprediksi satu output. Populer dalam klasifikasi sentimen, yaitu ketika inputnya berupa teks dan output-nya adalah kategori.

4. Many-to-many menggunakan multiple inputs dan outputs. Aplikasi paling umumnya adalah terjemahan mesin.


Jenis-jenis RNN:
RNN merupakan salah satu arsitektur jaringan saraf yang sangat berguna dalam memproses data berurutan. Namun, terkadang satu model RNN saja tidak cukup untuk menangani semua tugas yang berbeda. Karena itu, RNN telah berkembang menjadi beberapa jenis yang dibedakan berdasarkan bentuk dasar dan cara kerja masing-masing. Setiap jenis RNN ini dirancang untuk menangani situasi atau masalah tertentu dengan lebih baik. 

Sebagai contoh, ada jenis RNN sederhana yang disebut Vanilla RNN, yang memiliki keterbatasan dalam mengingat informasi dari waktu ke waktu. Kemudian, ada jenis RNN yang lebih canggih, seperti long short-term memory (LSTM) dan gated recurrent unit (GRU), yang dirancang khusus untuk mengatasi masalah hilangnya informasi jangka panjang. Dengan memahami perbedaan dan kelebihan masing-masing jenis RNN, pengembang dan peneliti dapat memilih model yang paling sesuai untuk tugas mereka.

1. RNN Sederhana (Vanila RNN)
RNN sederhana, atau sering disebut sebagai vanilla RNN, adalah bentuk dasar dari arsitektur recurrent neural network. Pada RNN sederhana, setiap neuron memiliki sambungan kembali ke dirinya sendiri. Ini memungkinkan RNN sederhana untuk menggunakan output sebelumnya sebagai input pada langkah waktu berikutnya dalam urutan data

Meskipun RNN sederhana tidaklah rumit dan mudah diimplementasikan, masalah vanishing gradient menjadi kendala utama dalam aplikasi pada data berurutan yang panjang. Inovasi seperti long short-term memory dan gated recurrent unit dikembangkan untuk mengatasi masalah ini dan meningkatkan kemampuan RNN dalam memodelkan hubungan

2. Long Short-Term Memory (LSTM)
LSTM adalah varian RNN yang dikembangkan untuk mengatasi masalah vanishing gradient. LSTM memiliki struktur yang lebih kompleks dengan mekanisme gerbang (gate mechanism) yang memungkinkan model untuk memilih dan melupakan informasi secara selektif. Hal ini membuat LSTM efektif dalam memahami konteks jangka panjang pada data berurutan, seperti pengenalan teks atau prediksi deret waktu

Dengan menggunakan mekanisme gerbang ini, LSTM dapat mengontrol aliran informasi lebih baik dan mempertahankan informasi yang relevan dalam cell state. Hal ini memungkinkan LSTM untuk efektif mengatasi masalah vanishing gradient dan memodelkan konteks jangka panjang pada data berurutan, seperti dalam pengenalan teks atau prediksi deret waktu


3. Gated Recurrent Unit (GRU)
Gated recurrent unit (GRU) adalah variasi dari LSTM karena keduanya memiliki kemiripan dalam desain. Dalam beberapa kasus, keduanya pun membuat hasil yang serupa.

GRU menggunakan gerbang pembaruan (update gate) dan gerbang reset (reset gate) untuk mengatasi masalah vanishing gradient. Gerbang-gerbang ini memutuskan informasi yang penting dan meneruskannya ke output. Gerbang-gerbang ini dapat dilatih untuk menyimpan informasi dari waktu yang lama tanpa menghilang seiring berjalannya waktu atau menghapus informasi tidak relevan.

Berbeda dengan LSTM, GRU tidak memiliki cell state. GRU hanya memiliki state tersembunyi (hidden state). Lalu, sebab arsitekturnya yang sederhana, GRU memiliki waktu pelatihan lebih singkat dibandingkan model LSTM. Arsitektur GRU mudah dipahami karena mengambil input xt dan state tersembunyi dari timestamp sebelumnya ht-1, lalu menghasilkan state tersembunyi baru ht.


Cara Kerja RNN:

Langkah 1: Pengolahan Input

RNN menerima input berurutan, seperti kata-kata dalam sebuah kalimat atau frame pada video. Setiap elemen input direpresentasikan dengan vektor fitur numerik. Misalnya, dalam pemrosesan teks, setiap kata dapat diubah menjadi vektor berdasarkan representasi tertentu, seperti word embedding. Input ini diberikan satu per satu ke jaringan, dimulai sejak elemen pertama hingga elemen terakhir dari urutan data.

Langkah 2: Perhitungan Aktivasi

Setiap unit (neuron) dalam RNN menghitung aktivasi berdasarkan input saat ini dan status internal (state) dari unit pada waktu sebelumnya. Aktivasi ini mencerminkan informasi yang telah dipelajari dari konteks sebelumnya dalam urutan data. Perhitungan aktivasi dilakukan menggunakan fungsi aktivasi, seperti tangen hiperbolik (tanh) atau fungsi sigmoid

Langkah 3: Pembaruan Status Internal

Setiap unit RNN memiliki status internal yang menyimpan informasi dari elemen-elemen sebelumnya dalam urutan data. Status internal tersebut diperbarui pada setiap langkah waktu dengan mempertimbangkan aktivasi saat ini dan status internal sebelumnya. Pembaruan status internal dapat dijelaskan dengan rumus matematis yang melibatkan operasi matematika, seperti perkalian matriks antara vektor input dan bobot serta penambahan bias.

Langkah 4: Output

Pada setiap langkah waktu, RNN menghasilkan output berdasarkan aktivasi saat ini atau status internal terakhir. Output ini dapat digunakan sebagai prediksi berikutnya dalam urutan (misalnya, kata berikutnya pada kalimat) atau sebagai hasil akhir dari proses pemrosesan urutan data. Output RNN dapat digunakan dalam berbagai tugas, seperti klasifikasi, regresi, atau generasi urutan.

Proses tersebut diulang untuk setiap elemen dalam urutan data. Hal itu memungkinkan RNN untuk memahami konteks temporal dan memproses data berurutan secara dinamis. Dengan memanfaatkan hubungan temporal antar elemen data, RNN dapat menghasilkan prediksi atau output yang relevan untuk berbagai tugas pemrosesan data. Secara keseluruhan, RNN adalah alat yang kuat untuk memodelkan informasi berurutan. RNN pun telah digunakan dalam berbagai aplikasi yang memerlukan pemahaman dan pengolahan data temporal. 

1. Bagaimana komputasi yang dilakukan oleh sebuah neuron dalam Artificial Neural Network? 
b. Sebuah neuron menghitung fungsi linear (z = Wx + b) diikuti oleh fungsi aktivasi.


2. Bagaimana lapisan konvolusi dalam CNN menghasilkan fitur-fitur yang semakin kompleks?
b. Dengan menambahkan lebih banyak lapisan konvolusi.

3. Mengapa tantangan utama RNN disebut "vanishing gradient problem"?
b. Vanishing gradient problem terjadi ketika RNN tidak mampu menghasilkan output untuk urutan input yang panjang
d. Vanishing gradient problem terjadi ketika RNN hanya dapat menghasilkan output untuk input dengan dimensi rendah.

4. Apa keuntungan menggunakan Convolutional Neural Network (CNN) dibandingkan dengan jaringan saraf feedforward tradisional untuk tugas pengolahan gambar?
a. CNN dapat memproses gambar secara lokal dengan mempertimbangkan relasi spasial antar piksel.




---------------------------------------------------------------------------
                Neural Network Dengan Tensorflow dan Keras
---------------------------------------------------------------------------
 Ryan memberikan sebuah saran kepada Diana, “Na, kamu pernah dengar tidak istilah ‘garbage in garbage out’? Istilah tersebut sangat ramai ketika aku mempelajari deep learning, coba deh pelajari itu juga.” Gigo mengacu pada prinsip bahwa kualitas keluaran dari sebuah sistem komputer (termasuk model machine learning) sangat tergantung pada kualitas masukan yang diterimanya. Jika data yang digunakan sebagai masukan berkualitas buruk atau tidak relevan, maka hasil yang diperoleh dari model juga akan berkualitas buruk atau tidak akurat 

Setelah menyadari hal itu, Diana melanjutkan pembelajarannya dengan metode slow but sure. Ia tidak mau melewatkan lagi aspek kecil yang dapat menjadi penghalang di kemudian hari


Pengenalan tensorflow dan Keras untuk Deep Learning:

Selain itu, kita juga akan belajar menggunakan TensorFlow Datasets untuk memudahkan pengaturan data loading, mempermudah pelatihan model sehingga Anda semakin dekat menjadi seorang praktisi machine learning.

“Bagaimana membuat berbagai macam layer yang sebelumnya dipelajari?” Sekarang saatnya kita mengenal sebuah kekuatan yang lebih besar untuk membangun model deep learning dan berbagai macam layer dengan lebih mudah, perkenalkan TensorFlow dan Keras! 

Seperti yang dapat Anda lihat pada gambar di atas, TensorFlow dan Keras merupakan framework yang sangat terkenal di kalangan praktisi machine learning, tetapi hal ini bukan berarti framework lainnya kurang bagus, ya. Hasil ini merupakan riset yang dilakukan oleh Jeff Hale pada tahun 2018 mengenai deep learning. Riset tersebut menggunakan 11 sumber data pada 7 kategori yang berbeda untuk mengukur popularitas framework berdasarkan ketertarikan pengguna. Hasilnya TensorFlow menang telak, bukan? Hal ini terjadi karena kedua framework ini memiliki beberapa kelebihan, seperti Open-source, mudah untuk debugging, compatibility yang baik, scalability yang fleksibel, komunitas yang luas, dan masih banyak lagi. 


TensorFlow (TF)

TensorFlow (TF) adalah platform open-source end-to-end yang dikembangkan oleh Google Brain dan sangat populer untuk pengembangan machine learning berskala besar. TensorFlow memiliki ekosistem tools, library, dan sumber daya komunitas yang komprehensif dan fleksibel, sehingga memungkinkan para peneliti dan pengembang membangun serta menerapkan aplikasi machine learning dengan mudah

Perjalanan TensorFlow sangatlah panjang, semuanya berawal pada perilisan TensorFlow pada tahun 2015 TensorFlow dibuat open-source di bawah lisensi Apache. Perkembangan yang signifikan dan cepat terus diperlihatkan oleh TensorFlow, hingga pada tahun 2019 sebuah update dikeluarkan oleh Google dengan nama baru yaitu TensorFlow 2.0. 

Mengapa update ini sangat berpengaruh? Ada beberapa hal yang perlu kita highlight dari perkembangan TensorFlow ini mulai dari hadirnya TensorBoard, fitur kolom, kompatibilitas hardware, hingga kemunculan TensorFlow Datasets. 

Secara umum, TensorFlow memiliki banyak kelebihan seperti yang sudah disebutkan di atas. Tentunya Anda penasaran terkait kelebihan tersebut, bukan? Mari kita bahas kelebihan TensorFlow satu per satu agar Anda lebih terpacu untuk menyelesaikan misi menjadi praktisi machine learning yang andal.

1. Open Source
Sebagai sebuah frameworkopen-source tentunya penggunaan TensorFlow tidak mengeluarkan biaya dan bisa digunakan oleh semua orang mulai dari pelajar, guru, dosen, peneliti, data scientist, machine learning engineer, dan lainnya. Selain itu, hal ini juga memungkinkan untuk menggunakan TensorFlow kapan saja serta di mana saja sesuai kebutuhan pengguna tanpa harus memikirkan pengeluaran sedikitpun.

2. Scalability
Hingga saat ini, TensorFlow tidak terbatas hanya pada satu platform saja. Kelebihan ini tentu menguntungkan bagi pengguna karena kita dapat membangun sebuah sistem AI menggunakan TensorFlow di berbagai platform dengan komputasi atau performa yang sama. Selain itu, model yang dihasilkan TensorFlow juga dapat menyesuaikan komputasi yang digunakan berdasarkan jumlah requests. Sangat menarik, bukan? 

3. Compatibility
Saat ini, kompatibilitas/kecocokan merupakan salah satu faktor yang paling penting. Nah, pada kasus ini, TensorFlow kompatibel dengan berbagai macam bahasa pemrograman, seperti Python, C++, JavaScript, Kotlin, dan lain sebagainya. Selain itu TensorFlow juga dapat diimplementasikan pada berbagai platform menggunakan TensorFlow Lite, TensorFlow Js dan TensorFlow Serving.

Selain itu, TensorFlow juga menyediakan beberapa package yang dapat disandingkan dengan bahasa pemrograman yang lebih banyak, seperti C#, Haskell, Julia, MATLAB, R, Pascal, Rust, OCaml, Crystal, dan masih banyak lagi.

Sebagai praktisi machine learning yang umumnya menggunakan Python, Anda tidak perlu khawatir lagi dengan requests dari stakeholder yang menggunakan bahasa pemrograman lain.

4. Parallelism
Hal krusial lainnya yang mampu ditangani TensorFlow adalah hardware acceleration Mungkin sebagian dari Anda bertanya, “Apa itu hardware acceleration?” Ini merupakan sebuah library yang memberikan TensorFlow kemampuan untuk membagikan ketersediaan hardware seperti CPU atau GPU pada proses pelatihan maupun prediksi.

TensorFlow hadir dengan tf.distribute() sebagai salah satu modul dalam TensorFlow yang menyediakan tools dan API untuk melatih model deep learning secara terdistribusi. Ini sangat berguna untuk mempercepat pelatihan dengan menggunakan beberapa perangkat keras seperti GPU atau TPU dan untuk mengelola pelatihan di lingkungan yang besar.

5. Graph
Untuk meningkatkan pengalaman developer ketika membangun sebuah machine learning, tentunya kita juga memerlukan pengukuran yang lebih mudah dengan menggunakan visualisasi. Hal itu akan meringankan beban kognitif yang dibebankan kepada otak kita. Setelah memikirkan banyak hal pada setiap tahapan pembangunan machine learning, tentunya kita juga memerlukan sebuah alat yang dapat mempermudah perhitungan performa model.

Oleh karena itu, TensorFlow menyediakan sebuah alat bernama TensorBoard yang dapat membantu kita untuk menghitung performa model yang disajikan dalam bentuk visualisasi. TensorBoard memungkinkan pelacakan metrik percobaan, seperti loss dan akurasi, memvisualisasikan grafik model, memproyeksikan embedding ke dimensi yang lebih kecil, dan banyak lagi. Tentunya alat ini akan sangat membantu, bukan?

6. Architectural Support
TensorFlow didukung oleh beberapa hardware yang dapat melakukan proses pelatihan, mulai dari CPU, GPU, hingga TPU. Pada tahap eksplorasi, CPU dapat dikatakan sangat cukup untuk melakukan pelatihan machine learning maupun deep learning. Namun, untuk beberapa kasus seperti dataset yang sangat besar atau data unstructured (gambar, suara, video, dan lain sebagainya), GPU akan memproses pelatihan dengan lebih cepat. Hal ini dikarenakan GPU memiliki sebuah arsitektur yang dapat melakukan proses yang sangat banyak secara bersamaan.

Arsitektur ini bernama Compute Unified Device Architecture (CUDA) yang sayangnya pada saat ini hanya dimiliki oleh brand NVIDIA. Eitsss, jangan khawatir, Anda juga dapat menggunakan VGA lainnya untuk melakukan proses pelatihan dengan lebih cepat. Anda dapat melihat perbandingannya pada video di sini ya. 

Lalu, bagaimana jika tidak memiliki VGA tambahan atau komputer yang mempunyai spesifikasi rendah? Tentunya ada beberapa cara agar Anda dapat melakukan pelatihan dalam waktu yang lebih singkat dengan hardware yang mumpuni.

Anda dapat menggunakan Google Colab dan Kaggle untuk mendukung proses pelatihan dengan dataset yang besar sehingga dapat meminimalisasi waktu pelatihan. Kedua platform tersebut menyediakan environment yang dapat menjalankan kebutuhan pelatihan dengan sangat baik. Anda juga dapat mengatur penggunaan CPU, GPU, bahkan TPU saat menggunakan platform tersebut. Wow, keren bukan?

7. Library Management
Sebagai salah satu framework yang dikembangkan oleh Google, tentunya TensorFlow memiliki update yang sangat baik. TensorFlow sendiri melakukan pembaruan dalam waktu yang sangat cepat agar dapat memberikan pengalaman yang maksimal kepada seluruh developernya. Hingga saat ini, pembaruan TensorFlow masih terus dikembangkan. Hal ini dibuktikan dengan rilisnya versi 2.16.1 pada 8 Maret 2024. Tentunya dengan pembaruan ini membuat para praktisi machine learning tidak perlu merasa takut dengan framework yang deprecated sehingga kreatifitas developer bisa tetap tersalurkan..

Kelebihan di atas merupakan highlight yang perlu kita perhatikan. Selain itu, masih ada banyak sekali kelebihan TensorFlow yang akan Anda rasakan ketika membangun model machine learning menggunakan framework tersebut. Sampai di sini tentunya Anda sudah paham alasan TensorFlow menjadi sangat populer di kalangan praktisi machine learning, ‘kan?  



Keras: 
Halo para pejuang praktisi machine learning! Sekarang saatnya kita mempelajari Keras sebagai salah satu library open-source yang sangat populer. Keras merupakan sebuah open-source high-level neural network yang dikembangkan oleh François Chollet pada tahun 2015. 

Keras adalah API untuk mengembangkan jaringan saraf tiruan. Dengan Keras, kita akan lebih mudah dalam membuat sebuah multi layer perceptron dan convolutional neural network karena kemampuannya untuk membangun dan mengelola model dengan pendekatan berbasis lapisan (layer modeling). Aplikasi dari Keras sangat luas dan memungkinkan kita untuk membangun jaringan saraf tiruan dengan tujuan klasifikasi gambar, pemrosesan bahasa alami, pengenalan suara, dan prediksi time series.

Komponen inti pembangun sebuah jaringan saraf tiruan dalam Keras adalah layer. Sebuah layer pada Keras, sama dengan sebuah layer pada MLP yang memiliki beberapa perceptron. Terdengar sangat kompleks, bukan? Namun, penggunaan Keras saat ini sangatlah mudah.

Pada pertengahan tahun 2017, Keras diadopsi dan bergabung dengan TensorFlow. Ini menjadi sebuah keuntungan bagi para praktisi machine learning karena integrasi tersebut membuat Keras lebih mudah diakses oleh pengguna cukup dengan memanggil fungsi tf.keras.module. 

Sebelumnya, kita sudah mendengar MLP menggunakan Keras. Jika Anda memiliki pertanyaan, “Apa itu MLP dan bagaimana cara menggunakannya?”, ini merupakan jalur yang tepat! Mari kita bahas bersama-sama untuk mengetahui jawaban pertanyaan tersebut.

Multi Layer Perceptron (MLP) adalah sebuah jaringan saraf yang terdiri dari satu layer input, satu atau lebih hidden layer, dan satu output layer. MLP yang memiliki banyak hidden layer disebut juga Deep Neural Network (DNN).

mnist = tf.keras.datasets.fashion_mnist
 
(x_train, y_train), (x_test, y_test) = mnist.load_data()
X_train, x_test = x_train / 255.0, x_test / 255.0
 
model = tf.keras.models.Sequentials([
    tf.keras.layers.Flatten(input_shape=(28, 28)),
    tf.keras.layers.Dense(512, activation=tf.nn.relu),
    tf.keras.layers.Dense(10, activation=tf.nn.softmax) #ada 10 kelas
])
model.compile(optimizer = tf.optimizers.Adam(),
                              loss = ‘sparse_categorical_crossentropy’,
                              Metrics = [‘accuracy’])
model.fit(x_train, y_train, epochs = 10)


Langkah pertama adalah kita perlu mempersiapkan data kemudian membaginya menjadi data latih dan data uji. Data fashion MNIST bisa kita dapatkan dengan mudah dari library datasets yang disediakan Keras. Nah, untuk kalian yang belum tahu, fashion MNIST ini merupakan dataset yang umum digunakan dalam tugas-tugas machine learning dan computer vision, terutama untuk klasifikasi gambar. Dataset ini memiliki struktur yang mirip dengan dataset MNIST yang terkenal namun berisi gambar-gambar item pakaian daripada angka tulisan tangan dan sudah disediakan oleh TensorFlow Dataset.

Dalam klasifikasi gambar, setiap piksel pada gambar memiliki nilai dari 0 sampai 255. Selanjutnya, perlu melakukan normalisasi dengan membagi setiap piksel pada gambar dengan 255. Dengan normalisasi, model machine learning dapat dilatih dengan lebih cepat, stabil, dan akurat, sehingga meningkatkan performa dan generalisasi model pada data baru

Pada langkah berikutnya, definisikan arsitektur dari jaringan saraf yang akan kita latih. Untuk membuat sebuah MLP, kita hanya perlu mendefinisikan sebuah input layer, hidden layer, dan sebuah output layer. 

Untuk membuat sebuah model MLP di Keras, kita bisa memanggil fungsi tf.keras.models.Sequential([...]) dan menampungnya pada sebuah variabel. Model sequential pada keras adalah tumpukan layer-layer yang sama seperti pada sebuah MLP. Kode tersebut dapat ditulis seperti berikut.

- Input Layer
Layer yang memiliki parameter ‘input_shape’. Input_shape sendiri adalah resolusi dari gambar-gambar pada data latih. Dalam hal ini, sebuah gambar MNIST memiliki resolusi 28x28 piksel sehingga input shape-nya adalah (28, 28). Sebuah layer Flatten pada Keras akan berfungsi untuk meratakan input. Meratakan di sini artinya mengubah gambar yang merupakan matriks 2 dimensi menjadi larik/array 1 dimensi. Pada kasus kita, sebuah gambar MNIST yang merupakan matriks 28x 28 elemen akan diubah menjadi array satu dimensi sebesar 784 elemen

- Hidden Layer
Dense layer pada Keras merupakan layer yang dapat dipakai sebagai hidden layer dan output layer pada sebuah MLP. Parameter unit merupakan jumlah perceptron pada sebuah layer. Kita dapat menggunakan fungsi aktivasi rectified linear unit (Relu) atau fungsi aktivasi lainnya pada layer ini.

- Output layer
Layer ini dapat didefinisikan dengan membuat sebuah Dense layer. Jumlah unit menyesuaikan dengan jumlah label pada dataset. Untuk fungsi aktivasi pada layer output, gunakan fungsi aktivasi Sigmoid jika hanya terdapat 2 kelas/label pada dataset. Untuk dataset yang memiliki 3 kelas atau lebih, gunakan fungsi aktivasi Softmax. Fungsi aktivasi softmax akan memilih kelas mana yang memiliki probabilitas tertinggi. Untuk data fashion MNIST, kita akan menggunakan fungsi aktivasi softmax karena terdapat 10 kelas.

Sampai sini, tentunya Anda sudah dapat membuat arsitektur dari MLP, tetapi model kita belum bisa melakukan tugasnya yaitu memprediksi. Agar model dapat belajar dan memprediksi, kita perlu memanggil fungsi compile pada model tersebut dan menambahkan optimizer dan loss function. 

Untuk optimizer, kita bisa menggunakan Adam seperti yang sudah dijelaskan pada modul sebelumnya. Selanjutnya untuk loss function, kita dapat menggunakan sparse categorical entropy pada kasus klasifikasi 3 kelas atau lebih. Untuk masalah 2 kelas, loss function yang lebih tepat adalah binary cross entropy. Parameter metrics berfungsi untuk menampilkan metrik yang dipilih pada proses pelatihan model.

Setelah membuat arsitektur MLP dan menentukan optimizer serta loss functionnya, kita dapat melatih model kita pada data training. Parameter epoch merupakan jumlah berapa kali sebuah model melakukan propagasi balik (back propagation).



Pra-Pemrosesan Data Untukk Model:
Tahap pemrosesan data merupakan perubahan dari data mentah yang dibersihkan dan diatur untuk tahap pemrosesan berikutnya. Selama tahapan pemrosesan data, data mentah harus diperiksa dengan sangat cermat agar kita dapat memahami karakteristik dataset yang akan dilatih.

Dataset untuk setiap masalah tentunya berbeda. Contohnya pada dataset untuk klasifikasi gambar dan dataset untuk pemrosesan bahasa alami atau NLP. Pada dataset untuk klasifikasi gambar, sampelnya berupa kumpulan gambar atau matriks hasil representasi sebuah gambar seperti gambar berikut. 

Berbeda dengan dataset NLP, sampelnya tentu berupa kalimat-kalimat yang terdiri dari sejumlah kata tertentu seperti gambar di bawah.

Tidak seperti manusia yang bisa mengenali gambar atau memahami kalimat secara langsung, kita perlu melakukan pemrosesan dahulu agar data tersebut siap diterima oleh model. 

Secara umum, ada empat tahapan yang perlu kita lakukan dalam memproses data.

1. Ubah dataset ke dalam bentuk larik/array. Yup, larik berisi angka-angka adalah format data yang dapat diterima oleh model kita. Sama seperti pada kelas Belajar Machine Learning untuk Pemula, model kita menerima gambar sebagai matriks atau larik 2 dimensi.
2. Pisahkan atribut dan label pada data. Model kita akan mempelajari korelasi antara atribut dan label pada dataset.
3. Ubah skala data dalam skala yang seragam. Nama teknik ini adalah normalization. Normalization dilakukan karena NN akan memproses nilai yang berada di antara 0 dan 1  sehingga membuat komputasi lebih optimal. Teknik ini dibutuhkan sehingga komputasi lebih optimal.
4. Terakhir, pisahkan dataset ke dalam data latih dan data uji. Betul, kita memerlukan data uji untuk mengevaluasi kinerja dari model yang telah kita latih.


Pemrosesan Data Gambar:
Kita akan menerapkan data augmentation untuk data latih dan data validasi agar dataset yang Anda miliki dapat diterima dengan baik oleh model sebelum pelatihan dimulai. Pada tahap ini, Anda memiliki dua opsi yang bisa digunakan untuk melakukan augmentasi, kondisi ini menyesuaikan dengan versi TensorFlow yang Anda instal pada komputer atau environment Anda. 

Kasus pertama jika menggunakan TensorFlow versi <= 2.9, Anda dapat melakukan augmentasi menggunakan fungsi ImageDataGenerator untuk data latih dan data validasi. Saat ini, peringatan penghentian penggunaan akan muncul ketika Anda menggunakan fungsi tersebut.

Namun, untuk kasus kedua ketika menggunakan TensorFlow versi > 2.9, Anda dapat menggunakan tf.data dan layer augmentasi yang secara langsung dapat dimasukkan pada jajaran layer sequentials Anda.

Sampai di sini sudah mulai cukup menantang, ‘kan? Jangan risau terlebih dahulu, mari kita bahas bersama-sama implementasi dari kedua kasus di atas agar data yang kita miliki dapat melakukan proses pelatihan dengan baik.


Data Augmentasi dengan Tensorflow <= 2.9
Seperti yang sudah Anda pelajari sebelumnya, pada kasus ini kita dapat menggunakan fungsi ImageDataGenerator. ImageDataGenerator merupakan sebuah fungsi yang sangat berguna untuk mempersiapkan data latih dan data validasi. Beberapa kemudahan yang disediakan ImageDataGenerator antara lain, preprocessing data, pelabelan sampel otomatis, dan augmentasi gambar.

Augmentasi gambar merupakan sebuah teknik yang dapat digunakan untuk memperbanyak data latih dengan cara menduplikasi gambar yang telah ada dengan menambahkan variasi tertentu seperti rescale, rotation, zoom, dan lain sebagainya. Anda juga dapat melihat detail mengenai augmentasi gambar menggunakan ImageDataGenerator pada tautan di sini.

Kode berikut menunjukkan proses augmentasi gambar pada setiap sampel di dataset.

from tensorflow.keras.preprocessing.image import ImageDataGenerator
train_datagen = ImageDataGenerator(
                    rescale=1./255,
                    rotation_range=20,
                    horizontal_flip=True,
                    shear_range = 0.2,
                    fill_mode = 'nearest')
 
test_datagen = ImageDataGenerator(
                    rescale=1./255)

Fungsi di atas berguna untuk memperbanyak data latih dengan mengubah kondisi gambar menyesuaikan dengan layer yang digunakan, berikut hasil gambar yang telah dilakukan augmentasi.

Terlihat sangat mudah, bukan? Walaupun terlihat mudah tetapi fungsi ini berguna untuk meningkatkan keberagaman data pelatihan dengan cara membuat variasi dari data yang ada. Tujuan utamanya adalah untuk meningkatkan kinerja model dengan mengurangi overfitting dan meningkatkan generalisasi.


Data Augmentasi dengan TensorFlow > 2.9
Setelah Anda mempelajari penggunaan data augmentasi menggunakan versi lama, tahapan ini mungkin terasa useless karena Anda sudah dapat menjalankan augmentasi dengan lancar dan mudah. Namun, metode kedua ini akan sangat berguna ketika perusahaan atau proyek Anda memiliki kebutuhan untuk menggunakan library terbaru. Selain itu, kasus ini juga akan membantu Anda ketika TensorFlow versi lama sudah deprecated dan tidak bisa digunakan lagi.

Dengan menggunakan TensorFlow versi > 2.9, Anda dapat mengubah ukuran gambar dan piksel secara eksplisit dengan memanfaatkan layers sequential seperti kode berikut.

IMG_SIZE = 180
 
resize_and_rescale = tf.keras.Sequential([
  layers.Resizing(IMG_SIZE, IMG_SIZE),
  layers.Rescaling(1./255)
])

Selanjutnya, Anda harus melakukan data augmentasi menggunakan berbagai macam fungsi augmentasi, seperti tf.keras.layers.Resizing, tf.keras.layers.Rescaling, tf.keras.layers.RandomFlip, tf.keras.layers.RandomRotation, dan lain sebagainya. Penggunaan fungsi menyesuaikan kebutuhan dan karakteristik dataset yang Anda miliki ketika membangun sebuah model machine learning. Berikut penggunaan beberapa layer augmentasi dengan menggunakan tf.keras.layers.

data_augmentation = tf.keras.Sequential([
  layers.RandomFlip("horizontal_and_vertical"),
  layers.RandomRotation(0.2),
])

Pada tahap terakhir, Anda perlu memasukkan data augmentasi tersebut pada layer sequentials ketika membangun struktur neural network agar seluruh dataset yang ada dapat diproses dan melakukan pelatihan dengan baik. Berikut contoh kode yang dapat Anda lakukan untuk menerapkan data augmentasi.

model = tf.keras.Sequential([
  # Menambahkan processing image yang telah didefinisikan sebelumnya
  resize_and_rescale,
  data_augmentation,
  layers.Conv2D(16, 3, padding='same', activation='relu'),
  layers.MaxPooling2D(),
  # Sesuaikan sisa layer dengan kasus yang Anda miliki
])

Pada kasus ini, ada tiga hal penting yang perlu Anda perhatikan ketika membangun model neural network menggunakan TensorFlow versi > 2.9.
1. Data augmentasi akan berjalan pada device yang digunakan, baik itu lokal maupun cloud environment seperti Google Colab atau Kaggle Kernel. Proses ini akan berjalan secara bersamaan dengan eksekusi layer lainnya. Hal ini menyebabkan komputasi yang dilakukan akan lebih berat sehingga penggunaan GPU akan lebih menguntungkan.

2. Ketika Anda mengekspor model menggunakan model.save, layer processing ini akan disimpan bersama dengan layer lainnya. Jika Anda nantinya menggunakan model ini, model ini akan secara otomatis menstandarkan gambar (sesuai dengan konfigurasi layer Anda). Hal ini dapat membantu Anda untuk mengimplementasikan ulang logika tersebut di sisi server dengan lebih mudah.

3. Data augmentasi tidak akan aktif pada saat pengujian sehingga gambar input hanya akan ditambah selama pemanggilan Model.fit (bukan Model.evaluate atau Model.predict).


Dengan menggunakan kasus kedua ini, komputasi yang dilakukan akan lebih berat karena berjalan bersamaan dengan layer lainnya ketika proses pelatihan dijalankan. Namun, Anda tidak perlu khawatir karena ini terjadi hanya pada saat proses pelatihan saja. 

Sampai di sini, Anda sudah dapat melakukan augmentasi terhadap data dengan tipe gambar (unstructured data) sehingga data yang Anda miliki sudah dapat diterima dengan baik oleh model yang akan dibangun. Selanjutnya, Anda akan mempelajari pemrosesan data bahasa agar dapat membuat model dengan tipe lainnya. Silakan rehat sejenak dan mengambil secangkir kopi untuk menemani perjalanan indah yang akan Anda jalani pada tahap berikutnya, ya!


Pemrosesan Data Bahasa:
Halo pelopor machine learning masa depan! Apakah Anda sudah menyiapkan secangkir kopi dan semangat yang membara untuk mengarungi perjalanan terakhir kita pada materi ini? Tarik jangkar dan mari kita berlayar!

Pada kasus ini, kita akan belajar pemrosesan bahasa manusia. Berbeda dengan pemrosesan gambar yang telah kita pelajari, pemrosesan teks memiliki tantangan tersendiri, seperti perbedaan panjang teks, bahasa, serta bagaimana merepresentasikan teks ke dalam format yang dapat diterima oleh sebuah model.

Mungkin tebersit di benak Anda sebuah pertanyaan, “Bagaimana komputer memproses sebuah kalimat?” Jika Pertanyaan tersebut muncul, Anda berada pada jalur yang tepat! Berikut garis besar cara komputer untuk memproses sebuah kalimat.

Gambar di atas merepresentasikan langkah umum pemrosesan data yang telah kita bahas sebelumnya. Hal ini karena komputer memiliki sifat tidak seperti manusia yang bisa mengenali gambar atau memahami kalimat secara langsung, kita perlu melakukan pemrosesan dahulu agar data tersebut siap diterima oleh model. Sekarang saatnya kita mempelajari tahapan untuk mengubah kata atau kalimat menjadi angka atau larik sehingga dapat diterima oleh model neural network.

Tanpa tanpa berlama-lama lagi, mari kita langsung bahas bersama pemrosesan data yang perlu dilakukan untuk data bahasa alami pada kode berikut.

sentences = ["I love my cat"]
 
tokenizer = Tokenizer(num_words = 100)
tokenizer.fit_on_texts(sentences)
sequences = tokenizer.texts_to_sequences(sentence)
 
print(tokenizer.word_index)
print(sequences)


Output
{'i': 1, 'love': 2, 'my': 3, 'cat': 4}
[[1, 2, 3, 4]]

Dengan menggunakan kode di atas, Anda sudah dapat mengubah kalimat menjadi sebuah larik/array yang dapat digunakan untuk proses pelatihan model neural network. Tokenizer digunakan untuk membangun indeks kata dari korpus teks. Ini membuat kamus dari kata-kata yang ada dalam teks dan memberi indeks unik untuk setiap kata. Sedangkan texts_to_sequences bertugas untuk mengonversi teks menjadi urutan indeks numerik berdasarkan kamus yang dibuat.

Namun, kondisi tersebut masih ringkih dan belum mendapatkan performa yang cukup baik karena bentuk dari data yang Anda miliki masih beragam dan terbatas pada kata-kata yang tidak termasuk dalam dataset. 

Untuk mengatasi permasalahan tersebut, kita dapat menggunakan fungsi oov_token dan padding agar dataset yang Anda miliki lebih baik karena memiliki shape yang serupa. 

Pada kasus ini, oov_token berguna untuk mengatasi kata yang tidak termasuk pada tokenizer sehingga akan diubah menjadi special value yang dapat kita tentukan sendiri. Oleh karena itu, model dapat melakukan perhitungan ketika menemukan kata yang tidak terlihat. Lalu, padding berguna untuk menyesuaikan semua kamus dalam satu urutan yang serupa. Jadi, semua urutan memiliki panjang yang sama. Berikut merupakan contoh implementasi penggunaan oov_token dan padding secara bersamaan.

sentences = ["I love my cat",
             "Do you think my cat is cute?"]
 
 
tokenizer = Tokenizer(num_words = 100, oov_token="<OOV>")
tokenizer.fit_on_texts(sentences)
sentences = ["I love my cat",
             "Do you think my cat is cute?",
             "Additional cat for you"]
sequences = tokenizer.texts_to_sequences(sentences)
 
padded = pad_sequences(sequences, padding="post", truncating="post", maxlen=10)
 
print("Tokenizer: ",tokenizer.word_index)
print("Sequences: ",sequences)
print("Padded: ",padded)
 

Output
Tokenizer:  {'<OOV>': 1, 'my': 2, 'cat': 3, 'i': 4, 'love': 5, 'do': 6, 'you': 7, 'think': 8, 'is': 9, 'cute': 10}
Sequences:  [[4, 5, 2, 3], [6, 7, 8, 2, 3, 9, 10], [1, 3, 1, 7]]
Padded:  [[ 4  5  2  3  0  0  0  0  0  0]
 [ 6  7  8  2  3  9 10  0  0  0]
 [ 1  3  1  7  0  0  0  0  0  0]]


Seperti yang Anda lihat pada kode di atas, dengan menggunakan oov_token dan padding, kita akan mendapatkan sebuah larik/array yang memiliki ukuran sama sehingga memberikan performa yang lebih baik ketika pelatihan model dijalankan. Hal ini dikarenakan padding merupakan teknik yang digunakan untuk memastikan bahwa semua urutan teks (sequences) memiliki panjang yang sama. Ini penting karena sebagian besar model machine learning, terutama neural networks, mengharuskan input mereka memiliki ukuran yang konsisten.

Sampai pada tahap ini, Anda sudah memiliki sebuah data yang siap untuk melakukan pelatihan dan dapat diterima dengan baik oleh layer embedding. Ngomong-ngomong, layer embedding ini berguna untuk mengubah larik menjadi representasi kata yang memungkinkan kata-kata dengan arti yang sama memiliki representasi yang sama. Sedikit bocoran layer embedding ini akan mengubah larik/array menjadi seperti berikut.



Menggunakan Model Untuk Melakukan Prediksi:
Sama seperti model ML yang disediakan pada library Scikit Learn, mekanisme kerja dari model library TensorFlow dan Keras API juga sama. Yup, untuk melatih model, kita hanya perlu memanggil fungsi fit() dan mengisi parameter atribut dan label pada dataset serta jumlah epoch yang harus dilakukan. 

Nantinya, Anda akan menjadi seorang praktisi machine learning andal yang dapat membangun segala macam model neural network di berbagai industri atau perusahaan kalian kelak. Namun, sekarang kita akan mempelajari pembuatan model neural network atau jaringan saraf tiruan dasar sehingga Anda dapat melakukan perubahan atau penyesuaian dengan lebih andal pada kasus yang Anda hadapi kelak.

Bayangkan ketika Anda mempelajari fungsi matematika pada bangku sekolah sekitar kelas 8 atau 9. Tentunya Anda akan sangat mudah ketika menyelesaikan sebuah fungsi(x) = x + 9


x = -4.0, -3.0, -2.0, -1.0, 0.0, 1.0, 2.0, 3.0, 4.0, 5.0
f(x) = 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0


Sebuah model dari API Keras dapat menerima masukan dengan tipe data numpy array sehingga kita bisa membuat 2 buah objek bertipe numpy array, satu untuk atribut (x) dan satu lagi sebagai labelnya (f(x)). Untuk jenis tipe data lain yang dapat diterima sebagai masukan sebuah model dari API Keras, Anda dapat mengunjungi tautan berikut. 

Mari kita mulai dengan melakukan impor library yang akan digunakan seperti kode di bawah ini.

import tensorflow as tf
import numpy as np
from tensorflow import keras

Selanjutnya, kita harus mengubah angka-angka di atas menjadi sebuah larik/array agar dapat diterima oleh Keras sehingga proses pelatihan akan berjalan dengan baik. Kita dapat menggunakan kode berikut untuk mengubahnya menjadi sebuah larik/array.

X = np.array([-4.0, -3.0, -2.0, -1.0, 0.0, 1.0, 2.0, 3.0, 4.0, 5.0], dtype=float)
Y = np.array([5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0], dtype=float)

Kemudian, kita buat model neural network atau jaringan saraf tiruan (JST) dengan memanggil fungsi tf.keras.Sequential(). Sequential adalah model JST yang paling sederhana dan telah kita pelajari sebelumnya. Pada model sequential, setiap layer pada jaringan saraf tiruan terhubung secara sekuensial. 

model = tf.keras.Sequential([
    tf.keras.layers.Dense(units=1, input_shape=[1])
    ])

Pada model sequential di atas, kita akan membuat sebuah model dengan jumlah layer yang dapat diatur menyesuaikan dengan kebutuhan dan data yang digunakan. Untuk membuat sebuah layer, kita dapat menggunakan fungsi keras.layers.Dense(). Sekadar informasi, jumlah layer yang Anda buat nantinya perlu melewati tahapan trial and error karena tidak ada petunjuk baku mengenai penggunaan layer beserta parameternya.

Loh, apa itu parameter? Pada kode di atas, terdapat parameter units dan input_shape, mari kita bahas kedua parameter tersebut.

- Parameter units dari fungsi keras.layers.Dense() adalah jumlah perceptron yang dimiliki oleh layer tersebut. Yang perlu diperhatikan pada model sequential adalah layer pertama dari model tersebut haruslah memiliki parameter input_shape agar model bisa mengenali bentuk input yang akan diprosesnya.
- Parameter input_shape menunjukkan bentuk dari setiap elemen input yang akan diterima oleh model. Pada kasus yang kita alami, setiap elemen dari data adalah sebuah bilangan numerik 1 digit sehingga input_shape dapat diisi dengan angka 1. Jika sebuah elemen dari dataset kita berupa gambar yang memiliki dimensi 32*32 piksel, input_shape yang sesuai adalah [32,32].


Kemudian, hal yang paling penting selanjutnya adalah menentukan optimizer dan loss dari model agar model kita bisa melakukan pelatihan. Untuk menentukan optimizer dan loss, kita menggunakan fungsi compile, sedangkan untuk regresi yang sederhana, kita dapat menggunakan stochastic gradient descent sebagai optimizer, dan mean squared error sebagai loss function model.

Jadi rumusnya compile:

1. Klasifikasi
- Adam
- accuracy, cross entropy loss

2. Regresi
- stochastic gradient descent
- mse,

Istilah "stochastic" dalam SGD merujuk pada sifat acak dari algoritma tersebut. Kehadiran ketidakpastian ini disebabkan oleh penggunaan mini-batch. Dengan melatih subset data yang berbeda pada setiap iterasi, langkah-langkah SGD melalui perhitungan komputasi menjadi tidak terduga, dengan menambahkan unsur keacakan sehingga membantu model keluar dari loss minimum lokal. Karakteristik ini membuat SGD sangat efektif dalam mengoptimalkan fungsi yang tidak cembung dan memiliki dimensi tinggi yang sering ditemui dalam domain pembelajaran mesin. Fungsi akhir dari optimizers ini untuk menemukan nilai loss terkecil secara keseluruhan dari proses pelatihan yang dijalankan sebanyak x iterasi.

Setelah kita mengetahui salah satu optimizers SGD, tidak afdal jika kita tidak mempelajari mean square error pada loss function agar semakin lihai dalam pengembangan jaringan saraf tiruan.

Fungsi kerugian (loss function) adalah sebuah metrik yang digunakan dalam machine learning untuk mengevaluasi seberapa baik model memprediksi target pada data pelatihan. Fungsi ini mengukur seberapa jauh prediksi model dari nilai sebenarnya yang diharapkan. Tujuan utama dari fungsi kerugian adalah untuk memberikan umpan balik kepada model tentang seberapa baik atau buruk performanya sehingga model dapat disesuaikan selama proses pelatihan untuk meminimalkan kesalahan prediksi.

Fungsi kerugian umumnya disesuaikan berdasarkan jenis masalah yang sedang dihadapi. Beberapa contoh fungsi kerugian termasuk mean squared error (MSE) untuk masalah regresi, Cross-Entropy Loss (atau Log Loss) untuk klasifikasi biner, dan Categorical Cross-Entropy untuk klasifikasi multi kelas. Pilihan fungsi kerugian yang tepat sangat penting karena dapat memengaruhi performa dan konvergensi model.

Pada kasus regresi yang sedang dihadapi, kita dapat menggunakan fungsi mean square error sebagai loss functionnya. Mean squared error (MSE) adalah salah satu metrik yang umum digunakan dalam kasus regresi untuk mengevaluasi seberapa baik model memprediksi nilai kontinu. MSE mengukur rata-rata dari kuadrat selisih antara prediksi model dengan nilai sebenarnya yang diharapkan. Semakin kecil nilai MSE, semakin baik model dalam memprediksi data. Secara matematis, MSE dapat dihitung dengan rumus berikut.

Untuk mengimplementasikan optimizers dan loss function menggunakan TensorFlow, Anda hanya memerlukan sebuah fungsi sederhana yaitu model.compile(). Selain itu, Anda juga perlu memasukkan metode yang akan digunakan, seperti contoh pada kasus ini adalah SGD dan MSE

model.compile(optimizer='sgd', loss='mean_squared_error')

Last but not least, setelah perjalanan panjang, akhirnya kita perlu memanggil sebuah fungsi yang paling terkenal dari machine learning yaitu fit(). Fungsi fit() memungkinkan kita menyuruh model untuk mempelajari hubungan antara atribut dan label pada dataset. 

Selain atribut dan label, parameter lain yang diperlukan sebuah model Keras pada fit adalah epochs. Epochs adalah berapa kali sebuah model neural network harus belajar memperbaiki akurasinya. Kita dapat menggunakan fungsi tersebut dengan kode di bawah ini.

model.fit(X, y, epochs=520)

Jumlah epochs di atas dapat disesuaikan dengan kebutuhan dan kasus yang kita miliki. Semakin banyak epochs, belum tentu menghasilkan loss lebih kecil. Pada beberapa kasus, jumlah epochs yang besar bisa menyebabkan loss tidak mengalami perubahan (stagnan) atau bahkan menyebabkan loss naik kembali. Tentunya, Anda perlu melakukan trial and error untuk memperhatikan loss yang didapatkan ketika melakukan pelatihan. 

Setelah menjalankan fungsi tersebut, dapat Anda lihat bahwa pada setiap epoch yang baru akan memiliki nilai error yang semakin menurun pada kasus regresi ini.

Ketika model telah dilatih, selanjutnya kita dapat menggunakan model tersebut untuk memprediksi data yang belum pernah dilihatnya menggunakan fungsi predict. Hal ini merupakan salah satu tahap paling penting ketika kita membangun sebuah model machine learning. Karena sejatinya, machine learning dibangun untuk melakukan tugas yang telah kita tentukan sebelumnya.

Seperti pada gambar di atas, kita akan melakukan prediksi untuk menghasilkan sebuah nilai sehingga dapat menghitung kualitas model yang telah kita bangun. Anda dapat melakukan prediksi dengan menggunakan kode berikut.

model.predict([4, 5])
Ketika kita menjalankan kode di atas, hasil yang kita dapat seharusnya adalah 13 dan 14 di mana f(4) = 4 + 9 dan f(5) = 5 + 9. Namun, ketika kita menjalankan kode tersebut, output yang dihasilkan adalah sebagai berikut.

1/1 [==============================] - 0s 40ms/step
array([[12.999903],
       [13.999887]], dtype=float32)
Jika Anda amati secara saksama, nilai yang dihasilkan ketika menjalankan fungsi prediksi hanya mendekati angka 13 dan 14 tidak 100% presisi. Kenapa demikian? Karena neural  network tidak memprediksi kepastian melainkan menghitung probabilitas yang terjadi. Pada kasus kita, neural network mempelajari bahwa pola yang terdapat pada dataset kemungkinan adalah x + 9 sehingga prediksi yang dihasilkan adalah probabilitas yang mendekati angka 13 dan 14. Semakin banyak data yang dilatih maka tingkat error training juga semakin rendah sehingga prediksi dari neural network akan semakin mendekati 13 dan 14.

Karena kita memiliki nilai loss yang sangat kecil, Anda dapat menggunakan fungsi .round() agar mengembalikan nilai dengan lebih presisi. Namun, tentunya ini akan menjadi resiko ketika kita memiliki loss function yang besar karena akan menjadi noise pada hasil prediksi yang kita lakukan sehingga hasilnya akan mengembalikan nilai sesuai dengan input yang sebelumnya kita latih.


model.predict([4, 5]).round()

Output
1/1 [==============================] - 0s 41ms/step
array([[13.],
       [14.]], dtype=float32)




Model Sekuensial dengan Beberapa Layer:
Dengan menghadapi kasus yang ada di industri, sangat minim kemungkinannya kita dapat menyelesaikan masalah tersebut hanya dengan satu layer. Sebetulnya, tidak ada panduan khusus untuk menentukan jumlah layer beserta parameter yang digunakan ketika membangun sebuah model neural network. Sekali lagi, sebagai seorang praktisi machine learning, Anda perlu melakukan trial and error untuk meningkatkan intuisi sehingga kelak tidak perlu lagi melakukan ribuan percobaan pada kasus serupa.

Sekarang mari kita bayangkan jika Anda bekerja di sebuah instansi kesehatan dan perlu membangun sebuah model yang dapat melakukan prediksi tumor otak. Pada kasus ini, kita memiliki 15.000 data gambar yang perlu kita latih agar dapat melakukan klasifikasi kondisi kesehatan otak. Tentunya, semakin banyak data yang kita miliki, semakin baik model yang akan dibangun

Jika model yang kita buat berdasarkan data kompleks seperti di atas hanya memiliki satu layer, alih-alih akurasi mendekati 99%, justru kesalahan yang kita dapatkan yang akan mendekati 99%. Di sinilah kita harus paham cara menggunakan multiple layer ketika mengembangkan model neural network. Dengan menggunakan jumlah layer yang lebih banyak, komputasi yang dilakukan oleh komputer akan lebih kompleks, tetapi sebanding dengan hasil yang didapatkan. Secara teori, semakin banyak layer yang digunakan, semakin detail juga perhitungan yang dilakukan oleh komputer.

Agar lebih memahami perbedaan jumlah layer tersebut, mari kita lihat visualisasi yang dapat merepresentasikan komputasi yang harus dilakukan oleh komputer dengan jumlah layer lebih dari satu.

Seperti yang Anda lihat pada gambar di atas, tentunya perhitungan matematis yang dilakukan oleh komputer akan lebih banyak. Hal ini menyebabkan proses pelatihan lebih lama, tetapi model yang kita bangun akan memiliki performa yang lebih baik. Ada satu hal yang perlu Anda ketahui, jumlah hidden layer pada pembangunan model neural network di atas bukanlah acuan pasti yang harus Anda ikuti. Anda dapat mengurangi dan menambahkan layer sesuai dengan kebutuhan dan performa dari model neural network yang dibangun.

Untungnya, sangatlah mudah untuk menambahkan layer pada model karena kita cukup menambahkan fungsi Dense sesuai jumlah layer yang diinginkan. Selanjutnya, mari kita tarik kembali contoh pembuatan model neural network pada materi sebelumnya. Kita akan memodifikasi layer tersebut dengan jumlah yang lebih banyak. 

Pada kode di bawah, kita memanggil fungsi dense sebanyak empat kali yang menunjukkan bahwa model kita memiliki empat buah layer. Layer pertama hanya memiliki 20 buah perceptron, layer kedua memiliki 15 buah perceptron, layer ketiga memiliki tiga buah perceptron, dan layer terakhir memiliki satu buah perceptron. Ingat, layer pertama dari model sequential harus memiliki parameter input_shape agar model bisa mengenali bentuk input yang akan diterimanya.

model = tf.keras.Sequential([
    tf.keras.layers.Dense(units=20, input_shape=[1]),
    tf.keras.layers.Dense(units=15),
    tf.keras.layers.Dense(units=10),
    tf.keras.layers.Dense(units=1)
    ])



Membuat Model Klasifikasi Dua Kelas:
Namun, rasanya masih ada satu hal yang kurang agar pengalaman belajar Anda lebih maksimal, yakni membuat model neural network dari awal hingga dapat melakukan inference.

Pada modul sebelumnya, kita telah membahas proses pengembangan model neural network untuk mengatasi permasalahan regresi sederhana. Selanjutnya, sudah waktunya kita akan lanjut mengembangkan model neural network untuk melakukan klasifikasi biner.

Mari kita mulai dengan sebuah permasalahan sederhana, bagaimana cara komputer mengklasifikasikan buah jeruk dan anggur? Tentunya komputer tidak memiliki kemampuan untuk melakukan tugas tersebut tanpa adanya proses pelatihan. Oleh karena itu, yuk, kita buat sebuah sistem yang dapat melakukan tugas tersebut.

Bayangkan ketika Anda memiliki jumlah label yang sangat banyak mungkin 30 label, 50 label, dan seterusnya. Hal tersebut akan memakan banyak tenaga dan waktu hanya untuk mengubah label menjadi data numerik, bukan? Oleh karena itu, kita dapat menggunakan opsi lainnya yaitu menggunakan fungsi LabelEncoder yang disediakan oleh sklearn. Fungsi ini memiliki kemampuan yang sama yaitu untuk mengubah data kategori menjadi numerik berdasarkan urutan abjad. 

from sklearn import preprocessing
 
label_encoder = preprocessing.LabelEncoder() 
df['name'] = label_encoder.fit_transform(df['name']) 
print(df)

Kini dataset di atas sudah memiliki enam buah kolom yang bertipe numerik. Seru, ya? Sayangnya sampai pada tahap ini, model kita belum dapat memproses dataset ini karena dataset masih dalam bentuk dataframe. Betul, dataset harus dalam bentuk array agar dapat diproses oleh model. Nah untungnya kita dapat melakukan ini dengan mudah menggunakan atribut values dari dataframe. Values mengembalikan numpy array yang dikonversi dari dataframe.

X = dataset[:,1:6]


artinya X atau atribut independen merupakan semua baris namun pada kolom indeks ke-1 sampai ke-6.

Sampai di sini, sebetulnya Anda dapat melanjutkan ke tahap selanjutnya, yaitu pembagian dataset menjadi data pelatihan dan data testing. Namun, dataset yang kita gunakan masih memiliki rentang yang cukup besar pada masing-masing kolomnya. Dalam hal ini, fitur dengan rentang nilai yang lebih besar dapat mendominasi perhitungan sehingga normalisasi diperlukan untuk menyeimbangkan pengaruh setiap fitur dalam model. 

Salah satu cara yang sangat penting dilakukan agar model neural network dapat mempelajari dataset dengan baik adalah melakukan normalisasi. Kita dapat menggunakan fungsi fit_transform() dari sebuah objek MinMaxScaler dari library preprocessing SKLearn untuk menormalisasi data kita. Hal ini akan mengubah seluruh data yang dipanggil memiliki rentang 0-1 atau -1 sampai 1, Anda dapat melakukan normalisasi dengan menjalankan perintah berikut.


from keras.models import Sequential
from keras.layers import Dense

Masih ingatkan struktur membangun model neural network yang sudah kita pelajari bersama pada modul sebelumnya? Untuk model yang kita kembangkan adalah model sequential yang memiliki 3 buah layer seperti di bawah. Activation function pada 2 layer pertama yang dapat digunakan adalah relu untuk latihan ini. Anda dapat bereksplorasi menggunakan activation function lain. Untuk layer terakhir, isi parameter unit isi dengan 1 di mana output dari model neural network kita merupakan satu buah bilangan numerik. Activation function pada layer terakhir dipilih sigmoid karena sigmoid memetakan probabilitas dari 0 sampai 1. Sigmoid sangat cocok digunakan pada masalah klasifikasi biner.

model = Sequential([    
                    Dense(32, activation='relu', input_shape=(5,)),    
                    Dense(32, activation='relu'),    
                    Dense(1, activation='sigmoid')])

Kemudian setelah arsitektur dari model neural network sudah dibentuk, seperti biasa kita perlu menentukan optimizer dan loss function untuk menyelesaikan model ini. Untuk optimizer, kita akan menggunakan stochastic gradient descent (SGD) yang merupakan optimizer yang sangat umum dan cocok digunakan pada dataset yang berukuran kecil. Lalu, untuk loss yang sesuai adalah ‘binary_crossentropy’ karena masalah pada latihan kita kali ini adalah 2 kelas (biner). Selain itu, jika kita ingin menampilkan akurasi pada setiap proses pelatihan model, kita dapat menambahkan parameter metrics dan mengisinya dengan string ‘accuracy’.

Untuk melihat performa model pada data baru, Anda perlu melakukan benchmark supaya dapat melihat performa model apakah overfit, underfit, atau goodfit. Masih ingat pada tahap pembagian dataset? Kita membagi 30% data menjadi data testing yang berguna untuk melakukan evaluasi. Untuk melihat loss dan akurasi model pada data test, gunakan fungsi evaluate pada model. Fungsi Evaluate mengembalikan 2 nilai. Yang pertama adalah nilai loss, dan yang kedua adalah nilai akurasinya. 


model.evaluate(X_test, Y_test, batch_size=1)




Membuat dan Melatih Model untuk Klasifikasi Banyak Kelas (Multi Kelas):

Iris Classification

Masih ingatkan kalau sebuah model neural network tidak bisa memproses string sebagai kategori? Hal tersebut menyebabkan kita harus mengubah nilai pada kolom Species menjadi numerik terlebih dahulu agar bisa diproses oleh model neural network. Sebelumnya kita sudah mempelajari penggunaan LabelEncoder dari SKLearn dan transformasi secara manual, tentunya sangat membosankan jika menggunakan metode yang sama terus, bukan? 

Karena pada kasus ini label kita merupakan data kategorikal mari kita pelajari salah satu metode transformasi lainnya, yaitu one hot encoding menggunakan fungsi get_dummies(). Fungsi ini memungkinkan mengubah setiap variabel dalam menjadi angka sebanyak nilai yang berbeda. Kolom pada output masing-masing diberi nama sesuai dengan nilai. Jika inputnya berupa DataFrame, nama variabel asli akan ditambahkan ke nilai tersebut sebagai nama kolom. 

pd.get_dummies(df.Species, dtype=int)


Sesudah itu lakukan pembagian data menjadi data latih dan data uji. Tidak ada acuan baku untuk pembagian ukuran dataset ini. Tujuan dari pembagian dataset ini untuk membagi data menjadi dua bagian seperti namanya. Biasanya, data dipisahkan menjadi dua bagian, satu bagian digunakan untuk mengevaluasi atau menguji data dan bagian lainnya untuk melatih model. Namun, terdapat metode lain seperti membagi menjadi tiga bagian menjadi data latih, data uji, dan data validasi. Biasanya teknik ini disebut dengan cross validation. 

Untuk arsitektur model, kita kali ini menggunakan 3 buah layer. Activation function yang digunakan pada layer terakhir dipilih softmax karena activation tersebut umum dipakai untuk klasifikasi multi kelas seperti ini. Jika penasaran terkait softmax lebih detail, silakan membaca kembali pada modul sebelumnya, ya. 

model = Sequential([    
                    Dense(64, activation='relu', input_shape=(4,)),    
                    Dense(64, activation='relu'),    
                    Dense(3, activation='softmax')])

Lanjutkan dengan menentukan optimizer dan loss function dari model. Untuk masalah klasifikasi multi kelas, Anda dapat menggunakan loss ‘categorical_crossentropy’.

model.compile(optimizer='Adam',
              loss='categorical_crossentropy',
              metrics=['accuracy'])

categorical_crossentropy adalah fungsi kerugian (loss function) yang sering digunakan dalam tugas klasifikasi multikelas di mana label targetnya adalah dalam bentuk one-hot encoding. Fungsi ini umumnya digunakan dalam kasus di mana output dari model adalah distribusi probabilitas untuk setiap kelas. 

Tujuan dari categorical_crossentropy adalah untuk mengevaluasi seberapa baik model memprediksi distribusi probabilitas kelas yang benar. Dengan mengoptimalkan fungsi ini selama proses pelatihan menggunakan algoritma seperti Adam atau varian-modifikasi lainnya, model berusaha untuk membuat prediksi yang semakin mendekati distribusi probabilitas yang diharapkan.

Selain itu, categorical_crossentropy juga berguna karena memberikan umpan balik kepada model ketika ada perbedaan besar antara probabilitas prediksi dan probabilitas target, membantu model untuk mengoreksi diri dan meningkatkan performa klasifikasi multikelas.

Nah, pada latihan ini ada sedikit perubahan yang perlu kita pelajari lagi. Fungsi fit() sekarang kita tampung ke dalam objek hist (history). Untuk apa kita melakukan hal ini? Temukan jawabannya di materi berikutnya, ya.

hist = model.fit(X_train, Y_train, epochs=100)
Sampai di sini, Anda sudah berhasil membuat model untuk menyelesaikan kasus multi kelas. Seperti pada gambar di atas, akurasi yang kita dapatkan kurang lebih 0.98 (98%), tetapi ini dapat berbeda setiap kali melatih model seperti yang telah kita bahas sebelumnya. Jadi jangan risau, ya.

Terakhir, kita perlu menguji akurasi prediksi dari model yang telah dibangun menggunakan data uji yang belum pernah dilihat.

model.evaluate(X_test, Y_test, batch_size=1)
Hasil evaluasi yang kita lakukan ternyata mendapatkan akurasi yang dapat diandalkan karena mencapai 0.95 (95%) yang berarti kita berhasil memprediksi 43 dari 45 data dengan benar. Wow, angka yang sangat baik, bukan?




Plot Loss dan Akurasi dari Trained Model:
Di materi sebelumnya, Anda pasti penasaran dengan alasan menampung fungsi fit pada sebuah objek history. Jawabannya adalah karena kita dapat membuat plot dari akurasi dan loss model pada saat proses pelatihan. 

Jika Anda ingat pada latihan sebelumnya, kita perlu melihat performa model yang dibangun melalui teks yang memiliki banyak sekali parameter. Hal itu akan membebankan otak kita dengan banyak informasi karena harus membandingkan performa pada setiap epochs. Sangat melelahkan, bukan? Sebagai manusia, pikiran kita selalu mencari hal yang lebih mudah untuk dirangkai. Oleh karena itu, gambar merupakan media visual yang tepat karena sebuah gambar akan memiliki nilai setara dengan ribuan kata. 

Stimulus visual memiliki peran yang cukup penting dalam mengumpulkan informasi dengan lebih baik. Kemampuan yang dapat mempertahankan informasi ini disebut dengan kognisi visual. Dengan ini, pikiran kita akan lebih mudah mendapatkan informasi dari sebuah gambar.

Nah, dengan menggunakan plot akan sangat berguna untuk melihat proses keseluruhan pembelajaran model dengan lebih mudah. Alih-alih melihat setiap iterasi dan membandingkan secara manual, kita dapat membuat sebuah visualisasi dari performa model yang sudah dilatih. 

Untuk melihat bagaimana plot bekerja mari kita kerjakan latihan berikut. Kita akan menggunakan dataset dan model yang sama dengan latihan sebelumnya. Anda dapat menggunakan latihan sebelumnya dan menambahkan kode-kode berikut di bawah latihan sebelumnya.

import matplotlib.pyplot as plt
Matplotlib.pyplot merupakan sebuah modul dalam library Matplotlib yang menyediakan antarmuka untuk membuat visualisasi data dalam bentuk grafik atau plot. Pada latihan ini kita akan membuat sebuah plot dari objek history. Objek history menampung informasi dari akurasi dan loss model pada setiap epoch di proses pelatihan dengan tipe data dictionary sehingga kita bisa membuat plot akurasi atau loss. 

Masih ingatkan cara memanggil key pada dictionary? Untuk mengakses loss kita bisa memanggil fungsi history pada objek history dan menentukan metrik loss yang akan diambil.

plt.plot(hist.history['loss'])
plt.title('Model loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train'], loc='upper right')
plt.show()

Dari hasil plot di atas, Anda dapat melihat bahwa akurasi model stagnan pada epoch di sekitar 20 dan juga kembali stagnan pada epoch di atas 80. Dengan menyimpan informasi pada tahap pelatihan model, kita dapat melihat performa model dengan jauh lebih nyaman bukan? Mari bandingkan jika kita tidak menggunakan plot sebagai visualisasi performa model 



Mencegah Overfitting dengan Dropout dan Batch Normalization

Seperti yang telah dipelajari pada materi sebelumnya, masalah umum yang dihadapi model machine learning adalah overfitting. Masih ingat bukan tentang overfitting? Yup, overfitting adalah situasi ketika sebuah model memiliki performa yang bagus saat mengenali data latih, tetapi buruk saat mengenali data-data baru yang belum pernah ditemuinya.

Overfitting pada machine learning dapat terjadi ketika data pelatihan yang kita miliki terlalu sedikit dan tidak merepresentasikan semua kemungkinan dari nilai sesungguhnya. Kondisi lainnya adalah ketika kita memiliki data pelatihan yang sangat banyak, tetapi memiliki informasi yang tidak relevan dengan data di lapangan. Hal ini menyebabkan model harus mempelajari data yang kompleks tetapi tidak memiliki pola yang berguna.

Penyebab di atas juga berlaku pada jaringan saraf tiruan. Terus, apa solusinya dan bagaimana kita mengetahui kualitas dari model neural network? Tenang saja, Anda sudah berada di jalur yang tepat. Pada latihan sebelumnya, kita sudah memisahkan data latih dan data uji sehingga kualitas model melalui evaluasi yang sudah dilakukan dapat terlihat. Sebagai contoh perhatikan tabel berikut.

Model C adalah contoh kasus overfitting yang jelas, yaitu dengan akurasi pelatihan tinggi, tetapi akurasi pengujian rendah. Penting untuk mempertimbangkan pengetahuan tentang domain masalah dan karakteristik data untuk menilai tingkat overfitting yang dapat diterima. Sampai di sini cukup terbayang ‘kan untuk menentukan model kita overfitting atau tidak?

Sekarang mari kita bahas beberapa cara untuk mencegah overfitting pada model neural network. 


Dropout:
Salah satu cara mencegah overfitting adalah dengan menggunakan dropout. Dropout adalah fungsi standar yang umum digunakan industri untuk mencegah overfitting. Seperti yang kita ketahui, semakin kompleks sebuah model machine learning, semakin tinggi kemungkinan model tersebut mengalami overfitting. Dropout bekerja dengan cara mengurangi kompleksitas model neural network tanpa mengubah arsitektur model tersebut. 

Lalu, bagaimana cara dropout bekerja? Nama dropout mengacu pada unit/perceptron yang di-dropout (dibuang) secara temporer pada sebuah layer. Contohnya seperti gambar di bawah ini di mana besaran dropout yang dipilih adalah 0.5 sehingga 50% dari perceptron hidden layer kedua dimatikan secara berkala pada saat pelatihan. Dropout memilih neuron yang akan "dimatikan" secara acak selama setiap iterasi pelatihan.

Seperti yang Anda tahu, weight dari setiap neuron pada sebuah layer itu bersifat statis. Hal ini menyebabkan jaringan saraf terlalu menyesuaikan dengan karakteristik pada data latih sehingga menyebabkan overfitting. Nah, penerapan dropout akan membantu mengatasi permasalahan overfitting yang disebabkan oleh permasalahan tersebut.

Untuk mengimplementasikan dropout sendiri sangatlah mudah jika menggunakan Keras. Anda cukup menambahkan layer dropout pada hidden layer di model yang sedang dibuat. Agar lebih jelas perhatikan contoh berikut.

from keras.layers import Dense, Dropout
model = Sequential([
                    Dense(64, activation='relu', input_shape=(4,)),
                    Dense(64, activation='relu'),
                    Dropout(0,5),
                    Dense(3, activation='softmax')])

Untuk menggunakan dropout, kita cukup menambahkan fungsi layer tf.keras.layers.Dropout() dan mengisi parameter berupa persentase yang kita inginkan seperti di atas. Dropout akan otomatis menghilangkan jumlah neuron yang aktif pada layer sebelumnya. Sangat mudah, bukan


Batch Normalization:

BatchNormalization adalah layer yang digunakan dalam jaringan saraf tiruan (neural network) untuk mempercepat konvergensi pelatihan dan mengurangi sensitivitas terhadap inisialisasi parameter. Layer ini bekerja dengan melakukan normalisasi input dari layer sebelumnya pada setiap batch sehingga memungkinkan pelatihan yang lebih stabil dan cepat. 

Proses normalisasi dalam BatchNormalization dilakukan dengan mengurangi nilai rata-rata dari setiap batch dan mengukurnya nilai varian. Kemudian layer ini akan melakukan normalisasi pada data dan melakukan pergeseran selama pelatihan. Hal ini membantu dalam mengatasi masalah seperti gradien menghilang (vanishing gradient) dan memungkinkan mempercepat konvergensi pada model jaringan saraf.

Penggunaan BatchNormalization umum dalam pembangunan model jaringan saraf modern karena kemampuannya untuk meningkatkan stabilitas dan kinerja pembelajaran.

Untuk mengimplementasikan BatchNormalization sendiri sangatlah dengan menggunakan Keras. Anda cukup menambahkan layer BatchNormalization pada model neural network. Agar lebih jelas lihatlah contoh di bawah.

from keras.layers import Dense, Dropout, BatchNormalization
model = Sequential([
                    Dense(64, activation='relu', input_shape=(4,)),
                    Dense(64, activation='relu'),
                    Dropout(0,5),
                    BatchNormalization(momentum=0.99),
                    Dense(3, activation='softmax')])

Pada contoh di atas, kita menggunakan salah satu parameter yaitu momentum. Momentum pada BatchNormalization mengacu pada parameter yang dapat mengontrol perhitungan nilai rata-rata dan varians yang dihitung pada setiap batch selama proses pelatihan model. Dengan menggunakan momentum, nilai rata-rata dan varians dihitung berdasarkan gabungan dari nilai-nilai rata-rata dan varians dari mini-batch saat ini dan nilai-nilai sebelumnya yang telah disesuaikan.

Ketika melakukan normalisasi, kita tidak hanya menggunakan nilai dari satu batch saja, tetapi juga mempertimbangkan nilai beberapa batch sebelumnya untuk mendapatkan estimasi yang lebih stabil dari rata-rata dan varians. Ini membantu mengurangi fluktuasi yang mungkin terjadi dalam statistik batch dan meningkatkan konsistensi normalisasi.

Nilai momentum ini sering kali disetel di antara 0,1 dan 0,999. Nilai yang lebih tinggi menunjukkan bahwa kita lebih bergantung pada nilai sebelumnya daripada nilai dari batch saat ini. Sebagai contoh, jika momentum diatur ke 0,9, 90% informasi dari batch sebelumnya akan digunakan dan hanya 10% dari statistik batch saat ini yang akan dipertimbangkan.

Penggunaan momentum pada BatchNormalization membantu menghasilkan data yang lebih stabil dan dapat meningkatkan kinerja pelatihan pada model jaringan saraf tiruan. 

Meskipun BatchNormalization dapat membantu mengurangi overfitting, ini bukan jaminan bahwa model tidak akan mengalami overfitting. Overfitting masih dapat terjadi jika model terlalu kompleks untuk jumlah data pelatihan yang sedikit. Selain itu, ada kemungkinan juga terdapat banyak noise pada data, atau jika ada masalah lainnya dengan proses pelatihan. 

Sebenarnya, masih ada banyak parameter yang dapat kita atur pada BatchNormalization, tetapi hal yang paling penting terdapat pada momentum. Jika Anda masih penasaran dengan parameter lainnya, silakan baca pada tautan berikut: BatchNormalization.



Optimasi Pelatihan Menggunakan Callback:
Pada latihan sebelumnya, pelatihan model berlangsung sebanyak jumlah epoch yang sudah ditentukan. Jika kita amati, epoch terakhir memperlihatkan nilai akurasi dari model tidak meningkat lagi. Contoh kasus lainnya, bayangkan kita menentukan epoch sebanyak 100 dan ternyata pada epoch ke-20, performa dari model sudah memenuhi target. Jika itu terjadi, tentunya kita harus menghentikan proses pelatihan agar tidak membuang-buang waktu, bukan?

Untungnya, kita dapat memberi tahu model untuk berhenti ketika telah mencapai metrik tertentu sehingga proses pelatihan model menjadi lebih singkat. Bayangkan ketika waktu untuk eksekusi 1 epoch sebesar 6 detik. Untuk mengeksekusi 100 epoch berarti membutuhkan waktu selama 600 detik. Jika model telah mencapai target akurasi yang kita inginkan misalnya pada epoch ke-30, dan model otomatis berhenti melakukan pelatihan, kita bisa menghemat waktu eksekusi 70 epoch yaitu selama 420 detik. Wow!

Kasus yang lebih ekstrem terjadi jika Anda membangun sebuah model neural network dengan banyak layer dengan waktu eksekusi 1 epochs sebesar 600 detik atau 10 menit tentunya Anda tidak ingin menunggu selama 60.000 detik atau setara dengan 1000 menit, ‘kan? Anda dapat menghemat cukup banyak waktu dengan hanya menggunakan callbacks. Tentu hal ini menjadi sebuah solusi yang sangat canggih, bukan?
Fungsi callbacks membantu kita untuk memberi tahu model agar berhenti melakukan pelatihan ketika sudah mencapai target tertentu serta mencatat dan melakukan utilitas lain seperti menyimpan performa terbaik model. Nah, langsung saja kita praktik menggunakan callbacks. Kita masih menggunakan dataset dan model dari latihan klasifikasi iris pada materi sebelumnya. Kode utuhnya seperti di bawah ini.

Setelah semuanya siap, kita perlu membuat sebuah kelas bernama myCallback(). Perhatikan baik-baik. Parameter pertama pada kelas callbacks di atas harus memiliki sifat inherit tf.keras.callbacks.Callback. Selanjutnya, kita buat fungsi yang paling penting yaitu on_epoch_end(). Fungsi inilah yang akan berperan untuk memberi tahu model agar berhenti melakukan pelatihan ketika telah mencapai target.

Kode if(logs.get(‘accuracy’)>0.9) sangat intuitif menunjukkan kalau kode itu memiliki arti “jika akurasi lebih besar dari 0.9, maka eksekusi perintah berikutnya”. Kita juga dapat menulis kode print(“\nAkurasi telah mencapai >90%!”) untuk ditampilkan ketika callbacks aktif.

Kode self.model.stop_training = True adalah kode yang memberi tahu model untuk menghentikan pelatihan. Setelah kelas ini dibuat, buatlah objek dari kelas tersebut. Berikut kode lengkap untuk membuat callbacks.

class myCallback(tf.keras.callbacks.Callback):
  def on_epoch_end(self, epoch, logs={}):
    if(logs.get('accuracy')>0.9):
      print("\nAkurasi telah mencapai >90%!")
      self.model.stop_training = True
callbacks = myCallback()

Setelah membuat sebuah objek callbacks, selanjutnya kita harus menambahkan hyperparameter tersebut pada saat melakukan proses pelatihan. Untuk menggunakan callbacks sangat gampang, kita hanya perlu menambahkan parameter callbacks dan mengisi objek callbacks yang telah kita buat pada parameter tersebut.

hist = model.fit(X_train, Y_train, epochs=100, callbacks = [callbacks])

Dari latihan di atas dapat dilihat model berhasil menghentikan pelatihan ketika telah mencapai akurasi yang kita tentukan pada epoch ke 5 dari 100. Kita telah menghemat waktu eksekusi sebanyak 96 epoch. Keren, bukan?

Dengan waktu pelatihan yang lebih cepat, tentunya Anda dapat melakukan eksplorasi jauh lebih banyak. Selain menggunakan callbacks, sebenarnya masih banyak fungsi yang sangat berguna ketika kita melakukan pelatihan, salah satu contoh lainnya adalah EarlyStopping. 



Menggunakan Dataset dari Tensorflow:
Ada dua cara untuk mengakses dataset yang sudah disediakan TensorFlow yaitu melalui tf.data.Datasets dan juga TensorFlow Datasets. Keduanya memiliki fungsi yang sangat mudah digunakan tetapi memiliki variasi data yang berbeda. Sudah tidak sabar, ya? Yuk, kita bahas cara penggunaan keduanya bersama-sama.

Membuat Model Menggunakan tf.data.Datasets
As usual, hal pertama yang kita lakukan adalah melakukan impor library TensorFlow agar dapat mengakses seluruh fungsi yang ada.

import tensorflow as tf
Kemudian kita buat objek untuk menampung dataset kita dan masukkan fungsi tf.keras.datasets.<nama_dataset> pada objek tersebut. Karena pada kasus ini menggunakan dataset mnist, kode yang digunakan sebagai berikut.

mnist = tf.keras.datasets.mnist
Selanjutnya, kita perlu membagi dataset yang sudah ada menjadi dua bagian yaitu pelatihan dan uji menggunakan fungsi load_data(). Fungsi load_data() dari objek dataset mengembalikan bentuk dataset yang telah dibagi menjadi atribut latih, label latih, atribut uji, dan label uji. Sangat gampang, bukan? Hanya dengan beberapa baris kode kita sudah memiliki data latih dan data uji yang siap dipakai.

(gambar_latih, label_latih), (gambar_testing, label_testing) = mnist.load_data()

Kita bisa menampilkan label dan salah satu gambar dari data latih menggunakan library matplotlib untuk memastikan data yang kita pilih sudah sesuai dan siap pakai.

import numpy as np
np.set_printoptions(linewidth=200)
import matplotlib.pyplot as plt
plt.imshow(gambar_latih[0])
print(label_latih[0])

Namun, dataset yang kita miliki belum dinormalisasi sehingga kita harus melakukannya secara manual. Kode di bawah berfungsi untuk membagi setiap piksel pada gambar sebesar 255 karena nilai sebuah piksel berkisar dari 0 sampai 255. 

gambar_latih  = gambar_latih / 255.0
gambar_testing = gambar_testing / 255.0
Dalam konteks pemrosesan citra atau pengenalan pola, normalisasi gambar sangat penting untuk memastikan bahwa nilai piksel berada dalam rentang yang dapat diolah dengan baik oleh model. Dalam hal ini, kode tersebut membagi nilai piksel dari gambar pelatihan (gambar_latih) dengan 255.0.

Karena nilai piksel pada umumnya dalam rentang 0 hingga 255, pembagian dengan nilai 255.0 akan menghasilkan nilai piksel yang lebih kecil yaitu dalam rentang 0 hingga 1. Ini membantu model pembelajaran mesin untuk belajar lebih efisien karena nilai yang lebih kecil cenderung memiliki efek yang lebih stabil selama proses pembelajaran.

Selanjutnya, kita akan menggunakan 3 layer sebagai arsitektur model ini. Untuk layer pertama, kita menggunakan layer khusus yaitu Flatten yang berfungsi untuk mengubah input kita yang berupa matriks 2 dimensi menjadi array 1 dimensi. 

Untuk melakukan hal tersebut tentu sangatlah mudah jika menggunakan TensorFlow, kita hanya perlu memanggil layer Flatten dan mengisi parameter seperti biasanya.

model = tf.keras.models.Sequential([
    tf.keras.layers.Flatten(input_shape=(28,28)),
    tf.keras.layers.Dense(128, activation=tf.nn.relu),
    tf.keras.layers.Dense(10, activation=tf.nn.softmax)
])

Terakhir, tentukan optimizer dan loss untuk model, lalu lakukan pelatihan pada model.

model.compile(optimizer = tf.optimizers.Adam(),
              loss = 'sparse_categorical_crossentropy',
              metrics=['accuracy'])
model.fit(gambar_latih, label_latih, epochs=5)

 

Membuat Model Menggunakan TensorFlow Dataset:
TensorFlow menyediakan kumpulan dataset siap pakai untuk dilatih dengan TensorFlow, Jax, dan framework machine learning lainnya. TensorFlow Dataset (TFDS) bertugas untuk melakukan pengunduhan dan menyiapkan data secara deterministik dengan menggunakan tf.data.Dataset (atau np.array).

Sebelumnya, kita sudah mempelajari tf.data.Datasets, sekarang kita mengenal rivalnya yaitu TFDS. Namanya, cukup mirip, ya? Jangan bingung antara TFDS (library ini) dengan tf.data.Datasets (API dari TensorFlow). 

TFDS adalah wrapper tingkat tinggi di sekitar tf.data. Jika Anda tidak terbiasa dengan API ini, kami menyarankan Anda untuk membaca panduan resmi dari TensorFlow pada tautan berikut: tf.data.

Di sisi lain, TFDS juga menyediakan lebih banyak datasets yang siap digunakan. Tanpa berlama-lama, mari kita mulai perjalanan ini dengan melakukan impor library yang akan digunakan.

import tensorflow as tf
import tensorflow_datasets as tfds

Setelah berhasil menginstal library TFDS, kita dapat melihat list atau daftar dari dataset yang tersedia pada TFDS. Untuk melihat seluruh dataset yang tersedia, kita bisa menggunakan fungsi tfds.list_builders() atau dapat dilihat pada katalog TensorFlow Datasets.

tfds.list_builders()

(Datasetnya beneran banyak jir, sampe ada kdd untuk anomaly detection) 

 Seperti yang Anda lihat pada gambar di atas, ada banyak sekali dataset yang tersedia ketika kita menggunakan TFDS, baik itu untuk pembelajaran maupun eksplorasi mandiri. Setelah melihat banyak sekali dataset, tentunya kita harus menentukan dataset mana yang akan digunakan, bukan? Tenang saja, caranya sangat mudah. Kita hanya perlu menggunakan fungsi tfds.load() seperti berikut.

ds = tfds.load('mnist', split='train', shuffle_files=True)
assert isinstance(ds, tf.data.Dataset)

Kode di atas memuat dataset MNIST menggunakan TensorFlow Datasets. Mari kita jelaskan satu per satu detail dari kode di atas.

- Fungsi load() memungkinkan Anda untuk menggunakan dataset yang telah disediakan oleh TensorFlow Datasets. 
- Parameter pertama ('mnist') menunjukkan nama dataset yang ingin dimuat. 
- Parameter kedua ('split='train'') menunjukkan bahwa kita hanya memuat bagian pelatihan dari dataset MNIST. 
- Parameter ketiga ('shuffle_files=True') mengindikasikan bahwa file-file dataset akan diacak sebelum dimuat, sehingga urutan data tidak akan tetap sama setiap kali dataset dimuat ulang. 


Pada baris kedua mungkin ada sebuah sintaksis yang cukup asing bagi kita, yaitu assert isinstance. Wah, apa tuh bang? Fungsi tersebut adalah sebuah pernyataan asersi (assertion statement) yang memeriksa apakah variabel ds adalah sebuah objek tf.data.Dataset. Assertion ini berguna untuk memastikan bahwa data yang dimuat benar-benar dalam bentuk dataset TensorFlow yang dapat digunakan untuk melatih model.

Masih ingat dengan ketentuan membagi data latih dan data uji? Yup, dengan menjalankan kode di atas, kita belum membagi dataset menjadi proporsi yang dibutuhkan. Untuk melakukan itu menggunakan TFDS, kita dapat melakukannya dengan kode berikut.

(train_images, train_labels), (test_images, test_labels) = tfds.as_numpy(tfds.load('mnist',
              split = ['train', 'test'],
              batch_size=-1,
              as_supervised=True))

Selanjutnya, kita akan menggunakan berbagai macam layer sebagai arsitektur model ini. Untuk layer pertama, kita menggunakan layer khusus yaitu Conv2D() yang digunakan untuk mengekstraksi fitur dari gambar input menggunakan operasi konvolusi. Dilanjutkan oleh MaxPooling2D yang berguna untuk mereduksi dimensi spasial dari representasi gambar berdasarkan layer sebelumnya.

Setelah trial and error mengenai layer Convo2D dan MaxPooling2D, barulah kita menggunakan Flatten yang berfungsi untuk mengubah input kita yang berupa matriks 2 dimensi menjadi array 1 dimensi.

Hufttt, terlihat sangat rumit, ya? Sebenarnya tidak serumit yang Anda bayangkan kok. Dengan menggunakan TensorFlow, kita hanya perlu membangun arsitektur seperti pada latihan-latihan sebelumnya. Mari kita telisik kode di bawah ini. 

model = tf.keras.Sequential([
    tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(28, 28, 1)),
    tf.keras.layers.MaxPooling2D(2, 2),
    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),
    tf.keras.layers.MaxPooling2D(2,2),
    tf.keras.layers.Conv2D(128, (3,3), activation='relu'),
    tf.keras.layers.MaxPooling2D(2,2),
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(64,activation="relu"),
    tf.keras.layers.Dense(128,activation="relu"),
    tf.keras.layers.Dense(10, activation="softmax")
])

Seperti yang Anda lihat di atas, kita menggunakan beberapa layer Conv2D dan MaxPooling2D. Hal ini sangat berfungsi untuk melakukan perhitungan konvolusi yang lebih detail terhadap data pelatihan yang kita miliki. Hal yang harus diperhatikan ada pada layer pertama, yaitu input_shape. Yup, kita harus menyesuaikan ukuran gambar yang ada pada dataset dengan model yang kita bangun.

Selanjutnya, kita juga harus menggunakan layer Flatten seperti yang sudah kita bahas berkali-kali sebelumnya. Terakhir, pastikan units yang ditetapkan sesuai dengan jumlah kelas yang ada pada kasus Anda. Karena kasus ini merupakan multi kelas dengan 10 kelas, units yang dibutuhkan adalah 10 dengan activation softmax.

Sebelum melakukan pelatihan, kita harus menentukan tiga buah argumen untuk melakukan compile model yang telah dibangun. Tentunya Anda masih ingatkan terhadap optimizers, loss function, dan metrics? Benar, kita harus menentukan konfigurasi yang tepat untuk kasus yang dihadapi 

model.compile(
    optimizer=tf.keras.optimizers.RMSprop(),
    loss=tf.keras.losses.SparseCategoricalCrossentropy(),
    metrics=['accuracy'],
)

Setelah model.compile() dipanggil, model sudah siap untuk dilatih dengan memanggil fungsi model.fit() dengan data pelatihan yang sesuai.

model.fit(train_images, train_labels, batch_size=50, epochs=5)

Terakhir, untuk memastikan semuanya aman dan memiliki performa yang dapat diandalkan, Anda perlu melakukan evaluasi terhadap data uji menggunakan kode berikut. 

model.evaluate(test_images, test_labels, batch_size=1)

Yuhuu, Anda sudah dapat membuat sebuah model menggunakan TensorFlow Datasets. Sampai di sini, ada banyak sekali petualangan yang menunggu Anda. Jumlah datasets yang ada pada TFDS sangat banyak, belum lagi Anda juga dapat menggunakan open repository lainnya. Oleh karena itu jangan lupa untuk mengisi bahan bakar karena perjalanan yang sangat panjang akan dimulai dari titik ini. 



Penggunaan Batch Loading:
Ngomong-ngomong tentang hyperparameter, masih ingatkah apa itu hyperparameter? Mari kita sedikit flashback pada modul sebelumnya. Hyperparameter adalah parameter yang tidak ditentukan oleh model itu sendiri selama proses pembelajaran, tetapi harus ditentukan oleh developer atau pembuat model sebelum pelatihan dimulai. Mereka adalah variabel yang mengontrol proses pembelajaran dan arsitektur model, seperti kecepatan pembelajaran, jumlah layer dan neuron dalam jaringan saraf, ukuran batch, dan lain-lain.

Nah, Batch loading adalah proses pelatihan ketika neural network melakukan pembaruan parameternya (weight) setelah membaca sejumlah sampel data tertentu. Misal dataset kita berisi 800 buah gambar pizza. Tanpa batch size, proses pembaruan parameter terjadi untuk seluruh sampel pada dataset. Sehingga, ketika tanpa menggunakan batch size, pada 1 epoch terdapat 800 kali pembaruan weight. Ketika 1 ukuran batch adalah 32 buah gambar pizza, terdapat 25 buah batch pada dataset. Pada batch loading, model baru melakukan pembaruan parameter setelah membaca satu batch atau 32 buah gambar pizza. Sehingga, proses pembaruan parameter pada 1 epoch hanya sebanyak 25 kali.

Apakah Anda sudah menyadari apa fungsi dari batch loading? Yup benar, dengan batch loading proses pelatihan data akan menjadi lebih cepat. Kita akan melihat langsung kegunaan dari batch loading pada latihan berikut.

Untuk dataset, kita akan menggunakan dataset mnist dan model yang sama dengan materi sebelumnya.

Selanjutnya, buatlah fungsi fit() untuk melakukan pelatihan. Perlu Anda perhatikan, di sini kita mulai menggunakan batch loading. Untuk menggunakan batch loading, kita hanya menambahkan satu parameter ‘batch_size’ pada fungsi fit(). 

Tahukah Anda bahwa fungsi fit() secara default menggunakan batch loading dengan batch size sebesar 32. Sehingga, ketika kita tidak mendefinisikan nilai parameter batch_size, ukuran batch akan diisi sebesar 32 secara default. Perhatikan bahwa pada setiap epoch memakan waktu selama sekitar 3 atau 4 detik.

model.fit(training_images, training_labels, batch_size=32, epochs=5)

Agar lebih paham, sekarang kita akan menggunakan batch_size yang lebih besar yaitu 128. Dapat kita lihat bahwa semakin besar batch size, waktu eksekusi tiap epoch akan semakin cepat. 

model.fit(training_images, training_labels, batch_size=128, epochs=5)

Seperti yang Anda lihat di atas, waktu pelatihan yang dilakukan memakan waktu yang lebih singkat. Silakan Anda bereksperimen menggunakan batch size yang lain untuk pengalaman belajar yang lebih maksimal. Bagaimana hasilnya? Silakan beri kesimpulan sendiri, ya. 

Pada latihan ini, kita telah memahami bahwa dengan menggunakan batch loading dapat mempercepat pelatihan. Untuk pemilihan batch size sendiri tidak ada aturan bakunya, tetapi yang umum dipakai adalah 32, 64, dan 128. Anda harus bereksperimen sendiri guna menemukan batch size yang cocok dengan masalah atau model yang sedang dibangun. 


Apa fungsi dari fungsi aktivasi dalam setiap neuron dalam neural network?
d. Fungsi aktivasi mengonversi input dari neuron sebelumnya menjadi output yang diinginkan.

Apa perbedaan antara SGD (Stochastic Gradient Descent) dan Adam optimizer dalam konteks pelatihan neural network?
c. SGD menggunakan momentum untuk mempercepat konvergensi, sementara Adam memperhitungkan momentum dan adaptasi learning rate.

Apa fungsi dari callback Plot Loss dan Akurasi dalam TensorFlow/Keras?
b. Untuk memantau dan memvisualisasikan perubahan loss dan akurasi model selama pelatihan.

Apa tujuan dari Batch Normalization dalam pelatihan neural network?
c. Mempercepat konvergensi model dengan memperbarui parameter bobot lebih cepat. (salah)

Mengapa teknik dropout efektif dalam mengatasi masalah overfitting pada neural network?
c. Dropout secara acak menghapus sebagian unit (neuron) pada setiap iterasi pelatihan, memaksa jaringan untuk belajar fitur yang lebih umum.

Apa keuntungan menggunakan one-hot encoding dalam tugas klasifikasi multi kelas?
A. One-hot encoding mengubah label kelas menjadi vektor biner, memungkinkan model untuk memprediksi probabilitas setiap kelas secara independen.
B. One-hot encoding mempercepat proses pelatihan model dengan mengurangi kompleksitas data input.
C (salah). One-hot encoding menghasilkan representasi biner untuk setiap kelas, memudahkan interpretasi output model.
D. One-hot encoding mengatasi masalah kelas yang tidak seimbang dalam dataset.

Apa keuntungan dari batch loading dalam pelatihan neural network dibandingkan dengan memuat seluruh dataset sekaligus?
A (salah). Batch loading menghasilkan hasil yang lebih akurat karena memperbarui parameter model lebih sering.
B. Batch loading memungkinkan penggunaan dataset yang lebih besar tanpa memori yang besar.
C. Batch loading mempercepat waktu pelatihan karena hanya memuat sebagian kecil dari dataset pada setiap iterasi.
D. Batch loading mengurangi risiko overfitting karena model tidak melihat seluruh dataset pada saat yang sama.

Apa yang dimaksud dengan fungsi loss dalam konteks pelatihan neural network?
c. Fungsi loss adalah fungsi yang mengukur seberapa baik model memprediksi output yang benar.


Apa fungsi utama dari callbacks dalam pelatihan neural network menggunakan TensorFlow dan Keras?
a. Callbacks digunakan untuk memonitor metrik kinerja model dan mengambil tindakan berdasarkan perubahan metrik tersebut. 




---------------------------------------------------------------------------
                        Natural Language Processing
---------------------------------------------------------------------------

Natural language processing (NLP) adalah salah satu cabang ilmu komputer yang mempelajari cara komputer berinteraksi dengan penggunaan bahasa dalam kehidupan sehari-hari. NLP bertujuan mengembangkan teknik-teknik agar komputer dapat memahami bahasa alami manusia.

Setiap negara memiliki bahasa ibu yang berbeda-beda, bukan? Bahasa alami yang digunakan oleh manusia dari berbagai negara memiliki perbedaan dalam hal penulisan dan pengucapan. Perbedaan penulisan dan pengucapan bahasa alami antar negara memengaruhi kompleksitas dalam pengembangan NLP. Hal ini menuntut pengembang NLP untuk memperhatikan variasi bahasa dan menciptakan model yang dapat mengakomodasi perbedaan tersebut. 

Singkatnya, NLP didefinisikan sebagai kemampuan komputer untuk memproses bahasa lisan ataupun tulisan yang digunakan dalam percakapan sehari-hari manusia. Dalam komputasi, bahasa harus diwakili sebagai rangkaian simbol yang mengikuti aturan tertentu. Jadi, harapannya dengan memanfaatkan teknologi NLP, pengembang bisa membuat komputer dapat memahami perintah yang ditulis dalam bahasa manusia standar. Hmm, komputer memahami bahasa manusia, menarik sekali, ya!

Peran NLP dalam kehidupan sehari-hari: 
Hai, teman-teman! Sadar ataupun tidak, kita semua pasti sangat sering berdampingan dengan yang namanya NLP, lo! Nah, NLP ini sebenarnya seperti sihir di balik layar gadget kita. Dengan kepiawaiannya mengolah bahasa manusia, NLP dapat menjadi andalan kita dalam kehidupan sehari-hari, dari mulai berbincang dengan asisten virtual sampai membaca update media sosial. 

Namun, apakah peran NLP hanya sebatas itu? Tentu tidak! Masih banyak lagi hal menarik yang bisa kita kupas bersama. Yuk, mari kita jelajahi lebih dalam!

Search Engine (Mesin Pencari)
Saat menggunakan mesin pencari, seperti Google, NLP membantu dalam memahami kata kunci atau pertanyaan yang Anda masukkan. Kemudian, NLP digunakan untuk menganalisis jutaan halaman web serta menyajikan hasil pencarian yang paling relevan dengan memperhitungkan konteks dan makna kata-kata. 

Asisten Virtual
Chatbot, seperti yang terdapat dalam layanan pelanggan online atau asisten virtual: Siri, Google Assistant, atau Alexa, menggunakan NLP untuk memahami dan merespons percakapan manusia secara alami. NLP memungkinkan mereka untuk menjawab pertanyaan, memberikan informasi, atau menyelesaikan tugas berdasarkan instruksi verbal dari pengguna.

Penerjemahan Otomatis
Layanan penerjemahan, seperti Google Translate menggunakan NLP untuk menerjemahkan teks dari satu bahasa ke bahasa lain secara otomatis. NLP membantu mengenali struktur dan makna kalimat untuk menghasilkan terjemahan yang lebih akurat. 

Analisis Sentimen Media Sosial
NLP digunakan untuk menganalisis sentimen publik terhadap merek, produk, atau layanan berdasarkan ulasan dan komentar di media sosial. Hal ini membantu perusahaan untuk memahami umpan balik pelanggan dan mengambil tindakan yang sesuai.

Deteksi Spam
NLP membantu dalam deteksi email spam dengan menganalisis pola dan konten email untuk mengidentifikasi pesan yang tidak diinginkan. Ini membantu dalam meningkatkan keamanan dan keandalan sistem ketika memilah email.

Pemeriksaan Tanda Baca, Tata Bahasa, dan Parafrase
Dalam pengolah kata atau pesan teks, NLP digunakan saat memeriksa tanda baca, tata bahasa, dan melakukan parafrase untuk memperbaiki struktur kalimat. NLP membantu pengguna menulis dengan benar dan efektif, mengidentifikasi kesalahan tata bahasa, serta memberikan rekomendasi untuk membuat kalimat lebih jelas dan mudah dipahami.

Chatbot dalam Layanan Pelanggan
Banyak perusahaan menggunakan chatbot berbasis NLP untuk menyediakan layanan pelanggan 24/7. Chatbot ini dapat merespons pertanyaan pelanggan, membantu dalam proses pemesanan atau pembayaran, dan memberikan informasi tentang produk atau layanan.

Melalui berbagai aplikasi ini, NLP tidak hanya memfasilitasi interaksi manusia dengan teknologi, tetapi juga membuka potensi baru pada analisis data teks yang mendalam. Kemampuan NLP memproses bahasa manusia menjadi kunci untuk membangun sistem yang lebih pintar dan responsif dalam berbagai bidang kehidupan.

Tensorflow + NLP, Mengapa?

Jadi, ketika membicarakan TensorFlow dan NLP, seakan-akan kita membahas dua kekuatan besar dalam dunia teknologi. Mengapa mereka bisa menjadi pasangan yang sangat kuat? Nah, TensorFlow bisa diibaratkan sebagai senjata utamanya. Melalui beragam tools yang dimilikinya, TensorFlow membantu kita dalam melatih model machine learning dengan sangat efisien. 

Di sisi lain, NLP menjadi tujuan utamanya, yaitu mengolah dan memahami bahasa manusia. Ketika kedua kekuatan tersebut digabungkan, ini seperti memberikan senjata canggih kepada pahlawan untuk menaklukkan segala tantangan yang berkaitan dengan bahasa. Tertarik untuk mengeksplorasi lebih dalam kisah epik tentang TensorFlow dan NLP? Let's goooooooo!

Jadi, TensorFlow adalah sebuah framework yang sangat populer dalam dunia machine learning dan deep learning yang dikembangkan oleh Google. Salah satu alasan utama TensorFlow sering digunakan dalam konteks NLP adalah karena kemampuannya yang sangat baik saat melakukan komputasi numerik. Komputasi dilakukan secara efisien, terutama pada operasi-operasi matriks yang sering digunakan dalam model NLP, seperti neural networks.

Selain itu, TensorFlow menyediakan berbagai model serta algoritma siap pakai untuk tugas-tugas NLP, seperti pengenalan teks, penerjemahan mesin, analisis sentimen, dan lainnya. Melalui TensorFlow Hub, pengguna mudah mengakses repository model yang dioptimalkan pada tugas NLP dan menggunakan model tersebut untuk membangun aplikasi NLP dengan cepat.

TensorFlow juga memiliki TensorFlow Extended (TFX). Itu adalah platform end-to-end untuk pengembangan, pelatihan, pengujian, dan implementasi model machine learning, termasuk model-model NLP. Ini memudahkan integrasi model NLP dalam lingkungan produksi secara lebih terstruktur.

Kelebihan lainnya adalah dokumentasi lengkap dan komunitas aktif di sekitar TensorFlow, membuatnya menjadi framework yang mudah dipelajari serta digunakan untuk pengembangan aplikasi NLP. TensorFlow juga mendukung integrasi dengan TensorFlow.js untuk menjalankan model NLP pada browser dan TensorFlow Lite untuk menjalankan model pada perangkat mobile atau embedded.

Dengan berbagai fitur ini, TensorFlow menjadi salah satu pilihan utama bagi para pengembang untuk membangun aplikasi NLP yang kuat, efisien, dan scalable. Namun, selain TensorFlow, ada juga framework dan library lain, seperti PyTorch, Hugging Face Transformers, dan spaCy. Framework-framework tersebut juga populer di kalangan praktisi NLP. Pilihan framework yang tepat tergantung pada kebutuhan proyek dan preferensi pengembang.



Text Preprocessing:
Nah, dalam konteks NLP, text preprocessing adalah langkah awal yang krusial sebelum kita masuk ke tahap analisis atau pemodelan. Jadi, mari kita telusuri bersama-sama hal yang sebenarnya dilakukan dalam proses ini serta alasan ini begitu penting untuk perjalanan kita dalam memahami dan memanfaatkan teks secara efektif.

Ide awalnya, pada NLP, data yang akan diolah seringkali berupa teks tidak terstruktur atau memiliki struktur tidak teratur. Contohnya, data teks bisa berupa dokumen, artikel, tweet, atau ulasan pelanggan yang tidak mengikuti format atau struktur tertentu. Oleh karena itu, sebelum menerapkan teknik-teknik NLP, seperti sentiment analysis atau topic modelling, kita perlu melakukan proses pengubahan bentuk data ini menjadi format yang lebih terstruktur.

Langkah-langkah pra-pemrosesan teks dalam NLP sangat penting untuk mengubah teks menjadi bentuk numerik yang dapat dipahami oleh komputer. Algoritma dan komputer secara alami memahami data dalam bentuk numerik, seperti vektor atau matriks. 

Let's prepare first!
Salah satu keunggulan Python adalah dukungannya terhadap banyak library sumber terbuka (open-source) dan memungkinkan implementasi yang kuat dalam NLP. Ada beragam library Python yang dapat digunakan untuk memproses dan menerapkan solusi dalam NLP. Beberapa di antaranya yang akan kita bahas adalah NLTK dan Sastrawi.

Natural Language Toolkit (NLTK)
NLTK adalah sebuah library Python yang digunakan untuk memanipulasi dan menganalisis teks dalam NLP. NLTK menyediakan berbagai alat yang sangat berguna untuk persiapan teks sebelum digunakan dalam machine learning atau algoritma deep learning. Tujuan utama NLTK adalah membantu dalam pemrosesan teks untuk keperluan analisis atau pengembangan model NLP.

Salah satu kelebihan NLTK adalah kemampuannya untuk melakukan berbagai tugas pra-pemrosesan teks, yaitu tokenisasi (pemecahan teks menjadi token), stemming (menghilangkan akhiran kata), lematisasi (mengubah kata menjadi bentuk dasarnya), penghilangan stopwords (kata-kata umum yang tidak memberikan nilai tambah), dan banyak lagi. 

Selain itu, NLTK juga dilengkapi dengan koleksi besar data bahasa (corpora) yang dapat digunakan untuk melatih model bahasa atau melakukan penelitian dalam bidang NLP. Cara termudah untuk menginstal NLTK adalah menggunakan pip, yaitu alat manajemen paket untuk Python, melalui perintah berikut pada command line atau terminal.

!pip install nltk
Dengan NLTK, pengguna dapat dengan mudah mengimplementasikan teknik-teknik NLP yang diperlukan untuk memproses dan menganalisis teks sebelum menggunakan data tersebut dalam konteks machine learning atau deep learning.


Python Sastrawi:
Python Sastrawi
Python Sastrawi adalah sebuah library Python yang digunakan untuk melakukan proses stemming dalam bahasa Indonesia. Stemming adalah proses mengubah kata-kata menjadi kata dasar atau kata akar dengan cara menghilangkan imbuhan atau awalan.

Library Python Sastrawi ini didasarkan pada kamus kata dasar bahasa Indonesia yang digunakan untuk mengidentifikasi akar kata dari kata-kata dalam teks berbahasa Indonesia. Proses stemming sangat berguna dalam pengolahan teks untuk analisis teks, pengelompokan kata, atau aplikasi NLP lainnya di lingkungan berbahasa Indonesia. Cara menggunakan Python Sastrawi untuk melakukan stemming dalam bahasa Indonesia adalah dengan menginstal library tersebut terlebih dahulu menggunakan pip.

!pip install Sastrawi
Berikut adalah beberapa langkah umum dalam pra-pemrosesan teks dalam NLP.


Case Folding:
Case folding adalah langkah sederhana pada pra-pemrosesan teks yang bertujuan untuk mengubah semua huruf dalam dokumen teks menjadi huruf kecil. Tujuannya adalah membuat teks lebih seragam dan memudahkan proses analisis teks, terutama dalam pengenalan kata-kata yang sama meskipun berbeda penulisan huruf besar-kecil.

Case folding adalah proses standarisasi teks dengan mengubah semua huruf dalam teks menjadi huruf kecil. Tujuan utamanya adalah untuk menciptakan konsistensi dalam representasi teks, sehingga mempermudah dalam analisis teks selanjutnya.

Dengan melakukan case folding, perbedaan huruf besar dan kecil dalam teks diabaikan, sehingga memungkinkan perbandingan dan pencarian teks menjadi lebih mudah dan efisien.

Dalam case folding, hanya huruf dari 'a' sampai 'z' yang dapat diterima. Karakter selain huruf akan dihapus dan dianggap sebagai pemisah antar kata. Hal ini berarti tanda baca, angka, dan karakter khusus lainnya akan diabaikan atau dihapus dari teks. Pada langkah ini, kita tidak perlu mengandalkan library eksternal dan dapat menggunakan modul-modul bawaan yang tersedia dalam Python.

Mengubah teks menjadi huruf kecil adalah suatu langkah penting pada pengolahan teks, terutama dalam search engine. Pengaturan indeks pencarian harus memperhitungkan perbedaan antara huruf besar dan kecil. Jadi, pencarian dapat mencocokkan nama dengan benar tanpa memedulikan kapitalisasi. 

Misalnya, jika sistem pencarian tidak memperlakukan "Machine Learning" dan "machine learning" sebagai entitas yang sama, pengguna mungkin mengalami kesulitan dalam menemukan kontak yang mereka cari, meskipun tampilan antarmuka pengguna menampilkan nama dengan huruf besar.

Berikut adalah contoh penggunaan Python untuk mengonversi teks menjadi lowercase.

teks_asli = "Ini Adalah Contoh Teks yang Akan Dikonversi Menjadi Lowercase."
 
# Mengubah teks menjadi lowercase
teks_lowercase = teks_asli.lower()
 
# Menampilkan hasil
print("Teks asli:", teks_asli)
print("Teks setelah diubah menjadi lowercase:", teks_lowercase)


Dalam contoh di atas, kita menggunakan metode .lower() pada string teks_asli untuk mengubah semua karakter huruf menjadi huruf kecil. Hasilnya, teks_lowercase akan berisi teks yang telah diubah menjadi huruf kecil.

Output dari program ini akan menunjukkan perbedaan antara teks asli dan teks setelah diubah menjadi lowercase. Metode .lower() sangat berguna dalam pengolahan teks untuk menormalisasi teks sebelum melakukan analisis lebih lanjut, seperti tokenisasi, pencocokkan kata, atau analisis teks lainnya.



Remove Special Characters:
Dalam pemrosesan teks, seringkali kita perlu membersihkan teks dari karakter khusus yang tidak diinginkan, seperti angka, tanda baca, simbol, atau karakter khusus lainnya. Langkah ini penting untuk mempersiapkan teks agar sesuai dengan kebutuhan analisis atau pemrosesan selanjutnya.

Karakter khusus, yakni !, @, #, $, ?, dan lainnya, tidak selalu memberikan kontribusi informasi yang berguna dalam analisis teks. Sebaliknya, mereka dapat mengganggu algoritma pemrosesan teks dan menyulitkan proses komputasi.

Menghapus Angka
Menghapus angka dari teks adalah langkah penting dalam pra-pemrosesan teks untuk fokus pada informasi teks yang relevan tanpa mempertimbangkan numerik. 

def hapus_angka(teks):
    teks_tanpa_angka = ''.join([char for char in teks if not char.isdigit()])
    return teks_tanpa_angka
 
teks_dengan_angka = "Ini adalah contoh teks dengan angka 12345 yang akan dihapus."
 
teks_tanpa_angka = hapus_angka(teks_dengan_angka)
 
print("Teks dengan angka:", teks_dengan_angka)
print("Teks tanpa angka:", teks_tanpa_angka)

Dalam contoh di atas, kita menggunakan fungsi hapus_angka() untuk menghapus semua karakter angka dari teks menggunakan list comprehension. Fungsi isdigit() digunakan untuk memeriksa bahwa sebuah karakter adalah angka. Hasilnya, teks_tanpa_angka akan berisi teks yang tidak mengandung karakter angka.

Dengan menghapus angka dari teks, kita dapat memfokuskan analisis pada informasi teks yang bersifat non-numerik. Hal ini sering digunakan dalam pemrosesan teks untuk analisis sentimen, klasifikasi teks, atau tugas NLP lainnya ketika numerik tidak relevan dengan konteks analisis tersebut.

Terkadang, kita perlu menghilangkan angka dari teks, tetapi ingin mempertahankan angka-angka penting, seperti nomor rumah atau nomor telepon. Oleh sebab itu, kita dapat menggunakan ekspresi regular (regex) untuk mengenali dan menghapus angka yang tidak relevan.

Inilah contoh penggunaan Regex untuk menghapus angka.

import re
 
def hapus_angka_tidak_relevan(teks):
    # Menggunakan regex untuk mengidentifikasi dan menghapus angka yang tidak relevan
    # Pola untuk mengenali angka yang harus dihapus, termasuk nomor rumah dan nomor telepon
    pola_angka_tidak_relevan = r"\b(?:\d{1,3}[-\.\s]?)?(?:\d{3}[-\.\s]?)?\d{4,}\b"
    hasil = re.sub(pola_angka_tidak_relevan, "", teks)
    return hasil.strip()
 
kalimat = "Di sini ada beberapa nomor rumah yaitu  123, 456, dan 789. Silakan hubungi 081234567890 untuk informasi lebih lanjut."
 
hasil_tanpa_angka = hapus_angka_tidak_relevan(kalimat)
 
# Menampilkan hasil
print("Kalimat dengan angka:", kalimat)
print("Kalimat tanpa angka tidak relevan:", hasil_tanpa_angka)



Pada contoh di atas, kita menggunakan regex r"\b\d+\b" untuk mencocokkan dan menghapus angka sebagai kata tunggal dalam teks. Ekspresi regex ini memastikan bahwa hanya angka-angka yang muncul sebagai kata terpisah yang dihapus.

Ekspresi regex ini memiliki beberapa komponen penting sebagai berikut.

- \b adalah anchor word boundary yang menandakan batas antara karakter word (kata) dan non-word (bukan kata). Ini memastikan bahwa kita hanya mencocokkan angka yang muncul sebagai kata terpisah.
- \d+ adalah pola regex yang mencocokkan satu atau lebih digit (angka).
- \b adalah anchor word boundary lagi yang menutup pola, memastikan bahwa angka yang dicocokkan adalah kata tunggal yang berdiri sendiri. 

Dengan menggunakan ekspresi regex ini, kita memastikan bahwa hanya angka-angka yang muncul sebagai kata terpisah yang akan dihapus dari teks. Ini memungkinkan kita untuk mempertahankan angka-angka dalam konteks nomor rumah atau nomor telepon.

Menghapus Tanda Baca
Tanda baca dalam kalimat tidak memengaruhi proses pra-pemrosesan teks dalam sistem NLP. Oleh karena itu, ini perlu dihapus agar tidak mengganggu kinerja sistem. Untuk menghapus tanda baca, seperti [!”#$%&’()*+,-./:;<=>?@[]^_`{|}~], Anda dapat menggunakan Python dengan menggunakan pendekatan berikut.

mport string
 
def remove_punctuation(text):
    # Membuat set yang berisi semua tanda baca
    punctuation_set = set(string.punctuation)
 
    # Menghapus tanda baca dari teks
    text_without_punctuation = ''.join(char for char in text if char not in punctuation_set)
 
    return text_without_punctuation
 
teks_asli = "Ini adalah contoh teks, dengan tanda baca! Contoh ini, digunakan? untuk demonstrasi."
 
teks_tanpa_tanda_baca = remove_punctuation(teks_asli)
 
print("Teks asli:", teks_asli)
print("Teks setelah menghapus tanda baca:", teks_tanpa_tanda_baca)

Dalam contoh di atas, kalimat mengandung beberapa tanda baca, seperti tanda seru (!), koma (,), titik (.), dan tanda tanya (?). Fungsi hapus_tanda_baca() digunakan untuk menghapus semua tanda baca dari kalimat menggunakan metode translate() dengan str.maketrans('', '', string.punctuation). Hasilnya adalah kalimat yang tidak mengandung tanda baca, seperti yang ditunjukkan dalam output.


Menghapus Whitespace dalam Teks
Whitespace (karakter kosong) merujuk kepada karakter yang tidak terlihat pada layar, seperti spasi, tab, newline, dan karakter kosong lainnya. Karakter whitespace umumnya digunakan untuk memisahkan kata-kata atau elemen dalam teks, tetapi kadang-kadang mereka dapat muncul secara tidak diinginkan di awal, akhir, atau pertengahan teks.

Menggunakan strip() untuk Menghapus Whitespace di Awal dan Akhir
Metode strip() pada string digunakan untuk menghapus whitespace di awal dan akhir string. Ini sangat berguna saat Anda ingin membersihkan teks dari spasi tambahan yang mungkin terdapat pada sebelum atau setelah teks utama. Berikut contoh penggunaannya.

teks = "   Ini adalah contoh kalimat dengan spasi di awal dan akhir.    "
teks_setelah_strip = teks.strip()
print(teks_setelah_strip)

Metode strip() sangat berguna dalam pra-pemrosesan teks untuk membersihkan teks dari karakter kosong yang tidak diinginkan, sebelum melakukan analisis atau pemrosesan lebih lanjut.

Menggunakan replace() untuk Menghapus Whitespace di Seluruh String
Jika Anda perlu menghapus semua whitespace dalam string, Anda dapat menggunakan metode replace() untuk mengganti setiap whitespace dengan string kosong (""). Contoh penggunaannya seperti ini.

teks_dengan_whitespace = "Ini adalah    contoh kalimat    dengan spasi    di dalamnya."
teks_tanpa_whitespace = teks_dengan_whitespace.replace(" ", "")
print(teks_tanpa_whitespace)

Pada contoh ini, metode replace(" ", "") digunakan untuk mengganti setiap spasi dengan string kosong sehingga menghapus semua whitespace dalam string teks_dengan_whitespace.

Kesimpulannya, metode strip() berguna untuk menghilangkan whitespace di awal dan akhir string, sementara replace() dapat digunakan untuk menghapus whitespace di seluruh string. Pilih metode sesuai dengan kebutuhan Anda berdasarkan lokasi dan jenis whitespace yang perlu dihapus, ya!


Stopwrod Removal (Filtering)
Penghapusan stopwords adalah langkah penting dalam pra-pemrosesan teks pada NLP. Stopwords adalah kata-kata umum yang sering muncul pada teks, tetapi tidak memiliki nilai informatif tinggi dalam analisis teks. Contohnya adalah kata-kata seperti "dan", "di", "ke", "yang", dan sebagainya. Tujuan penghapusan stopwords adalah membersihkan teks dari kata-kata umum tersebut sehingga fokus analisis dapat lebih pada kata-kata kunci yang lebih bermakna. 

Pada tahap filtering dalam pemrosesan teks, kita melakukan seleksi kata-kata penting dengan menghapus kata-kata yang dianggap kurang relevan atau memiliki informasi rendah. Hal ini biasanya dilakukan dengan mengimplementasikan daftar stopword. 

Misalnya, ketika melakukan pencarian melalui mesin pencari dengan pertanyaan "apa itu pengertian machine learning?", Anda ingin sistem memfokuskan hasil pencarian pada dokumen-dokumen yang berkaitan langsung dengan "pengertian machine learning" dan tidak terganggu oleh kata-kata umum, seperti "apa" atau "itu".

Mari kita bahas lebih rinci perbedaan antara stopwords yang disediakan oleh NLTK dan Sastrawi, serta cara kita dapat menggunakannya dalam konteks pengolahan teks dengan bahasa Indonesia. 

Stopwords NLTK (Natural Language Toolkit)
NLTK adalah toolkit yang sangat populer untuk NLP dalam Python. NLTK menyediakan koleksi stopwords untuk beberapa bahasa, termasuk bahasa Indonesia. Stopwords NLTK dapat digunakan untuk membersihkan teks dari kata-kata umum yang tidak memberikan nilai tambah dalam analisis teks.

Contoh Penggunaan Stopwords NLTK untuk Bahasa Indonesia

import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
 
# Download korpus stopwords bahasa Indonesia dari NLTK jika belum terunduh
nltk.download('stopwords')
nltk.download('punkt')  # Untuk tokenisasi kata
 
teks = "Perekonomian Indonesia sedang dalam pertumbuhan yang membanggakan."
 
# Tokenisasi teks menjadi kata-kata
tokens_kata = word_tokenize(teks)
 
# Ambil daftar stopwords bahasa Indonesia dari NLTK
stopwords_indonesia = set(stopwords.words('indonesian'))
 
# Filtering kata-kata dengan menghapus stopwords
kata_penting = [kata for kata in tokens_kata if kata.lower() not in stopwords_indonesia]
 
# Gabungkan kata-kata penting kembali menjadi teks
teks_tanpa_stopwords = ' '.join(kata_penting)
 
print("Teks asli:", teks)
Perekonomian Indonesia sedang dalam pertumbuhan yang membanggakan.

print("Teks setelah filtering stopwords NLTK:", teks_tanpa_stopwords)
Perekonomian Indonesia pertumbuhan membanggakan.

Dalam contoh di atas, kita menggunakan NLTK untuk mengunduh daftar stopwords bahasa Indonesia. Kemudian, kita tokenisasi teks menjadi kata-kata menggunakan word_tokenize() dari NLTK. Selanjutnya, kita melakukan filtering kata-kata dengan menghapus stopwords di dalamnya. Hasilnya adalah teks yang hanya berisi kata-kata penting tanpa stopwords.


Stopwords Sastrawi
Sastrawi adalah pustaka Python yang fokus pada pemrosesan teks dalam bahasa Indonesia, terutama untuk NLP. Sastrawi menyediakan daftar stopwords yang dioptimalkan untuk bahasa Indonesia sehingga sangat cocok digunakan pada analisis teks dalam bahasa Indonesia.

Contoh Penggunaan Stopwords Sastrawi untuk Bahasa Indonesia

from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory
from nltk.tokenize import word_tokenize
 
# Inisialisasi objek StopWordRemover dari Sastrawi
factory = StopWordRemoverFactory()
stopwords_sastrawi = factory.get_stop_words()
 
teks = "Perekonomian Indonesia sedang dalam pertumbuhan yang membanggakan."
 
# Tokenisasi teks menjadi kata-kata
tokens_kata = word_tokenize(teks)
 
# Filtering kata-kata dengan menghapus stopwords Sastrawi
kata_penting = [kata for kata in tokens_kata if kata.lower() not in stopwords_sastrawi]
 
# Gabungkan kata-kata penting kembali menjadi teks
teks_tanpa_stopwords = ' '.join(kata_penting)
 
print("Teks asli:", teks)
Perekonomian Indonesia sedang dalam pertumbuhan yang membanggakan.

print("Teks setelah filtering stopwords Sastrawi:", teks_tanpa_stopwords)
Perekonomian Indonesia sedang pertumbuhan membanggakan.


Dalam contoh di atas, kita menggunakan Sastrawi untuk mengakses daftar stopwords bahasa Indonesia. Kemudian, kita tokenisasi teks menjadi kata-kata menggunakan word_tokenize() dari NLTK. Selanjutnya, kita melakukan filtering kata-kata dengan menghapus stopwords Sastrawi dari kata-kata tersebut. Hasilnya adalah teks yang hanya berisi kata-kata penting tanpa stopwords.

Perbedaan Utama
Perbedaan utama antara stopwords NLTK dan Sastrawi sebagai berikut.
- Bahasa yang Didukung: NLTK menyediakan stopwords untuk beberapa bahasa termasuk bahasa Indonesia, sedangkan Sastrawi dioptimalkan untuk pemrosesan teks dalam bahasa Indonesia.
- Kustomisasi: Sastrawi memungkinkan dalam mengakses daftar stopwords yang dioptimalkan khusus untuk bahasa Indonesia, sementara NLTK memiliki koleksi stopwords pada berbagai bahasa dan bisa lebih umum.

Pilihan antara NLTK dan Sastrawi untuk filtering stopwords tergantung pada bahasa teks yang Anda proses serta kebutuhan analisis NLP Anda. Untuk bahasa Indonesia, Sastrawi adalah pilihan yang lebih spesifik dan sesuai karena fokus pada bahasa tersebut. Namun, NLTK juga dapat digunakan jika Anda memerlukan stopwords dalam berbagai bahasa atau jika Anda sudah menggunakan NLTK untuk alat NLP lainnya.


Tokenizing:
Tokenisasi (tokenizing) adalah proses membagi teks menjadi potongan-potongan lebih kecil yang disebut token. Token dapat berupa kata, frasa, atau entitas lain yang lebih kecil dari teks yang dianalisis. Tujuan tokenisasi adalah memecah teks menjadi unit-unit yang lebih mudah diolah atau diinterpretasi dalam analisis teks atau pemrosesan bahasa alami.

Dalam pemrosesan teks atau NLP, tokenisasi adalah langkah penting dalam pra-pemrosesan teks sebelum analisis lebih lanjut dilakukan, seperti klasifikasi teks, pembuatan model bahasa, atau ekstraksi fitur. Tujuan utama tokenisasi adalah mengubah teks menjadi urutan token yang terstruktur untuk digunakan dalam analisis atau aplikasi NLP.

Ada berbagai metode untuk melakukan tokenisasi, tergantung pada kebutuhan dan kompleksitas tugas yang dihadapi. Berikut adalah beberapa metode tokenisasi umum.

Tokenisasi Kata (Word Tokenization)
Memecah teks menjadi kata-kata individu. Biasanya, pemisah antar kata adalah spasi atau karakter whitespace lainnya.

from nltk.tokenize import word_tokenize
 
text = "Ini adalah contoh tokenisasi kata dalam pemrosesan teks."
tokens = word_tokenize(text)
print(tokens)

['Ini', 'adalah', 'contoh', 'tokenisasi', 'kata', 'dalam', 'pemrosesan', 'teks', '.']

Pada contoh di atas, teks dibagi menjadi token-token kata menggunakan word_tokenize() dari NLTK. Hasilnya adalah daftar kata-kata yang membentuk teks tersebut.

Tokenisasi Kalimat (Sentence Tokenization)
Memecah teks menjadi kalimat-kalimat. Pemisah antar kalimat dapat berupa tanda baca, seperti titik, tanda tanya, atau tanda seru.

text = "Ini adalah contoh tokenisasi kalimat. Apakah ini kalimat kedua? Ya, ini kalimat ketiga!"
sentences = sent_tokenize(text)
print(sentences)

[‘Ini adalah contoh tokenisasi kalimat.’, ‘Apakah ini kalimat kedua?’, 

‘Ya, ini kalimat ketiga!’]


Pada contoh di atas, teks dibagi menjadi kalimat-kalimat menggunakan sent_tokenize() dari NLTK. Hasilnya adalah daftar kalimat-kalimat dalam teks tersebut.

Tokenisasi Frasa (Phrase Tokenization)
Memecah teks menjadi frasa-frasa atau unit-unit frasa yang lebih besar dari kata tunggal.

text = "Pemrosesan teks adalah cabang ilmu komputer yang berfokus pada pengolahan teks dan dokumen."
phrases = text.split(',')
print(phrases)

['Pemrosesan', 'teks', 'adalah', 'cabang', 'ilmu', 'komputer', 'yang', 'berfokus', 'pada', 'pengolahan', 'teks', 'dan', 'dokumen', '.']


Pada contoh di atas, teks dibagi menjadi frasa-frasa menggunakan TreebankWordTokenizer() dari NLTK. Hasilnya adalah daftar frasa-frasa yang membentuk teks tersebut.

Tokenisasi Berdasarkan Aturan (Rule-based Tokenization)
Menggunakan aturan linguistik atau aturan pemrosesan teks untuk membagi teks menjadi token. Contohnya termasuk tokenisasi khusus untuk URL, email, atau entitas tertentu.

import re

text = "Pertama, kita perlu menyiapkan bahan-bahan yang diperlukan."
tokens = re.findall(r'\w+|\d+', text)
print(tokens)

['Pertama', 'kita', 'perlu', 'menyiapkan', 'bahan', 'bahan', 'yang', 'diperlukan']

Pada contoh di atas, teks dibagi berdasarkan aturan regex untuk mengenali URL. Hasilnya adalah daftar URL dalam teks tersebut.


Tokenisasi Berdasarkan Model (Model-based Tokenization)
Menggunakan model statistik atau model bahasa untuk memprediksi token dalam teks. Contohnya termasuk tokenisasi berbasis mesin pembelajaran, seperti model pembelajaran berbasis Transformer.

text = "Ini adalah contoh tokenisasi berbasis model."
tokens = text.split()
print(tokens)

['Ini', 'adalah', 'contoh', 'tokenisasi', 'berbasis', 'model.']

Dengan menggunakan berbagai metode tokenisasi ini, kita dapat memecah teks menjadi unit-unit lebih kecil sesuai dengan kebutuhan analisis atau aplikasi NLP yang sedang dilakukan.

Eitssss! Jangan ragu atau bingung! Setiap metode tokenisasi memiliki perannya sendiri, tergantung pada konteks dan tujuan analisis teks yang ingin dicapai. Anda tidak perlu menggunakan semua metode sekaligus, kok! Cukup gunakan metode tokenisasi yang dibutuhkan berdasarkan kebutuhan spesifik Anda.

Metode tokenisasi kata dan kalimat adalah yang paling umum serta sering digunakan dalam pemrosesan teks. Oleh karena itu, penting untuk memahami secara baik kedua metode tersebut agar dapat menerapkan analisis teks dengan lebih efektif.



Stemming:
Stemming adalah sebuah proses dalam bidang pemrosesan teks yang digunakan untuk menyederhanakan kata-kata ke bentuk dasarnya dengan cara menghilangkan awal ataupun akhiran kata. Tujuannya adalah agar kata-kata berbeda, tetapi berasal dari satu kata dasar, dapat direduksi menjadi bentuk yang lebih seragam.

Namun, perlu diingat bahwa hasil stemming tidak selalu sama dengan akar kata sebenarnya dalam bahasa. Stemming hanya berfokus pada pembuangan bagian awal ataupun akhir kata untuk menyederhanakan kata-kata, tanpa memperhatikan makna sebenarnya dari kata tersebut.

Misalnya, kita memiliki beberapa kata dalam bahasa Inggris sebagai berikut.

running → running
runs → runs
ran
easily
fairness → fairness

Dengan menggunakan algoritma stemming, kita dapat mereduksi kata-kata tersebut menjadi bentuk dasarnya sebagai berikut.

run
run
ran
easy
fair


from nltk.stem import PorterStemmer
 
# Inisialisasi stemmer
stemmer = PorterStemmer()
 
# Kata-kata asli
words = ["running", "runs", "runner", "ran", "easily", "fairness", "better", "best", "cats", "cacti", "geese", "rocks", "oxen"]
 
# Melakukan stemming pada setiap kata
for word in words:
    stemmed_word = stemmer.stem(word)
    print(f"Kata asli: {word}, Kata setelah stemming: {stemmed_word}")

Algoritma stemming, seperti Porter, Snowball, dan Lancaster menggunakan aturan-aturan sederhana untuk melakukan pemotongan pada kata-kata sehingga kita dapat mengelompokkan variasi kata yang mirip dalam bentuk dasar yang lebih umum. 

Stemming sangat berguna dalam pengolahan bahasa alami, pengindeksan teks, dan analisis teks untuk mengurangi variasi kata-kata serta memperlakukan kata-kata serupa dengan cara yang lebih konsisten.



Lemmatization:
Lemmatization adalah sebuah teknik dalam pemrosesan teks untuk mengubah kata-kata ke bentuk dasar mereka, yang disebut lemma (atau lema). Tujuannya adalah menyatukan kata-kata dengan akar sama agar dapat direpresentasikan melalui satu bentuk dasar yang merepresentasikan makna kata tersebut secara tepat.

Proses lemmatization melibatkan penentuan jenis kata (part of speech, POS) untuk memastikan kata dikembalikan ke lema yang benar. Misalnya, kata kerja (verb) memiliki lema yang berbeda dengan kata benda (noun).

from nltk.stem import WordNetLemmatizer
 
# Download wordnet jika belum di-download
nltk.download('wordnet')
 
# Inisialisasi lemmatizer
lemmatizer = WordNetLemmatizer()
 
# Kata-kata asli
words = ["Run", "Cat", "Good", "Goose", "Rock", "City", "Big", "Happy", "Run", "Sleep"]
 
# Melakukan lematisasi pada setiap kata
for word in words:
    lemma_word = lemmatizer.lemmatize(word.lower())  # Mengonversi ke huruf kecil untuk memastikan pemrosesan yang konsisten
    print(f"Kata asli: {word}, Kata setelah lematisasi: {lemma_word}")

Perbedaan utama antara lemmatization dan stemming adalah lemmatization lebih cermat karena mempertimbangkan aturan morfologi bahasa. Misalnya, lemmatization akan mengubah kata-kata seperti "studies", "studying" menjadi bentuk dasar "study", sementara stemming hanya memotong akhiran kata tanpa memperhatikan makna menjadi "studi" dan "study".

Stemming bertujuan memotong awalan atau akhiran kata untuk mencapai bentuk dasar, yang mungkin, menghasilkan kata "improv" dari semua bentuk tersebut. Kata ini adalah bentuk yang umum dan sering digunakan dalam proses stemming.

Sementara itu, tujuan lematisasi adalah mengembalikan kata-kata ke bentuk dasar dalam bahasa yang sesuai dengan makna sebenarnya. Dalam hal ini, "improve" dipertahankan sebagai bentuk dasar karena lemmatization memperhitungkan struktur morfologis kata tersebut.




Latihan Pra-pemrosesan Teks:

Pra-pemrosesan teks adalah tahap penting dalam analisis teks dan natural language processing (NLP). Tujuannya adalah membersihkan dan mempersiapkan teks mentah agar dapat diolah lebih lanjut dengan algoritma pemrosesan teks atau analisis.

1. Case Folding
Proses mengubah semua huruf dalam teks menjadi huruf kecil atau huruf besar agar konsisten. Misalnya, mengubah "TeKS" menjadi "teks" atau "TEKS".

2. Removal Special Characters
Menghapus karakter khusus atau simbol yang tidak relevan atau tidak diinginkan dari teks.
- Menghapus Angka: Menghilangkan semua angka dari teks.
- Menghapus Tanda Baca: Menghapus semua tanda baca dari teks.
- Menghapus White Space: Menghapus spasi tambahan atau karakter spasi ganda dari teks.
- Menggunakan strip(): Menggunakan metode strip() dalam pemrograman untuk menghapus spasi tambahan di awal dan akhir teks.
- Menggunakan replace(): Menggunakan metode replace() untuk mengganti spasi tambahan dengan string kosong sehingga menghapusnya dari seluruh teks.

3. Stopword Removal (Filtering)
Menghapus kata-kata yang umumnya tidak memberikan nilai tambah dalam analisis teks, seperti "dan", "atau", "yang", dll.
- Stopword NLTK (Natural Language Toolkit): Menggunakan koleksi kata-kata stopword yang disediakan oleh NLTK untuk menghapus stopword dari teks.
- Stopword Sastrawi: Penghapusan kata-kata stopword menggunakan kamus stopword yang disediakan oleh Sastrawi, pustaka pemrosesan bahasa alami bahasa Indonesia.

4. Tokenizing
Proses membagi teks menjadi bagian-bagian lebih kecil yang disebut token.
- Tokenisasi Kata (Word Tokenization): Memecah teks menjadi token berdasarkan kata-kata individual.
- Tokenisasi Kalimat (Sentence Tokenization): Memecah teks menjadi token berdasarkan kalimat-kalimat.
- Tokenisasi Frasa (Phrase Tokenization): Memecah teks menjadi token berdasarkan frasa-frasa atau unit-unit tertentu.
- Tokenisasi Berdasarkan Aturan (Rule-based Tokenization): Memecah teks menjadi token berdasarkan aturan tertentu, seperti pemisahan berdasarkan tanda baca.
- Tokenisasi Berdasarkan Model (Model-based Tokenization): Memecah teks menjadi token menggunakan model linguistik atau machine learning.


5. Stemming
Proses menghapus imbuhan dari kata untuk mengembalikannya ke bentuk dasarnya. Misalnya, mengubah "berlari", "berlarian", "lari" menjadi "lar".

6. Lemmatization
Proses mengubah kata-kata ke bentuk dasarnya (lema) dengan mempertimbangkan konteks dan struktur bahasa. Misalnya, mengubah "menyanyikan" menjadi "nyanyi".

teks_asli = "Ini Adalah Contoh Teks yang Akan Dikonversi Menjadi Lowercase."
 
# Mengubah teks menjadi lowercase
teks_lowercase = teks_asli.lower()

Setelah teks asli didefinisikan pada variabel teks_asli, kita menggunakan metode lower() untuk mengonversinya menjadi lowercase dan hasilnya disimpan dalam variabel teks_lowercase. Hasilnya kemudian ditampilkan menggunakan perintah print(). 

Dengan demikian, kita dapat dengan mudah melihat perbedaan antara teks asli dan yang telah diubah menjadi lowercase. Proses ini sangat berguna pada pemrosesan teks karena membuatnya menjadi konsisten dalam penggunaan huruf, memudahkan analisis, dan memungkinkan pencocokan yang lebih baik.


Removal Special Characters

Menghapus Angka
Angka seringkali dianggap tidak penting dalam pemrosesan teks dan dapat mengganggu kualitas model prediksi. Oleh karena itu, mereka perlu dihapus agar tidak mengganggu proses analisis teks. Berikut contoh kodenya. 

def hapus_angka(teks):
    teks_tanpa_angka = ''.join([char for char in teks if not char.isdigit()])
    return teks_tanpa_angka
 
# Contoh teks dengan angka
teks_dengan_angka = "Ini adalah contoh teks dengan angka 12345 yang akan dihapus."
 
# Memanggil fungsi untuk menghapus angka
teks_tanpa_angka = hapus_angka(teks_dengan_angka)


Fungsi hapus_angka(teks) menggunakan list comprehension untuk mengiterasi melalui setiap karakter dalam teks dan memeriksa bahwa karakter tersebut bukan angka dengan metode isdigit(). Karakter-karakter bukan angka kemudian digabungkan kembali menjadi sebuah string baru yang tidak mengandung angka. 

Contoh teks dengan angka (teks_dengan_angka) diberikan sebagai input untuk fungsi hapus_angka(), yang menghasilkan teks tanpa angka (teks_tanpa_angka). Hasilnya ditampilkan bersama dengan teks asli menggunakan perintah print(). Proses ini membantu meningkatkan kualitas pemrosesan teks dengan menghilangkan unsur yang mungkin tidak relevan atau mengganggu. 

Menghapus Tanda Baca
Menghapus tanda baca dapat membantu meningkatkan kualitas analisis teks serta kinerja model NLP. Berikut contohnya.

def remove_punctuation(text):
    # Membuat set yang berisi semua tanda baca
    punctuation_set = set(string.punctuation)
 
    # Menghapus tanda baca dari teks
    text_without_punctuation = ''.join(char for char in text if char not in punctuation_set)
 
    return text_without_punctuation
 
# Contoh teks dengan banyak tanda baca
teks_asli = """
Dalam dunia ini, banyak hal terjadi, dari yang kecil hingga yang besar. Kita bisa melihat keindahan, tapi juga kekejaman. Ada harapan, namun juga keputusasaan. Bagaimanapun, hidup terus berjalan, tak peduli apa pun yang terjadi!
"""
 
# Menghapus tanda baca dari teks
teks_tanpa_tanda_baca = remove_punctuation(teks_asli)

Fungsi remove_punctuation(text) digunakan untuk menghapus semua tanda baca dari teks yang diberikan. Pertama, sebuah set yang berisi semua tanda baca didefinisikan menggunakan modul string. Set ini kemudian digunakan untuk memfilter karakter-karakter dalam teks bahwa hanya karakter yang tidak termasuk set tanda baca yang dipertahankan. Hasilnya adalah teks baru yang tidak mengandung tanda baca.

Contoh teks yang mengandung banyak tanda baca (teks_asli) diberikan sebagai input untuk fungsi remove_punctuation(). Hasilnya teks_tanpa_tanda_baca atau teks yang tidak mengandung tanda baca. Kedua teks tersebut ditampilkan menggunakan perintah print().


Menghapus White Space
Menghapus whitespace adalah langkah penting dalam pra-pemrosesan teks untuk membersihkan serta merapikan teks agar lebih mudah diproses dan dianalisis. Whitespace termasuk spasi, tab, dan karakter newline yang sering kali tidak memberikan informasi penting dalam teks, tetapi dapat memengaruhi hasil analisis jika tidak dikelola dengan baik. 

Berikut contohnya.

teks = "   Ini adalah contoh kalimat dengan spasi di awal dan akhir.    "
teks_dengan_whitespace = "Ini adalah    contoh kalimat    dengan spasi    di dalamnya."

teks_setelah_strip = teks.strip()
print(teks_setelah_strip)

teks_tanpa_whitespace = teks_dengan_whitespace.replace(" ", "") print(teks_tanpa_whitespace)

Dalam potongan kode tersebut, pertama-tama kita memiliki sebuah string teks yang memiliki spasi di awal dan akhir. Kita menggunakan metode strip() untuk menghapus spasi tambahan di awal dan akhir teks. Hasilnya disimpan dalam variabel teks_setelah_strip. Kemudian, kita mencetak hasilnya.

Kemudian, kita memiliki string teks_dengan_whitespace dengan spasi di antara kata-kata. Kita menggunakan metode replace() untuk mengganti setiap spasi dengan string kosong sehingga menghapus spasi dalam teks. Hasilnya disimpan dalam variabel teks_tanpa_whitespace dan kemudian dicetak.

Dengan menggunakan strip() dan replace(), kita dapat mudah mengelola whitespace pada teks, baik di awal/akhir maupun di dalamnya.


Stopword Removal
Penghapusan stopword adalah salah satu tahap penting pada pra-pemrosesan teks untuk menghilangkan kata-kata yang umumnya tidak memberikan makna signifikan dalam analisis teks. Dalam penghapusan stopword, kata-kata seperti "dan", "atau", "yang", dan sejenisnya diidentifikasi serta dihapus dari teks. 

Berikut contohnya.

import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
 
# Download korpus stopwords bahasa Indonesia dari NLTK jika belum terunduh
nltk.download('stopwords')
nltk.download('punkt')  # Untuk tokenisasi kata
 
teks = "Perekonomian Indonesia sedang dalam pertumbuhan yang membanggakan."
 
# Tokenisasi teks menjadi kata-kata
tokens_kata = word_tokenize(teks)
 
# Ambil daftar stopwords bahasa Indonesia dari NLTK
stopwords_indonesia = set(stopwords.words('indonesian'))
 
# Filtering kata-kata dengan menghapus stopwords
kata_penting = [kata for kata in tokens_kata if kata.lower() not in stopwords_indonesia]
 
# Gabungkan kata-kata penting kembali menjadi teks
teks_tanpa_stopwords = ' '.join(kata_penting)

Potongan kode di atas menjelaskan cara menggunakan Natural Language Toolkit (NLTK) dalam bahasa Python untuk menghapus stopwords dari teks berbahasa Indonesia. Pertama, NLTK digunakan untuk mengunduh korpus stopwords bahasa Indonesia jika belum terunduh. Selanjutnya, teks disediakan dan tokenisasi dilakukan menggunakan word_tokenize() untuk membagi teks menjadi kata-kata. 

Setelah itu, daftar stopwords bahasa Indonesia diambil dari korpus NLTK. Kata-kata dalam teks kemudian difilter dengan menghapus stopwords menggunakan list comprehension. Hasilnya adalah teks yang hanya berisi kata-kata penting setelah stopwords dihilangkan.


Tokenizing

Word Tokenization

# Misalkan kita ingin memisahkan frasa berdasarkan tanda baca koma (,)
text = "Apel, jeruk, pisang, dan mangga."
phrases = text.split(',')

Potongan kode di atas adalah contoh sederhana dari tokenisasi kata atau frasa berdasarkan tanda baca koma (,). Dalam contoh ini, kita memiliki sebuah teks berisi daftar buah-buahan yang dipisahkan oleh koma. Untuk memisahkan frasa-frasa tersebut menjadi token-token, kita menggunakan metode split(',') yang membagi teks berdasarkan tanda koma. Ini menghasilkan daftar frasa-frasa yang dipisahkan. 

Output:

['Ini', 'adalah', 'contoh', 'kalimat', 'untuk', 'tokenisasi', 'kata', '.']

Hasilnya adalah sebuah list berisi frasa-frasa tersebut sebagai elemen-elemen individu. Dengan cara ini, kita dapat dengan mudah memisahkan teks menjadi bagian-bagian yang lebih kecil, yaitu frasa-frasa, berdasarkan tanda baca tertentu.

Sentence Tokenization
# Contoh aturan tokenisasi khusus untuk tokenisasi kata dalam bahasa Indonesia
import re
 
text = "Pertama, kita perlu menyiapkan bahan-bahan yang diperlukan."
tokens = re.findall(r'\w+|\d+', text)
print(tokens)

Potongan kode di atas adalah contoh tokenisasi kalimat dalam bahasa Indonesia dengan menggunakan aturan khusus. Dalam contoh ini, kita ingin membagi teks menjadi token-token kata atau angka. Kita menggunakan modul re (regular expression) untuk mencocokkan pola dalam teks.

Pola yang digunakan dalam re.findall(r'\w+|\d+', text) memiliki dua bagian:

- \w+ untuk mencocokkan urutan karakter alfanumerik (kata) dan
- \d+ untuk mencocokkan urutan karakter numerik (angka).

Hasilnya adalah sebuah list berisi token-token kata atau angka dalam teks tersebut. Dengan menggunakan aturan khusus ini, kita dapat mudah melakukan tokenisasi kalimat dalam bahasa Indonesia dan menghasilkan token-token yang sesuai dengan kebutuhan analisis teks kita.


Phrase Tokenization
# Misalkan kita ingin memisahkan frasa berdasarkan tanda baca koma (,)
text = "Apel, jeruk, pisang, dan mangga."
phrases = text.split(',')
print(phrases)

Dalam potongan kode yang diberikan, kita melakukan tokenisasi frasa berdasarkan tanda baca koma (,). Teks yang diberikan adalah "Apel, jeruk, pisang, dan mangga." dan kita ingin memisahkan teks tersebut menjadi frasa-frasa berdasarkan tanda koma.

Dengan menggunakan metode split(','), kita membagi teks menjadi bagian-bagian yang dipisahkan oleh tanda koma. Hasilnya adalah sebuah list berisi frasa-frasa tersebut sebagai elemen-elemen individu. Jadi, setiap frasa dihasilkan sebagai sebuah elemen dalam list.

Output:

['Apel', ' jeruk', ' pisang', ' dan mangga.']

Namun, perlu diperhatikan bahwa ada spasi di awal kata 'jeruk', 'pisang', dan 'dan mangga.'. Jika ingin membuang spasi tersebut, kita dapat menggunakan metode strip() setelah pemisahan frasa, seperti ini: phrases = [phrase.strip() for phrase in phrases].

Rule-based Tokenization
# Contoh aturan tokenisasi khusus untuk tokenisasi kata dalam bahasa Indonesia
import re
 
text = "Pertama, kita perlu menyiapkan bahan-bahan yang diperlukan."
tokens = re.findall(r'\w+|\d+', text)
print(tokens)

Potongan kode di atas adalah contoh penggunaan aturan tokenisasi khusus untuk memisahkan kata-kata pada teks dalam bahasa Indonesia. Pada contoh ini, kita menggunakan modul re (regular expression) untuk mencari pola dalam teks yang sesuai dengan aturan.

Pola yang digunakan dalam re.findall(r'\w+|\d+', text) adalah

- \w+: mencocokkan urutan karakter alfanumerik (kata) dan
- \d+: mencocokkan urutan karakter numerik (angka).

Dengan menggunakan re.findall(), kita mencari semua pola sesuai dengan salah satu dari pola yang diberikan, yaitu urutan karakter alfanumerik atau numerik, dalam teks. Hasilnya adalah sebuah list berisi token-token kata atau angka dalam teks tersebut.

['Pertama', 'kita', 'perlu', 'menyiapkan', 'bahan', 'bahan', 'yang', 'diperlukan']

Namun, perlu diperhatikan bahwa tokenisasi ini tidak mengambil tanda baca seperti koma (,) sebagai token. Jika Anda ingin mempertahankan tanda baca, Anda dapat menyesuaikan pola regular expression yang digunakan.

Model-based Tokenization
# Misalnya menggunakan spasi sebagai pemisah kata
text = "Ini adalah contoh tokenisasi berbasis model."
tokens = text.split()
print(tokens)

Potongan kode di atas adalah contoh sederhana dari tokenisasi kata dalam teks menggunakan spasi sebagai pemisah. Pada contoh ini, kita memiliki teks "Ini adalah contoh tokenisasi berbasis model." dan ingin memisahkan kata-kata dalam teks tersebut menjadi token-token.

Dengan menggunakan metode split(), teks tersebut dipisahkan menjadi token-token berdasarkan spasi di antara kata-kata. Hasilnya adalah sebuah list berisi kata-kata tersebut sebagai elemen-elemen individu.

Hasil dari potongan kode di atas adalah berikut.

Output:

['Ini', 'adalah', 'contoh', 'tokenisasi', 'berbasis', 'model.']

Setiap kata pada teks dipisahkan dan dihasilkan sebagai sebuah elemen dalam list. Dengan cara ini, kita dapat mudah memisahkan teks menjadi token-token kata sebagai langkah awal dalam pemrosesan teks lebih lanjut. 


Stemming:

from nltk.stem import PorterStemmer
stemmer = PorterStemmer()
words = ["running", "easily", "bought", "crying", "leaves"]
stemmed_words = [stemmer.stem(word) for word in words]
print(stemmed_words)

Potongan kode di atas adalah contoh penggunaan algoritma stemming Porter dalam modul NLTK untuk mendapatkan bentuk dasar kata-kata. Dalam contoh ini, kita menggunakan PorterStemmer dari NLTK untuk melakukan stemming pada beberapa kata.

Kita memiliki daftar kata-kata, seperti "running", "easily", "bought", "crying", dan "leaves". Melalui iterasi menggunakan list comprehension, setiap kata diubah menjadi bentuk dasarnya menggunakan metode stem() dari objek PorterStemmer. Hasilnya adalah daftar kata-kata yang telah di-stem menjadi bentuk dasarnya.

Hasil dari potongan kode di atas adalah berikut.

Output:

['run', 'easili', 'bought', 'cri', 'leav']



Lemmatization:

import nltk
nltk.download('wordnet')
from nltk.stem import WordNetLemmatizer
from nltk.corpus import wordnet
 
lemmatizer = WordNetLemmatizer()
words = ["running", "easily", "bought", "crying", "leaves"]
 
lemmatized_words = [lemmatizer.lemmatize(word, pos=wordnet.VERB) for word in words]
print(lemmatized_words)

Potongan kode di atas adalah contoh penggunaan WordNetLemmatizer dari NLTK untuk mendapatkan bentuk kata baku (lematisasi) dari kata-kata yang diberikan. Dalam contoh ini, kita menggunakan WordNetLemmatizer untuk melakukan lematisasi pada beberapa kata.

Setelah mengimpor WordNetLemmatizer dari modul NLTK, kita mengunduh korpus WordNet menggunakan perintah nltk.download('wordnet'). Korpus WordNet adalah sumber daya leksikal besar dalam bahasa Inggris yang digunakan untuk lematisasi dan penentuan kata-kata.

Hasil dari potongan kode di atas adalah berikut.

Output:

['run', 'easily', 'buy', 'cry', 'leave']

Kemudian, kita mendefinisikan daftar kata-kata, seperti "running", "easily", "bought", "crying", dan "leaves". Melalui iterasi menggunakan list comprehension, setiap kata diubah menjadi bentuk kata baku (lematisasi) menggunakan metode lemmatize() dari objek WordNetLemmatizer. Argumen pos yang diberikan adalah wordnet.VERB menunjukkan bahwa kata-kata yang diberikan adalah kata kerja (verb).

Lematisasi berbeda dengan stemming. Lematisasi memperhitungkan konteks dan memerlukan penentuan jenis kata (pos tag) sehingga hasil lematisasi lebih akurat serta umumnya lebih mudah dipahami.




Ekstraksi Fitur:

Ekstraksi fitur ini kayak kita mengekstrak 'keistimewaan' dari data teks kita. Misalnya, kita punya teks panjang tentang makanan, kita bisa 'mengekstrak' fitur-fitur penting, kayak 'resep', 'bahan-bahan', atau 'ulasan' dari teks itu."

"Jadi kayak kita 'mempercepat' proses pembacaan mesin untuk menemukan apa yang penting dalam teks?"

"Betul! Ini kayak kita bikin highlight untuk teks. Jadi mesin bisa langsung fokus ke bagian-bagian yang penting," jelas Diana sambil memberi analogi.

"Oh, jadi kayak memberi petunjuk ke mesin tentang apa yang harus diperhatikan dalam teks,"

"Exactly! Nah, ada beberapa cara kita bisa ekstrak fitur dari teks. Salah satunya adalah dengan Bag-of-Words atau BoW," 

"BoW itu kayak kita 'menghitung' berapa kali setiap kata muncul dalam teks, tanpa memperhatikan urutan kata-kata itu."

"Betul sekali! Terus ada juga TF-IDF, yang memberi bobot pada kata-kata berdasarkan seberapa pentingnya di suatu dokumen, tapi jarang muncul di dokumen lain,"

"Oh, jadi kayak memberi nilai ke kata-kata yang lebih 'langka', tapi penting, gitu?"


Ekstraksi fitur adalah proses menyaring informasi tersebut menjadi bentuk yang lebih ringkas dan bermakna bagi model kita. 

Dengan cara ini, kita bisa mengubah teks kompleks menjadi representasi numerik yang dapat dimengerti oleh algoritma machine learning


Ekstraksi fitur pada teks adalah kunci untuk mengubah teks menjadi bentuk yang dapat dipahami oleh algoritma machine learning, yaitu numerik. Saat kita berurusan dengan teks, seperti ulasan pelanggan, artikel berita, atau dokumen bisnis, kita perlu mengubahnya menjadi representasi numerik yang dapat diinterpretasikan oleh komputer. Tujuannya tidak hanya memproses teks, tetapi juga untuk mengekstrak informasi berharga yang tersembunyi di dalamnya. 


Dengan ekstraksi fitur teks, kita dapat mengidentifikasi pola, tema, atau sentimen untuk membangun model prediksi atau klasifikasi yang cerdas. Proses ini memungkinkan komputer untuk "memahami" teks sehingga dapat membuat keputusan atau memberikan wawasan berdasarkan data teks yang diberikan. Mari jelajahi lebih dalam bahwa ekstraksi fitur teks dapat mengubah cara kita berinteraksi dengan informasi yang ditemukan pada teks sehari-hari.

Berikut adalah beberapa teknik umum yang digunakan untuk ekstraksi fitur pada teks.

- Word Embedding
- Term Frequency-Inverse Document Frequency (TF-IDF)
- Bag of Words (BoW)
- N-gram
- POS Tagging (Part of Speech Tagging)
- Entity Recognition
- Pola atau Pola Kata (Pattern Matching)


1. Word Embedding:

Word embedding adalah teknik dalam NLP untuk merepresentasikan distribusi kata-kata di ruang vektor. Setiap kata direpresentasikan sebagai vektor numerik berdimensi rendah, yakni setiap dimensi vektor menunjukkan fitur atau atribut tertentu dari kata tersebut. Representasi ini memungkinkan kata-kata dengan makna serupa untuk memiliki vektor yang mendekat secara spasial dalam ruang embedding.

Tujuan utama dari word embedding adalah menangkap hubungan semantik dan sintaktis antarkata dalam teks. Kata-kata yang memiliki makna atau konteks mirip akan cenderung direpresentasikan dengan vektor serupa atau mendekati satu sama lain dalam ruang embedding. Contoh hubungan semantik yang dapat ditangkap oleh word embedding termasuk sinonim, antonim, hubungan hierarkis, dan hubungan konseptual lainnya.

Ada beberapa teknik word embedding yang populer digunakan dalam NLP

a. Word2Vec

Dikembangkan oleh Tomas Mikolov dari Google tahun 2013, Word2Vec adalah salah satu teknik word embedding yang paling terkenal. Word2Vec melatih model neural network untuk memprediksi kata berikutnya berdasarkan kata-kata sekitarnya (skip-gram) atau sebaliknya (continuous bag of words, CBOW). Proses ini menghasilkan vektor representasi kata yang menyandikan makna kata berdasarkan konteks kata tersebut muncul.

Word2Vec memodelkan kata-kata sebagai vektor numerik berdasarkan hubungan kata-kata yang muncul bersama dalam teks. Ide dasarnya adalah kata-kata yang sering muncul bersama dan digunakan dalam konteks serupa akan memiliki representasi vektor serupa. Jadi, vektor kata-kata yang terlatih dengan baik akan menangkap makna semantik dan hubungan antara kata-kata

Ada dua pendekatan utama dalam Word2Vec.

- Skip-gram
Model Word2Vec skip-gram melatih neural network untuk memprediksi kata target berdasarkan kata-kata sekitarnya (konteks) dalam suatu kalimat. Dalam pendekatan ini, model mencoba memprediksi kata tertentu (kata target) berdasarkan kata-kata di sekitarnya (konteks).

Pada gambar ilustrasi CBOW di atas, ada sebuah kalimat "Saya suka makan nasi". Model CBOW akan mengambil kata target "nasi" dan mencoba memprediksi kata-kata di sekitarnya, yaitu "Saya", "suka", dan "makan". Proses ini diulang dengan banyak kalimat berbeda untuk melatih model dan menghasilkan vektor yang merepresentasikan makna kata "nasi".

- Continuous Bag of Words (CBOW)
Pendekatan CBOW adalah kebalikan dari skip-gram. Alih-alih memprediksi kata target berdasarkan context, skip-gram justru berupaya memprediksi kata-kata di sekitar (context) berdasarkan kata tertentu (kata target).

Pada gambar ilustrasi skip-gram di atas, kata target "nasi" menjadi fokus utama. Model skip-gram akan mengambil kata "nasi" dan mencoba memprediksi kata-kata yang sering muncul di sekitarnya, seperti "makan", "saya", dan "suka". Proses ini diulang dengan banyak kalimat berbeda untuk melatih model dan menghasilkan vektor yang merepresentasikan makna kata "nasi".

Hasil dari Word2Vec adalah representasi vektor kata yang menyandikan makna kata berdasarkan konteks kata-kata tersebut muncul. Kata-kata dengan makna serupa akan memiliki vektor yang mendekat secara spasial dalam ruang vektor Word2Vec.


b. GloVe (Global Vectors for Word Representation)

GloVe adalah metode word embedding untuk menghasilkan vektor representasi kata-kata berdasarkan statistik dari matriks frekuensi kemunculan kata dalam teks (co-occurrence matrix). Ide utama di balik GloVe adalah memahami hubungan antar kata berdasarkan seberapa sering kata-kata tersebut muncul bersama-sama dalam korpus teks.

Proses GloVe melibatkan optimasi sebuah fungsi objektif yang menyesuaikan dengan vektor representasi kata-kata sedemikian rupa sehingga menggambarkan hubungan distribusi kata-kata dalam ruang vektor yang optimal. GloVe cenderung lebih fokus pada kem


c. FastText
FastText adalah ekstensi dari Word2Vec yang dikembangkan oleh Facebook AI Research (FAIR). Salah satu keunggulan utama FastText adalah kemampuannya untuk memperhitungkan struktur internal kata (subword information) dalam pembentukan representasi vektor kata. Ini membuat FastText lebih efektif ketika menangani kata-kata yang jarang atau tidak ditemukan dalam kosakata (out-of-vocabulary words).

FastText memecah kata menjadi bagian-bagian lebih kecil, seperti n-grams dan menghitung representasi vektor untuk setiap bagian subword ini. Kemudian, representasi vektor untuk kata diperoleh dengan menggabungkan vektor subword yang membentuk kata tersebut. Pendekatan ini membantu FastText untuk mengatasi variasi morfologi kata-kata dalam teks.


2. Term Frequency-Inverse Document Frequency (TF-IDF)

Korpus adalah kumpulan dokumen yang lebih besar.

TF-IDF (term frequency-inverse document frequency) adalah sebuah metode  pengolahan teks untuk mengevaluasi seberapa penting sebuah kata pada suatu dokumen dalam konteks korpus atau kumpulan dokumen yang lebih besar. 

TF-IDF adalah hasil perkalian antara term frequency (TF) dan inverse document frequency (IDF) untuk sebuah kata t dalam sebuah dokumen d dalam korpus D.

Tujuan utama dari TF-IDF adalah menimbang kata-kata sehingga kata-kata yang sering muncul dalam satu dokumen, tetapi jarang muncul di dokumen lain, dianggap lebih penting dan memiliki bobot lebih tinggi.


a. Term Frequency (TF)

Term frequency mengukur seberapa sering sebuah kata muncul dalam sebuah dokumen. Ini untuk memberikan bobot lebih tinggi pada kata-kata yang sering muncul dan dianggap lebih penting dalam konteks dokumen tersebut. 

TF dari sebuah kata ???? dalam dokumen ???? dapat dihitung dengan rumus berikut. 

           jumlah kata t dalam d
TF(t, d) = ----------------------
           total kata dalam d


Artinya:
- jumlah kata t dalam d adalah jumlah kemunculan kata t dalam
dokumen d.
- total kata dalam d adalah total jumlah kata dalam dokumen d.


b. Inverse Document Frequency (IDF)
Inverse document frequency mengukur seberapa jarang kata tertentu muncul di seluruh dokumen dalam korpus. Kata-kata yang muncul lebih sedikit pada dokumen dianggap memiliki informasi lebih spesifik atau unik dan mendapatkan bobot yang lebih tinggi melalui IDF.

IDF dari sebuah kata ???? di seluruh korpus dokumen D dapat dihitung dengan rumus berikut.
                 
                        total dokumen dalam D
IDF(t, D) = log ( --------------------------------- ) 
                  jumlah dokumen yang mengandung t

Artinya:
- total dokumen dalam korpus D adalah jumlah seluruh dokumen dalam korpus.
- jumlah dokumen yang mengandung kata t adalah jumlah dokumen ???? dalam korpus yang mengandung kata t.


TF-IDF (Term Frequency-Inverse Document Frequency)

TF-IDF adalah hasil perkalian antara term frequency (TF) dan inverse document frequency (IDF) untuk sebuah kata t dalam sebuah dokumen d dalam korpus D.

TF - IDF (t, d, D) = TF (t, d) x IDF (t, D)

Nah, TF-IDF adalah salah satu teknik yang sangat terkenal dalam pemrosesan teks, lo! Apakah Anda ingin mengetahui lebih lanjut mengenai perhitungan matematika di baliknya? Mari kita telusuri!

Langkah-langkahnya sebagai berikut. 

- Misalnya kita memiliki 3 kalimat sederhana.
Kalimat 1: "Saya suka makan nasi goreng"
Kalimat 2: "Nasi goreng adalah makanan favorit saya"
Kalimat 3: "Saya sering makan nasi goreng di pagi hari"

- Buat daftar kata-kata unik (term).
Term = ["Saya", "suka", "makan", "nasi", "goreng", "adalah", "makanan", "favorit", "sering", "di", "pagi", "hari"]

- Hitung Term Frequency (TF).

Kalimat	Kata	Jml Kata dalam Kalimat	Kemunculan Kata		TF

2	Suka	5			1/5			0.2	


- Hitung Document Frequency (DF)

DF dihitung sebagai jumlah kalimat yang mengandung kata tersebut. List kata didapatkan dari array term adalah Term = ["Saya", "suka", "makan", "nasi", "goreng", "adalah", "makanan", "favorit", "sering", "di", "pagi", "hari"].

Kata	DF
Suka	1


- Hitung Inverse Document Frequency (IDF)
                   N
IDF(t, D) = log ( ---- )
                  DF(t)


Artinya:
- N: jumlah kalimat dalam korpus (dalam kasus ini, N = 3).
- DF: document frequency.

Kata	DF	IDF
Suka	1	log(3/1) = 0.477


- Menghitung TF-IDF
Kalikan nilai TF dengan nilai IDF untuk setiap kata dalam setiap kalimat.

Kalimat		Kata	TF	IDF	TF-IDF
1		suka	1/5	0.477	0.0954

Setelah menghitung nilai TF-IDF, kita bisa melihat seberapa penting setiap kata dalam kalimat-kalimat yang diberikan berdasarkan koleksi dokumen tersebut. Ini membantu dalam analisis teks dan ekstraksi informasi dari dokumen.



Bag of Words (BoW)
Bag of Words (BoW) adalah sebuah pendekatan sederhana dalam pemrosesan teks yang digunakan untuk mewakili teks sebagai kumpulan kata-kata terurut tanpa memperhatikan tata urutan atau struktur kalimat. Pendekatan ini melibatkan langkah-langkah berikut.

- Tokenisasi: Teks dibagi menjadi unit-unit yang lebih kecil, seperti kata-kata atau n-gram (gabungan kata-kata berurutan sepanjang n).

n-gram adalah gabungan kata-kata berurutan sepanjang n

- Membuat vocabulary: Setiap kata unik dipetakan dalam teks pada indeks numerik. Ini menciptakan daftar kata yang disebut sebagai "vocabulary" atau "kosakata".

- Representasi vektor: Setiap dokumen direpresentasikan sebagai vektor, yakni setiap elemen vektor mengindikasikan frekuensi kemunculan kata dalam dokumen tersebut. Jadi, setiap dimensi vektor sesuai dengan kata dalam vocabulary dan nilai dalam dimensi tersebut menunjukkan jumlah kemunculan kata pada dokumen.


Langkah-langkah BoW sebagai berikut.

1. Tokenisasi
Kalimat 1: ["saya", "suka", "makan", "nasi", "goreng"]
Kalimat 2: ["nasi", "goreng", "adalah", "makanan", "favorit", "saya"]

2. Membuat Vocabulary
Kata-kata unik dari kedua kalimat: ["saya", "suka", "makan", "nasi", "goreng", "adalah", "makanan", "favorit"]

3. Representasi Vektor
Kalimat 1: [1, 1, 1, 1, 1, 0, 0, 0] # (mengindikasikan frekuensi kata dalam vocabulary)
Kalimat 2: [1, 0, 1, 1, 1, 1, 1, 1]

Di sini, setiap vektor adalah representasi numerik dari dokumen berdasarkan kosakata yang ada. Nilai pada posisi vektor menunjukkan jumlah kemunculan kata yang sesuai dalam dokumen. Vektor ini dapat digunakan sebagai input untuk algoritma pembelajaran mesin pada tugas seperti klasifikasi teks atau analisis sentimen.

Kelemahan dari Bag of Words adalah ia kehilangan informasi struktural dan urutan kata. Namun, pendekatan ini sering digunakan karena sederhana dan efektif untuk banyak aplikasi pemrosesan teks.


N-gram:
N-gram adalah sekumpulan n kata berurutan yang diambil dari sebuah teks atau urutan data. N-gram digunakan pada pemrosesan teks dan pengenalan pola untuk memahami hubungan antar kata dalam teks. Nilai n dalam "n-gram" menunjukkan jumlah kata yang diambil pada satu kali pengambilan. Contoh umum n-gram yaitu unigram (1-gram), bigram (2-gram), trigram (3-gram), dan seterusnya.

1. Unigram (1-gram): Unigram adalah satu kata tunggal yang diambil dalam urutan teks.
Contoh dari kalimat "Saya suka makan nasi goreng"
Unigram: ["Saya", "suka", "makan", "nasi", "goreng"]

2. Bigram (2-gram): Bigram adalah dua kata yang diambil dalam urutan teks.
Contoh dari kalimat yang sama: "Saya suka makan nasi goreng"
Bigram: ["Saya suka", "suka makan", "makan nasi", "nasi goreng"]

3. Trigram (3-gram): Trigram adalah tiga kata yang diambil dalam urutan teks.
Contoh dari kalimat yang sama: "Saya suka makan nasi goreng"
Trigram: ["Saya suka makan", "suka makan nasi", "makan nasi goreng"]



Latihan Ekstraksi Fitur pada Teks:
Ekstraksi fitur pada teks adalah tahap krusial dalam analisis teks, yaitu teks mentah diubah menjadi representasi numerik untuk digunakan oleh algoritma pembelajaran mesin. Tahapan ini menjadi penting karena algoritma pembelajaran mesin membutuhkan representasi numerik dari data untuk melakukan pemrosesan lebih lanjut. 

Dengan menggunakan fitur yang tepat, model pembelajaran mesin cenderung memiliki kinerja lebih baik dalam memprediksi atau mengklasifikasikan teks. Metode ekstraksi fitur yang umum digunakan termasuk Word Embeddings, Term Frequency-Inverse Document Frequency (TF-IDF), Bag-of-Words (BoW), dan N-gram. Melalui teknik-teknik ini, kita dapat mengungkap pola-pola penting dalam teks dan membangun model pembelajaran mesin yang memberikan wawasan berharga.


Word Embedding
Bayangkan Anda lagi main puzzle kata-kata. Anda punya sejumlah kata-kata yang tersusun dalam kalimat-kalimat, tetapi bagaimana cara mesin memahami makna sebenarnya dari kata-kata ini?

Nah, di situlah "word embedding" masuk ke permainan. Hal ini seperti memberi setiap kata-kata tempat spesial dalam ruang matematis yang besar. Bayangkan ruang itu seperti tempat parkir yang luas dan setiap kata punya tempat parkirnya sendiri.

Contohnya, kata-kata yang sering digunakan bersama-sama, seperti "anjing" dan "kucing", parkirnya dekat satu sama lain karena sering muncul dalam konteks yang sama. Sementara kata-kata seperti "meja" atau "laptop" mungkin parkirnya lebih jauh dari mereka karena maknanya berada dalam konteks berbeda.

Kode berikut adalah contoh implementasi Word2Vec menggunakan library Gensim dan NLTK di Python. Mari kita kupas bagian demi bagian.

1. Pertama-tama, kita mengimpor modul yang dibutuhkan.
from gensim.models import Word2Vec
from nltk.tokenize import word_tokenize
import nltk

2. Kemudian, kita men-download modul Punkt tokenizer dari NLTK.
nltk.download('punkt')

3. Langkah selanjutnya adalah mendefinisikan contoh data teks yang akan kita gunakan untuk melatih model Word2Vec.
text_data = [
    'Saya suka makan bakso',
    'Bakso enak dan lezat',
    'Makanan favorit saya adalah nasi goreng',
    'Nasi goreng pedas adalah makanan favorit saya',
    'Saya suka makanan manis seperti es krim',
]

4. Setelah itu, kita melakukan tokenisasi pada teks tersebut.
tokenized_data = [word_tokenize(sentence.lower()) for sentence in text_data]

5. Sekarang, saatnya membangun model Word2Vec menggunakan data teks yang sudah di-tokenisasi.
model = Word2Vec(sentences=tokenized_data, vector_size=100, window=5, min_count=1, workers=4)

Di sini, parameter yang kita gunakan sebagai berikut.
- sentences: Data teks yang telah di-tokenisasi.
- vector_size: Ukuran dari vektor representasi kata. Dalam kasus ini, 100.
- window: Jumlah maksimum kata-kata yang dianggap konteks dekat dalam satu kalimat.
- min_count: Jumlah minimum kemunculan sebuah kata dalam korpus agar kata tersebut diperhitungkan.
- workers: Jumlah thread yang digunakan dalam proses pembangunan model.

6. Setelah model dibangun, kita bisa menggunakan vektor kata untuk kata-kata tertentu atau mencari kata-kata yang mirip dengan kata tertentu.
word_vectors = model.wv
 
similar_words = word_vectors.most_similar('bakso', topn=3)
print("Kata-kata yang mirip dengan 'bakso':", similar_words)
 
vector = word_vectors['bakso']
print("Vektor untuk 'bakso':", vector)

Dalam contoh ini, kita mencari kata-kata yang mirip dengan 'bakso' dan mendapatkan vektor representasinya.

Jadi, dengan menggunakan Word2Vec, kita bisa melatih model untuk membuat representasi vektor dari kata-kata dalam teks yang berguna pada berbagai tugas NLP.

Output-nya berikut.

Kata-kata yang mirip dengan 'bakso': [('manis', 0.2529163062572479), ('nasi', 0.17018672823905945), ('enak', 0.15006466209888458)]


Hasil yang kita dapatkan dari kode tersebut adalah berupa dua hal, yaitu kata-kata yang mirip dengan 'bakso' dan vektor representasi untuk kata 'bakso'.

a. Kata-kata yang mirip dengan 'bakso'
Dari hasil pencarian, kita dapatkan tiga kata yang mirip dengan 'bakso', yaitu 'manis', 'nasi', dan 'enak'. Nilai yang tercantum di samping kata-kata tersebut adalah ukuran seberapa miripnya kata-kata tersebut dengan 'bakso'. Semakin tinggi nilai tersebut, semakin mirip kata-kata tersebut dengan 'bakso'. Jadi, dalam kasus ini, 'manis' adalah kata yang paling mirip dengan 'bakso', diikuti oleh 'nasi' dan 'enak'.

b. Vektor untuk 'bakso'
Vektor ini adalah representasi matematis dari kata 'bakso' dalam ruang vektor. Setiap angka dalam vektor tersebut mewakili fitur atau atribut tertentu dari kata 'bakso'.


Misalnya, nilai pertama mungkin mewakili seberapa sering kata 'bakso' muncul dalam konteks tertentu, nilai kedua mungkin mewakili hubungan kata 'bakso' dengan kata-kata lain, seperti 'makanan' atau 'kuliner' dan seterusnya. Dengan vektor ini, mesin dapat memahami kata 'bakso' secara matematis dan menggunakannya dalam berbagai analisis atau tugas pemrosesan bahasa alami.



Term Frequency-Inverse Document Frequency (TF-IDF):
Di sinilah TF-IDF (Term Frequency-Inverse Document Frequency) masuk permainan. Ini seperti memberi bobot kepada setiap kata dalam dokumen berdasarkan seberapa sering kata itu muncul pada dokumen itu sendiri (frekuensi kata) dan seberapa umumnya kata itu muncul di seluruh kumpulan dokumen.

Misalnya, kata-kata yang muncul banyak dalam satu dokumen, tetapi jarang muncul pada dokumen-dokumen lain mungkin dianggap lebih penting. Contohnya, dalam sebuah artikel tentang bakso, kata "bakso" mungkin muncul berkali-kali, tetapi mungkin jarang muncul pada artikel-artikel yang tidak berkaitan dengan makanan.

Untuk menghitung TF-IDF, kita bisa menggunakan alat seperti TfidfVectorizer pada Python. Ini membantu kita mengukur pentingnya kata-kata dalam setiap dokumen berdasarkan logika yang dibahas tadi. Jadi, dengan TF-IDF, kita bisa menemukan kata-kata yang paling mencirikan atau penting dalam setiap dokumen.

Kode berikut adalah contoh implementasi TF-IDF menggunakan library TfidfVectorizer pada Python. Mari kita kupas bagian demi bagian.

1. Pertama-tama, kita mengimpor modul yang dibutuhkan.
from sklearn.feature_extraction.text import TfidfVectorizer

2. Selanjutnya, kita memiliki data teks contoh yang terdiri dari beberapa dokumen.
documents = [
    "Saya suka makan bakso",
    "Bakso enak dan lezat",
    "Makanan favorit saya adalah nasi goreng",
    "Nasi goreng pedas adalah makanan favorit saya",
    "Saya suka makanan manis seperti es krim",
]

3. Setelah itu, kita inisialisasi objek TfidfVectorizer.
tfidf_vectorizer = TfidfVectorizer()

4. Lalu, kita menghitung TF-IDF dari dokumen-dokumen tersebut.
tfidf_matrix = tfidf_vectorizer.fit_transform(documents)

Di sini, TF-IDF (Term Frequency-Inverse Document Frequency) menghitung seberapa sering sebuah kata muncul dalam sebuah dokumen, lalu dibandingkan dengan seberapa sering kata tersebut muncul di seluruh koleksi dokumen. Ini membantu untuk menemukan kata-kata yang penting dalam sebuah dokumen.

5. Setelah itu, kita bisa melihat vocabulary (kata unik) yang dihasilkan oleh TfidfVectorizer.
print("Vocabulary:", tfidf_vectorizer.vocabulary_)

Vocabulary: {'saya': 14, 'suka': 16, 'makan': 9, 'bakso': 1, 'enak': 3, 'dan': 2, 'lezat': 8, 'makanan': 10, 'favorit': 5, 'adalah': 0, 'nasi': 12, 'goreng': 6, 'pedas': 13, 'manis': 11, 'seperti': 15, 'es': 4, 'krim': 7}


6. Terakhir, kita bisa melihat hasil dari TF-IDF matrix dalam bentuk array.
print("TF-IDF Matrix:")
print(tfidf_matrix.toarray())

Dengan TF-IDF Matrix ini, kita bisa melihat kata-kata yang paling penting dalam setiap dokumen berdasarkan konteksnya. Semakin tinggi nilai dalam tabel, semakin penting kata tersebut pada dokumen tersebut.
 


Bag of Words (BoW)
Bag of Words (BoW) adalah pendekatan sederhana dalam pemrosesan teks yang mengubah teks menjadi representasi numerik. Ide dasarnya adalah kita menganggap setiap dokumen sebagai "tas" (bag) kata-kata dan hanya peduli tentang keberadaan kata-kata dalam dokumen tersebut, bukan urutan atau konteksnya. Kemudian, untuk setiap dokumen, kita hitung berapa kali setiap kata muncul. 

Hasilnya adalah matriks, yakni setiap baris mewakili sebuah dokumen dan setiap kolom mewakili kata-kata unik dalam seluruh kumpulan dokumen. Dengan cara ini, BoW memungkinkan kita mengukur kemunculan kata-kata dalam teks secara numerik, yang dapat digunakan untuk berbagai analisis teks, yakni klasifikasi dokumen, analisis sentimen, dan banyak lagi. 

1. Import library CountVectorizer dari Scikit-learn yang digunakan untuk mengubah teks menjadi representasi Bag of Words (BoW).
from sklearn.feature_extraction.text import CountVectorizer

2. Data teks berisi beberapa dokumen contoh yang akan diolah.
documents = [
    "Ini adalah contoh dokumen pertama.",
    "Ini adalah dokumen kedua.",
    "Ini adalah dokumen ketiga.",
    "Ini adalah contoh contoh contoh."
]

3. Inisialisasi objek CountVectorizer.
vectorizer = CountVectorizer()

4. Melakukan fitting dan transformasi pada data teks menggunakan CountVectorizer. Proses ini akan menghitung frekuensi kemunculan setiap kata dalam setiap dokumen.
bow_matrix = vectorizer.fit_transform(documents)

5. Setelah transformasi, kita mendapatkan matriks Bag of Words (BoW), yang merupakan representasi numerik dari teks. Matriks ini berisi jumlah kemunculan setiap kata dalam setiap dokumen.
bow_matrix.toarray()

6. Kita juga mendapatkan daftar fitur (kata-kata) yang dihasilkan oleh CountVectorizer.
features = vectorizer.get_feature_names_out()

7. Hasilnya, kita mencetak matriks BoW beserta daftar fitur yang dihasilkan.
print("Matriks BoW:")
print(bow_matrix.toarray())
 
print("\nDaftar Fitur:")
print(features)

Matriks BoW:
[[1 1 1 1 0 0 1]
 [1 0 1 1 1 0 0]
 [1 0 1 1 0 1 0]
 [1 3 0 1 0 0 0]]
 
Daftar Fitur:
['adalah' 'contoh' 'dokumen' 'ini' 'kedua' 'ketiga' 'pertama']

Dengan menggunakan CountVectorizer, kita bisa mengonversi teks menjadi representasi numerik yang dapat diproses lebih lanjut oleh algoritma pembelajaran mesin atau analisis statistik. Ini memungkinkan kita untuk melakukan berbagai analisis dan pemrosesan teks dengan menggunakan teknik-teknik pemrosesan bahasa alami.

Matriks Bag of Words (BoW) adalah representasi numerik dari teks yang menunjukkan jumlah kemunculan setiap kata dalam setiap dokumen. Pada contoh ini, matriks BoW memiliki 4 baris yang mewakili 4 dokumen dan 7 kolom yang mewakili 7 kata unik dalam teks.

Misalnya, dalam dokumen pertama, kata 'adalah', 'contoh', 'dokumen', dan 'ini' muncul masing-masing 1 kali, sementara kata 'kedua', 'ketiga', dan 'pertama' tidak muncul. Daftar fitur adalah daftar kata-kata unik yang diurutkan secara alfabetis.



N-gram:

1. Import library yang diperlukan.
from nltk.util import ngrams

2. Tentukan beberapa kalimat contoh.
sentences = [
    "Saya suka makan bakso enak di warung dekat rumah.",
    "Nasi goreng adalah salah satu makanan favorit saya.",
    "Es krim coklat sangat lezat dan menyegarkan.",
    "Saat hari hujan, saya suka minum teh hangat.",
    "Pemandangan pegunungan di pagi hari sangat indah.",
    "Bola basket adalah olahraga favorit saya sejak kecil."
]

3. Iterasi melalui setiap kalimat dalam daftar kalimat.
for sentence in sentences:

4. Bagi setiap kalimat menjadi kata-kata individu.
words = sentence.split()

5. Buat 1-gram (unigram) dari kata-kata dalam kalimat tersebut.
unigrams = list(ngrams(words, 1))

6. Buat 2-gram (bigram) dari kata-kata dalam kalimat tersebut.
bigrams = list(ngrams(words, 2))

7. Buat 3-gram (trigram) dari kata-kata dalam kalimat tersebut.
trigrams = list(ngrams(words, 3))

8. Cetak hasil untuk setiap kalimat, termasuk unigram, bigram, dan trigram.
print("\nKalimat:", sentence)
print("1-gram:")
for gram in unigrams:
    print(gram)
print("\n2-gram:")
for gram in bigrams:
    print(gram)
print("\n3-gram:")
for gram in trigrams:
    print(gram)

Dengan tahapan ini, kita dapat membagi setiap kalimat menjadi unigram, bigram, dan trigram, serta menampilkan hasilnya. Dengan demikian, kita bisa melihat bahwa kata-kata pada teks tersebut berhubungan satu sama lain dalam berbagai tingkatan.



Binary vs Multiclass vs Multilabel Classification pada Text:

Pertama, binary classification, ini mirip dengan memilih antara dua pilihan: ya atau tidak, 0 atau 1. Di sini, kita mengelompokkan teks dalam dua kategori berbeda, misalnya positif dan negatif, spam dan bukan spam.

Kedua, multiclass classification, ini seperti memilih dari beberapa pilihan yang berbeda. Hal ini seperti saat kita harus memilih satu dari beberapa destinasi liburan favorit. Pada konteks klasifikasi teks, kita mengelompokkan teks dalam lebih dari dua kelas berbeda, seperti genre film atau kategori produk.

Ketiga, multilabel classification, ini serupa dengan memilih lebih dari satu jawaban yang benar. Misalnya, jika diminta untuk memilih hobi, Anda bisa memilih lebih dari satu, seperti membaca, berenang, dan hiking. Pada klasifikasi teks, kita dapat memiliki teks yang termasuk dalam beberapa kategori atau label sekaligus.

Jadi, tergantung pada tugas dan kebutuhan, kita bisa memilih salah satu dari ketiga pendekatan ini untuk mengklasifikasikan teks sesuai dengan tujuan. Nah, mari telusuri lebih dalam tentang masing-masing pendekatan ini dan momen kita sebaiknya menggunakannya.



1. Binary Classification

Binary classification adalah sebuah teknik dalam machine learning untuk memisahkan data pada dua kelas atau kategori yang saling eksklusif berdasarkan pemberian fitur atau atribut. Dalam binary classification, ada dua kelas yang mungkin untuk diprediksi, biasanya disebut sebagai kelas positif dan kelas negatif. 

Misalnya, dalam kasus analisis sentimen teks, kita mungkin ingin memprediksi bahwa sebuah email adalah "spam" atau "tidak spam". Di sini, "spam" dan "tidak spam" akan menjadi dua kelas dalam binary classification.

Contoh algoritma yang dapat digunakan untuk binary classification, yaitu Logistic Regression, Support Vector Machines (SVM), Decision Trees, Neural Networks, dan lainnya. Pengukuran performa yang umum digunakan untuk evaluasi model binary classification meliputi akurasi (accuracy), presisi (precision), recall, F1-score, dan Area Under the Receiver Operating Characteristic Curve (AUROC).

Dengan demikian, binary classification adalah salah satu tipe dasar dari klasifikasi dalam machine learning yang penting dan banyak digunakan pada berbagai aplikasi, seperti analisis sentimen, deteksi fraud, klasifikasi medis, dan banyak lagi.



2. Multiclass Classification

Multiclass classification adalah jenis masalah klasifikasi dalam machine learning bahwa model harus memprediksi kelas atau label dari data dalam lebih dari dua kategori yang berbeda. Dalam konteks ini, setiap contoh data dapat diklasifikasikan ke salah satu dari beberapa kelas dan setiap kelas mewakili kategori yang berbeda.

Misalnya, pada masalah pengenalan gambar untuk mengklasifikasikan genre film dalam kategori, seperti "Horror", "Action", "Science Fiction", dan "Western", kita memiliki empat kelas yang berbeda. Dalam multiclass classification, model akan belajar untuk mengenali pola atau fitur yang membedakan setiap kategori ini dan mengasosiasikan instance data dengan kelas yang paling sesuai.

Beberapa algoritma yang dapat digunakan untuk multiclass classification, antara lain, Logistic Regression (yang bisa digunakan dalam bentuk One-vs-Rest atau softmax), Support Vector Machines (SVM), Decision Trees, Random Forests, k-Nearest Neighbors (k-NN), dan Neural Networks.

Metrik evaluasi yang umum digunakan untuk mengukur performa model pada multiclass classification, termasuk akurasi, presisi, recall, F1-score, dan matriks konfusi (confusion matrix) dalam melihat seberapa baik model dapat memprediksi setiap kelas. 

Multiclass classification digunakan dalam berbagai aplikasi, yaitu pengenalan gambar, klasifikasi teks, klasifikasi medis, identifikasi objek, dan banyak lagi. 


3. Multi-label Classification

Multi-label classification adalah jenis masalah klasifikasi pada machine learning bahwa setiap instance data dapat dikategorikan dalam lebih dari satu label atau kelas sekaligus. Dengan kata lain, beberapa label atau kategori yang relevan dapat diberikan kepada satu instance data.

Contoh yang umum untuk multi-label classification adalah analisis teks untuk jenis dokumen. Misalnya, untuk sebuah artikel berita, kita ingin mengidentifikasi beberapa topik atau kategori relevan yang dapat digunakan sebagai tag, seperti "Anatomy", "Disease", "Organisms", "Information Science", dan sebagainya. Suatu artikel dapat memiliki beberapa tag atau label yang mencerminkan topik-topik pembahasan di dalamnyaa 


Proses multi-label classification melibatkan pembelajaran dari data latih yang telah dilabeli dengan beberapa label yang benar. Model machine learning akan belajar untuk memahami hubungan fitur-fitur dari data dengan kumpulan label-label yang mungkin relevan.

Dalam multi-label classification, output dari model berupa himpunan (set) dari label-label yang diprediksi untuk setiap instance data. Setiap label bisa hadir atau tidak hadir sebagai bagian dari prediksi. Misalnya, pada artikel berita tertentu, prediksi model bisa berupa himpunan label seperti {"Anatomy", "Disease"} untuk menunjukkan topik-topik yang dianggap relevan.

Algoritma yang dapat digunakan untuk multi-label classification, yakni Binary Relevance, Classifier Chains, Label Powerset, dan lainnya. Algoritma-algoritma ini memungkinkan model untuk mengatasi masalah klasifikasi dengan banyak label dan menghasilkan prediksi yang sesuai dengan kebutuhan.

Metrik evaluasi yang umum digunakan untuk mengukur performa model dalam multi-label classification, antara lain, Hamming Loss, Precision at k (P@k), Average Precision Score, dan lainnya. Evaluasi dilakukan dengan mempertimbangkan seberapa baik model dapat memprediksi himpunan label yang tepat untuk setiap instance data.

Aplikasi dari multi-label classification, yakni analisis teks, pengindeksan konten, klasifikasi gambar, kategorisasi produk, sistem rekomendasi, dan banyak lagi. Pemilihan algoritma serta strategi pendekatan yang tepat sangat tergantung dari sifat data dan tujuan bisnis pada analisis yang dilakukan.

Dengan memahami perbedaan antara binary, multiclass, dan multilabel classification dalam konteks klasifikasi teks, kita sekarang memiliki landasan kuat untuk menjelajahi serta menerapkan pendekatan yang paling sesuai dengan tujuan dan kebutuhan proyek. 

Meskipun ketiganya memiliki pendekatan berbeda, tetapi semuanya memiliki peran penting dalam analisis teks yang mendalam. Mari kita terus eksplorasi serta memanfaatkan berbagai teknik ini untuk memecahkan tantangan yang kompleks dan menarik dalam dunia pemrosesan bahasa alami.





Algoritma RNN (Recurrent Neural Network):


"RNN itu istimewa karena bisa memahami konteks dari urutan data. Misalnya, kalau kita sedang membaca kalimat, RNN bisa mengingat kata-kata sebelumnya untuk memahami kata yang sedang diproses."

Algoritma ini seperti saat kita menemukan kunci yang dapat membuka pintu untuk menjelajahi dunia baru dalam pemrosesan data berurutan. 

Dengan kekuatan dan fleksibilitasnya, RNN memungkinkan kita untuk memahami serta menganalisis teks, waktu, atau audio dengan cara yang lebih mendalam dan intuitif. Bayangkan RNN seperti alur cerita yang terhubung satu sama lain dan memungkinkan kita untuk "mengingat" informasi dari masa lalu ketika memproses input saat ini.

Pemrosesan Data Sekuensial

Pemrosesan data sekuensial adalah bidang ilmu berfokus pada analisis dan pemodelan data dengan keterkaitan antar elemennya, serta urutan data menjadi kunci untuk memahami maknanya. Data sekuensial ini berbeda dengan data tradisional yang statis dan independen satu sama lain.

Contoh data sekuensial yang umum ditemukan sebagai berikut.
- Teks: Urutan kata dalam kalimat, paragraf, dan dokumen.
- Audio: Sinyal suara yang berubah-ubah waktu, seperti pidato, musik, dan efek suara.
- Video: Urutan gambar yang membentuk sebuah video dengan informasi gerak dan perubahan waktu.
- Sinyal sensor: Data yang dikumpulkan dari sensor untuk memantau perubahan lingkungan, seperti suhu, tekanan, dan akselerasi.
- Aktivitas manusia: Urutan tindakan yang dilakukan manusia, seperti pola pergerakan, interaksi sosial, dan aktivitas sehari-hari.

Pemrosesan data sekuensial adalah cara kita memproses atau mengolah data satu per satu secara berurutan, seperti membaca buku dari halaman pertama hingga halaman terakhir. Hal ini berarti kita mengambil satu data pada satu waktu dan melakukan sesuatu dengan data tersebut sebelum melanjutkan ke data berikutnya.

Gagasan tentang pemrosesan data sekuensial mengharuskan pengembangan algoritma yang canggih untuk mengatasi tantangan ini. Salah satu contoh yang terkenal adalah RNN yang akan dipelajari lebih lanjut dalam materi berikutnya. Stay tuned, ya!


Pengenalan RNN

RNN adalah jenis arsitektur deep learning yang dirancang khusus untuk mengolah data sekuensial. Artinya, informasi dari langkah-langkah sebelumnya dalam urutan data memiliki pengaruh pada langkah-langkah selanjutnya. RNN dapat mengatasi masalah bahwa data yang diproses memiliki struktur berurutan atau bergantung pada konteks waktu.

Keunggulan utama RNN adalah kemampuannya untuk menangani urutan input variabel dalam panjang dan mengingat informasi dari langkah-langkah sebelumnya. Ini membuat RNN sangat berguna dalam berbagai tugas pemrosesan bahasa alami, pemodelan waktu, pemrosesan bahasa, dan banyak lagi.


Mekanisme Dasar RNN:
RNN memiliki struktur mirip dengan artificial neural network (ANN), dengan neuron-neuron yang terhubung dalam lapisan. Namun, ada perbedaan utama sebagai berikut.

- Loop internal: RNN memiliki loop internal yang menghubungkan neuron pada layer tersembunyi. Loop ini memungkinkan informasi dari input sebelumnya untuk disimpan dan digunakan dalam pemrosesan input selanjutnya.
- State: RNN memiliki state internal yang diperbarui dengan setiap input baru. State ini mewakili informasi yang "diingat" oleh RNN dari input sebelumnya.

Misalkan RNN memproses sebuah kalimat. Pada kata pertama, RNN menerima input kata tersebut dan memprosesnya. Informasi ini kemudian disimpan dalam memori jangka pendek. Saat kata kedua diterima, RNN tidak hanya memproses kata tersebut, tetapi juga mempertimbangkan informasi dari kata pertama yang disimpan dalam memori. Hal ini memungkinkan RNN untuk memahami konteks kalimat dan menghasilkan output yang lebih akurat.

Selain memori jangka pendek, RNN juga memiliki state. State ini mewakili representasi internal dari urutan input yang telah diproses. State diperbarui dengan setiap input baru, ini memungkinkan RNN untuk mengikuti perkembangan urutan data.

Misalkan RNN memproses deret waktu harga saham. Pada setiap langkah waktu, RNN menerima harga saham saat ini dan memperbaikinya. State RNN akan terus berubah seiring dengan perubahan harga saham. Ini memungkinkan RNN untuk mempelajari pola dan tren dalam deret waktu.


Struktur RNN:
RNN adalah jenis jaringan saraf yang memiliki struktur sedikit berbeda dari jaringan saraf biasa. Dalam RNN, ada lapisan masukan (input layer), lapisan tersembunyi (hidden layer), dan lapisan keluaran (output layer), mirip dengan jaringan saraf lainnya.

Namun, pembedanya dengan RNN adalah adanya koneksi rekuren, yang memungkinkan informasi untuk mengalir dari satu langkah waktu ke langkah waktu berikutnya dalam urutan data. Ini berarti setiap langkah waktu memiliki koneksi ke langkah waktu sebelumnya. Ini memungkinkan RNN untuk "mengingat" informasi dari langkah-langkah sebelumnya dalam urutan tersebut.

Struktur dasar RNN terdiri dari tiga komponen utama.
1. Input
Input ke RNN dapat berupa urutan data apa pun, seperti kata-kata dalam kalimat, sampel dalam sinyal waktu, atau piksel dalam gambar.

2. Recurrent Unit
Jantung dari RNN adalah unit berulang, yakni fungsi non-linear yang memproses input saat ini dan keadaan tersembunyi sebelumnya untuk menghasilkan output dan keadaan tersembunyi baru. Keadaan tersembunyi berisi informasi tentang input yang telah dilihat jaringan sejauh ini dan digunakan untuk memprediksi output di masa depan.

3. Output
Output dari RNN dapat berupa apa pun yang Anda inginkan, seperti kata berikutnya dalam kalimat, nilai berikutnya dalam sinyal waktu, atau deskripsi gambar.


Ciri utama RNN adalah adanya "rekurensi" atau siklus yang memungkinkan informasi untuk dioperasikan berulang kali dalam jaringan. Hal ini memungkinkan RNN untuk mempertimbangkan konteks historis dari urutan data saat ini yang sedang diproses.

Namun, RNN memiliki beberapa tantangan, termasuk kesulitan dalam mempertahankan informasi jangka panjang dan mengatasi masalah gradien yang melemah (vanishing gradient problem) saat melatih model dengan urutan panjang. Untuk mengatasi masalah ini, beberapa varian RNN telah dikembangkan, seperti long short-term memory (LSTM) dan gated recurrent unit (GRU), yang dirancang khusus mengatasi masalah memori jangka panjang dan gradien yang melemah.


A. Long Short-Term Memory (LSTM)
Jaringan LSTM diperkenalkan oleh Sepp Hochreiter dan Jürgen Schmidhuber pada tahun 1997 untuk mengatasi keterbatasan ini. LSTM memiliki struktur yang lebih kompleks dibandingkan dengan RNN tradisional, dengan fitur sel memori dan berbagai mekanisme gating yang mengontrol aliran informasi.

LSTM adalah jenis ANN yang termasuk kategori Recurrent Neural Network (RNN). LSTM didesain khusus untuk mengatasi keterbatasan RNN konvensional dalam menangani data sekuensial dengan ketergantungan jangka panjang.

Keunggulan Utama LSTM:
- Memiliki memori jangka panjang: LSTM mampu menyimpan informasi penting dari data sekuensial dalam jangka waktu yang lama, bahkan ketika terdapat celah waktu yang panjang antara data yang relevan.

- Mengatasi masalah vanishing gradient problem: LSTM tidak mudah terpengaruh oleh masalah gradien menghilang yang sering terjadi pada RNN konvensional sehingga memungkinkan LSTM untuk belajar dari data sekuensial yang lebih panjang.

- Lebih fleksibel: LSTM dapat dimodifikasi dengan mudah untuk berbagai aplikasi, seperti terjemahan mesin, pengenalan suara, dan analisis deret waktu.


Struktur LSTM:

LSTM terdiri dari beberapa komponen utama sebagai berikut.

- Sel memori: Menyimpan informasi penting dari data sekuensial.
- Gerbang atau Gate: Mengontrol aliran informasi ke dalam dan luar sel memori. Ada tiga jenis gerbang.
	- Input Gate: Mengatur informasi baru yang akan ditambahkan ke sel memori.
	- Forget Gate: Mengatur informasi yang akan dihapus dari sel memori.
	- Output Gate: Mengatur informasi yang akan dikeluarkan dari sel memori.


Berbicara terkait jenis-jenis gate pada LSTM, berikut penjelasan lebih lengkapnya.

- Forget Gate:
Forget gate adalah gerbang yang memutuskan seberapa banyak informasi dari memori sel sebelumnya harus dilupakan. Nilai forget gate berkisar antara 0 dan 1, 0 berarti semua informasi akan dihapus dan 1 berarti tidak ada informasi yang dihapus.

Bayangkan Anda memiliki kotak memori untuk menyimpan informasi penting. Kotak ini memiliki kapasitas terbatas, jadi Anda tidak bisa menyimpan semuanya. Di sinilah forget gate berperan, seperti petugas kebersihan khusus untuk kotak memori Anda.

Forget gate menerima dua input.
- Input saat ini ????(????), yang dapat berupa input baru dalam urutan atau data apa pun yang masuk ke LSTM pada langkah waktu ????.

- Output dari sel LSTM sebelumnya ht−1, disebut juga sebagai hidden state pada langkah waktu sebelumnya.

Rumus Forget Gate LSTM:

- Ft adalah output dari forget gate pada langkah waktu t.
- o- adalah fungsi sigmoid, yang menghasilkan nilai antara 0 dan 1.
- Wf adalah matriks bobot untuk forget gate.
- ht−1 adalah output (hidden state) dari langkah waktu sebelumnya.
- xt adalah input pada langkah waktu t.
- bf adalah bias untuk forget gate.

Operasi [ht−1, xt] menunjukkan penggabungan (concatenation) output dari langkah waktu sebelumnya dan input pada langkah waktu saat ini. Output operasi ini adalah nilai biner yang menentukan seberapa banyak informasi dari sel LSTM sebelumnya akan dilupakan. Nilai yang mendekati 1 menunjukkan bahwa informasi tersebut harus diingat, sementara nilai yang mendekati 0 menunjukkan bahwa informasi tersebut harus dilupakan.


- Input Gate
Input gate dalam LSTM adalah komponen yang bertanggung jawab untuk mengatur seberapa banyak informasi baru akan ditambahkan ke memori jangka panjang sel LSTM. Input gate memutuskan seberapa pentingnya setiap bagian informasi baru yang masuk dan berkontribusi terhadap pembaruan memori sel.

LSTM itu seperti menghadiri pesta dengan banyak orang yang ingin berbagi informasi (data). Input gate berperan sebagai penjaga selektif untuk mengatur siapa saja yang boleh masuk dan berinteraksi dengan Anda (sel memori) di pesta tersebut.

Ada 2 langkah yang dilakukan oleh input gate.
- Input gate menggunakan sigmoid untuk menentukan pentingnya setiap elemen dalam input baru; mendekati 1 untuk elemen penting dan mendekati 0 untuk yang tidak penting. 
- Kemudian, nilai ini digunakan untuk mengalikan kandidat nilai baru yang dihasilkan oleh fungsi tanh dan menghasilkan usulan informasi baru untuk ditambahkan ke memori sel LSTM.

Rumus:

- ft adalah output dari input gate pada langkah waktu t.
- o- adalah fungsi sigmoid.
- Wf adalah matriks bobot untuk input gate.
- ht−1 adalah output (hidden state) dari langkah waktu sebelumnya.
- xt adalah input pada langkah waktu t.
- bf adalah bias untuk input gate. 

 Dengan menggunakan xt  dan ht−1, LSTM dapat mempertimbangkan informasi baru dan informasi yang telah disimpan sebelumnya saat membuat keputusan tentang hal yang harus dilakukan selanjutnya, seperti hal yang harus diingat, dilupakan, atau diperbarui dalam memori jangka panjang.


- Output Gate
Output gate dalam LSTM bertanggung jawab untuk mengontrol seberapa banyak informasi dari memori sel yang akan dikeluarkan sebagai output pada langkah waktu tertentu. Hal ini memungkinkan LSTM untuk memilih informasi paling relevan dan membatasi informasi kurang penting atau tidak relevan yang akan disampaikan ke langkah waktu berikutnya atau lapisan keluaran (output layer) dari model. 

Secara matematis, output gate dihitung menggunakan rumus berikut.

- Ot adalah output dari output gate pada langkah waktu t.
- o- adalah fungsi sigmoid.
- Wo adalah matriks bobot untuk output gate.
- Ht−1 adalah output (hidden state) dari langkah waktu sebelumnya.
- xt adalah input pada langkah waktu t.
- bo adalah bias untuk output gate.


Output gate menghasilkan nilai antara 0 dan 1 untuk setiap elemen dalam output, menentukan seberapa banyak informasi dari memori sel yang akan dikeluarkan. Nilai mendekati 1 menunjukkan elemen yang sangat penting dan harus dikeluarkan, sedangkan nilai yang mendekati 0 menunjukkan elemen kurang penting dan bisa diabaikan.


Gated Recurrent Unit (GRU):

GRU adalah jenis unit pada RNN yang dikembangkan untuk memodelkan ketergantungan jarak jauh dalam data urutan dengan cara yang lebih efisien daripada model sebelumnya, seperti unit LSTM. GRU diperkenalkan oleh Kyunghyun Cho pada tahun 2014 sebagai alternatif yang lebih sederhana dan komputasional lebih efisien.

Berbeda dengan LSTM yang memiliki tiga gate (forget gate, input gate, dan output gate), GRU hanya memiliki dua gate (update gate dan reset gate). Hal ini membuat struktur GRU menjadi lebih sederhana dan komputasional lebih efisien.

Keunggulan Utama GRU:
1. Komputasi Lebih Ringan: GRU memiliki struktur yang lebih sederhana daripada LSTM sehingga membutuhkan lebih sedikit sumber daya komputasi.
2. Training yang Lebih Cepat: Karena strukturnya yang lebih sederhana, GRU cenderung lebih cepat dalam proses pelatihan dibandingkan dengan LSTM.
3. Pencegahan Vanishing Gradient: GRU memiliki mekanisme update yang memungkinkannya mengatasi masalah vanishing gradient, meskipun tidak sekompleks LSTM.
4. Penghematan Memori: GRU memerlukan lebih sedikit parameter daripada LSTM, ini dapat menghemat memori saat melakukan pelatihan dan inferensi pada model.

Struktur GRU:
GRU terdiri dari beberapa komponen utama sebagai berikut.

- Sel memori: Menyimpan informasi penting dari data sekuensial.
- Gerbang atau Gate: Mengontrol aliran informasi ke dalam dan luar dari sel memori. Ada dua jenis gate.

	- Reset gate: Mengontrol seberapa banyak informasi dari status tersembunyi sebelumnya yang akan disimpan.
	- Update Gate: Mengontrol seberapa banyak informasi baru dari masukan saat ini yang akan ditambahkan ke hidden state

Berbicara terkait jenis-jenis gate pada GRU, berikut penjelasan lebih lengkapnya.

1. Reset Gate
Reset gate, diwakili oleh rt, memutuskan seberapa banyak informasi dari masa lalu yang akan diabaikan atau dilupakan dalam komputasi saat ini. Inputnya terdiri dari output dari langkah waktu sebelumnya, ht−1, dan input saat ini, xt 

Bayangkan Anda sedang belajar di sebuah ruangan. Setiap kali Anda memasuki ruangan untuk belajar, Anda melihatnya dalam keadaan kosong. Namun, sebelum mulai belajar, Anda melakukan pengecekan cepat untuk melihat adakah informasi penting dari sesi belajar sebelumnya yang harus diingat.

Jika tidak ada yang penting, Anda "mengatur ulang" ruangan itu dengan menghapus semua yang tidak relevan. Jika ada sesuatu yang masih relevan atau penting untuk diingat, Anda membiarkannya tetap ada. 

Persamaan untuk reset gate adalah berikut.
- Wr adalah matriks bobot,
- ht−1 adalah output sebelumnya,
- xt adalah input saat ini,
- br adalah bias, dan
- o- adalah fungsi sigmoid.


Update gate menentukan banyak informasi dari output terakhir yang akan diteruskan ke output saat ini. Output dari reset gate adalah vektor nilai antara 0 dan 1, yang menentukan seberapa banyak informasi dari masa lalu yang harus dilupakan atau diabaikan dalam komputasi saat ini.


2. Update Gate
Update gate, diwakili oleh zt, menentukan seberapa banyak informasi dari output terakhir yang akan diteruskan ke output saat ini. Inputnya juga terdiri dari output pada langkah waktu sebelumnya, ht−1, dan input saat ini, xt. 

Sekarang, bayangkan bahwa Anda belajar di ruangan yang memiliki papan tulis. Setiap kali belajar, Anda menulis informasi baru di papan tulis tersebut. Namun, sebelum menulis informasi baru, Anda memutuskan bahwa informasi yang telah ada di papan tulis masih relevan atau perlu diperbarui. 

Jika informasi sebelumnya masih relevan, Anda membiarkannya tetap ada. Namun, jika informasi sebelumnya tidak lagi relevan atau perlu diperbarui dengan informasi baru, Anda menghapusnya sebelum menulis yang baruu.

Persamaan untuk update gate sebagai berikut.
Wz adalah matriks bobot,
ht−1 dan xt adalah output dan input,
bz adalah bias, dan
o- adalah fungsi sigmoid.

Output dari update gate adalah vektor nilai antara 0 dan 1 dan menentukan seberapa banyak informasi dari output terakhir yang akan diteruskan ke output saat ini. Ketika update gate mendekati 1, itu menunjukkan bahwa model harus mengingat informasi saat ini dengan sangat baik. Ketika update gate mendekati 0, itu menunjukkan bahwa model harus memperbarui informasi lama dengan yang baru.


Contoh Pengaplikasian RNN
Berikut adalah beberapa contoh pengaplikasian RNN.

Natural language processing (NLP): Terjemahan bahasa, analisis sentimen, dll.
Pengenalan suara: Pengenalan ucapan, generasi ucapan, dll.
Pengenalan gambar: Klasifikasi gambar, deskripsi gambar, dll.
Time series: Prediksi harga saham, deteksi anomali, dll.
Text generation: Pembuatan cerita, penyusunan lagu, dll.
Robotika: Kontrol gerakan robot.
Biomedis: Pendeteksian penyakit berdasarkan data medis.



Studi Kasus Implementasi Klasifikasi Teks pada NLP: Sentimen Analisis Review APK Play Store

Sentimen Analisis Review APK Play Store adalah proses menganalisis dan mengevaluasi sentimen atau perasaan yang diungkapkan oleh pengguna dalam ulasan atau review mereka di Play Store, platform distribusi aplikasi Android resmi dari Google. 

Tujuannya adalah memahami bahwa ulasan-ulasan tersebut cenderung positif, negatif, atau netral terhadap suatu aplikasi. Analisis sentimen dapat dilakukan dengan menggunakan berbagai teknik pemrosesan bahasa alami dan alat analisis data untuk mengidentifikasi pola-pola dalam teks ulasan, seperti kata-kata kunci, nada, atau konteks kalimat, yang mengindikasikan evaluasi positif atau negatif.

Proses ini penting bagi pengembang aplikasi untuk memahami umpan balik pengguna mereka, mengidentifikasi area-area yang perlu diperbaiki, dan menanggapi masalah-masalah yang mungkin muncul. Dengan memahami sentimen pengguna, pengembang dapat mengambil tindakan untuk meningkatkan kualitas dan kepuasan terhadap aplikasi mereka.

Analisis sentimen adalah salah satu cabang penting dalam pemrosesan bahasa alami yang memungkinkan kita untuk secara otomatis mengidentifikasi apakah ulasan pengguna cenderung positif atau negatif. Dengan demikian, proyek ini akan membantu kita dalam memahami umpan balik pengguna dengan lebih baik, dan dapat digunakan untuk mengambil keputusan yang lebih baik dalam pengembangan dan perbaikan aplikasi.

Dalam perjalanan ini, kita akan menggali konsep-konsep penting dalam analisis sentimen, seperti pengolahan teks, ekstraksi fitur, dan penggunaan model machine learning untuk memprediksi sentimen ulasan. Kami akan menggunakan bahasa pemrograman Python dan beberapa pustaka populer seperti NLTK, scikit-learn, dan TensorFlow.

from google_play_scraper import app, reviews, Sort, reviews_all
 
import pandas as pd
pd.options.mode.chained_assignment = None
import numpy as np
seed = 0
np.random.seed(seed)
import matplotlib.pyplot as plt
import seaborn as sns 
 
import datetime as dt 
import re
import string
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
 
from Sastrawi.Stemmer.StemmerFactory import StemmerFactory
from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory
 
from wordcloud import WordCloud 
import nltk 


Scraping Dataset
Scraping data adalah cara untuk mengambil informasi dari halaman web dengan otomatis. Hal ini seperti menyapu (scrape) data dari sebuah situs web, mirip seperti cara Anda mengambil informasi dari buku atau majalah dengan membacanya sekilas.

Ketika ingin menganalisis ulasan atau pendapat orang tentang sebuah aplikasi di Google Play Store, kita bisa menggunakan teknik scraping untuk mengumpulkan ulasan-ulasan tersebut secara otomatis. Ini memungkinkan kita untuk memiliki banyak data yang bisa dianalisis lebih lanjut.

Ketika mengambil data dari Google Play Store atau situs web lainnya, kita harus berhati-hati untuk tidak melanggar peraturan. Kadang-kadang, situs web memiliki aturan yang melarang pengambilan data secara otomatis. Jadi, pastikan untuk membaca dan mengikuti aturan-aturan tersebut.

from google_play_scraper import app, reviews_all, Sort

scrapreview = reviews_all(
    'com.byu.id',          # ID aplikasi
    lang='id',             # Bahasa ulasan (default: 'en')
    country='id',          # Negara (default: 'us')
    sort=Sort.MOST_RELEVANT, # Urutan ulasan (default: Sort.MOST_RELEVANT)
    count=1000             # Jumlah maksimum ulasan yang ingin diambil
)

Kode di atas kita gunakan untuk mengambil semua ulasan dari sebuah aplikasi di Google Play Store dengan ID 'com.byu.id'. Kita menggunakan pustaka google_play_scraper untuk mengakses ulasan dan informasi aplikasi dari Google Play Store. Dalam kode ini, kita menggunakan fungsi reviews_all() untuk mengambil semua ulasan dari aplikasi tersebut. Proses ini mungkin memerlukan beberapa saat untuk menyelesaikan tugasnya, terutama jika ada banyak ulasan yang perlu diambil. 

Anda juga dapat menyesuaikan bahasa ulasan (dalam contoh ini, kita menggunakan bahasa Indonesia) dan negara (dalam contoh ini, kita menggunakan Indonesia) sesuai  dengan kebutuhan. Pengguna juga dapat menentukan jumlah maksimum ulasan yang ingin diambil (dalam contoh ini, 1000 ulasan). Ulasan akan diurutkan berdasarkan relevansi, ini berarti ulasan yang dianggap paling relevan akan ditampilkan terlebih dahulu.

import csv
 
with open('ulasan_aplikasi.csv', mode='w', newline='', encoding='utf-8') as file:
    writer = csv.writer(file)
    writer.writerow(['Review'])  # Menulis header kolom
    for review in scrapreview:
        writer.writerow([review['content']])  # Menulis konten ulasan ke dalam file CSV

Kode ini untuk menyimpan ulasan sebuah aplikasi dalam file CSV. Pertama, kita membuka file CSV baru untuk penulisan. Kemudian, kita menambahkan header kolom "Review". Selanjutnya, kita iterasi melalui setiap ulasan dalam variabel scrapreview dan menulis konten ulasan dalam file CSV. Jadi, file CSV akan berisi semua ulasan aplikasi dengan struktur yang sesuai.

Setelah menjalankan kode tersebut, dataset ulasan aplikasi akan muncul di bagian kiri Google Colab Anda. Anda dapat mengklik pada nama file 'ulasan_aplikasi.csv' pada panel file untuk melihatnya atau melakukan operasi lain, seperti mengunduhnya atau mengunggahnya ke penyimpanan cloud.

app_reviews_df.to_csv('ulasan_aplikasi.csv', index=False)
app_reviews_df = pd.DataFrame(scrapreview)

Kode ini untuk mengambil ulasan aplikasi menggunakan scrapreview dan menyimpannya dalam sebuah DataFrame. Setelah itu, kita menampilkan jumlah baris dan kolom dalam dataset serta lima baris pertama untuk tinjauan cepat. Akhirnya, dataset disimpan dengan format CSV dan nama 'ulasan_aplikasi.csv' untuk digunakan dalam analisis selanjutnya.

Info dataset ini memberikan gambaran komprehensif tentang struktur dan konten dari DataFrame app_reviews_df dengan total 199 baris dan 11 kolom. Kolom-kolom tersebut meliputi reviewId, userName, userImage, content, score, thumbsUpCount, reviewCreatedVersion, at, replyContent, repliedAt, dan appVersion. Jenis data dari kolom-kolom ini bervariasi, termasuk tipe data objek (string), integer, dan datetime.

Ada beberapa kolom dengan nilai null, khususnya replyContent dan repliedAt, yang menunjukkan bahwa tidak semua ulasan memiliki balasan dari pengembang aplikasi. Dengan demikian, info dataset ini memberikan pemahaman yang baik tentang struktur dan keberadaan data sehingga memudahkan kita dalam analisis selanjutnya.

clean_df = app_reviews_df.dropna()

Kode ini membuat DataFrame baru disebut clean_df dari DataFrame app_reviews_df dengan menghapus baris dengan nilai yang hilang (NaN). Metode dropna() digunakan untuk menghapus baris dengan setidaknya satu nilai yang hilang. Dengan menggunakan langkah ini, kita dapat membersihkan dataset dari baris yang tidak lengkap sehingga memastikan keberlangsungan analisis data untuk lebih konsisten.



Preprocessing Text;
Langkah-langkah preprocessing ini bertujuan menghilangkan noise, mengonversi teks ke format yang konsisten, serta mengekstraksi fitur-fitur penting untuk analisis lebih lanjut.

Fungsi-fungsi yang disediakan akan sangat berguna untuk langkah-langkah preprocessing teks. Mari kita jelaskan masing-masing fungsi dengan lebih singkat.


import re
import string
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from Sastrawi.Stemmer.Factory import StemmerFactory


1. cleaningText(text): Membersihkan teks dengan menghapus mention, hashtag, RT (retweet), tautan (link), angka, dan tanda baca. Selain itu, karakter newline diganti dengan spasi dan spasi ekstra pada awal dan akhir teks dihapus.

def cleaningText(text):
    text = re.sub(r'@[A-Za-z0-9]+', '', text) # menghapus mention
    text = re.sub(r'#[A-Za-z0-9]+', '', text) # menghapus hashtag
    text = re.sub(r'RT[\s]', '', text) # menghapus RT
    text = re.sub(r"http\S+", '', text) # menghapus link
    text = re.sub(r'[0-9]+', '', text) # menghapus angka
    text = re.sub(r'[^\w\s]', '', text) # menghapus karakter selain huruf dan angka
 
    text = text.replace('\n', ' ') # mengganti baris baru dengan spasi
    text = text.translate(str.maketrans('', '', string.punctuation)) # menghapus semua tanda baca
    text = text.strip(' ') # menghapus karakter spasi dari kiri dan kanan teks
    return text

2. casefoldingText(text): Mengonversi semua karakter dalam teks menjadi huruf kecil (lowercase) untuk membuat teks menjadi seragam.

def casefoldingText(text): # Mengubah semua karakter dalam teks menjadi huruf kecil
    text = text.lower()
    return text


3. tokenizingText(text): Memecah teks menjadi daftar kata atau token. Ini membantu dalam mengurai teks menjadi komponen-komponen dasar untuk analisis lebih lanjut.

def tokenizingText(text): # Memecah atau membagi string, teks menjadi daftar token
    text = word_tokenize(text)
    return text

4. filteringText(text): Menghapus kata-kata berhenti (stopwords) dalam teks. Daftar kata-kata berhenti telah diperbarui dengan beberapa kata tambahan.

def filteringText(text): # Menghapus stopwords dalam teks
    listStopwords = set(stopwords.words('indonesian'))
    listStopwords1 = set(stopwords.words('english'))
    listStopwords.update(listStopwords1)
    listStopwords.update(['iya','yaa','gak','nya','na','sih','ku',"di","ga","ya","gaa","loh","kah","woi","woii","woy"])
    filtered = []
    for txt in text:
        if txt not in listStopwords:
            filtered.append(txt)
    text = filtered
    return text

5. stemmingText(text): Menerapkan stemming pada teks, yakni mengurangi kata-kata menjadi bentuk dasarnya. Anda menggunakan pustaka Sastrawi untuk melakukan stemming dalam bahasa Indonesia.

def stemmingText(text): # Mengurangi kata ke bentuk dasarnya yang menghilangkan imbuhan awalan dan akhiran atau ke akar kata
    # Membuat objek stemmer
    factory = StemmerFactory()
    stemmer = factory.create_stemmer()
 
    # Memecah teks menjadi daftar kata
    words = text.split()
 
    # Menerapkan stemming pada setiap kata dalam daftar
    stemmed_words = [stemmer.stem(word) for word in words]
 
    # Menggabungkan kata-kata yang telah distem
    stemmed_text = ' '.join(stemmed_words)
 
    return stemmed_text

6. toSentence(list_words): Menggabungkan daftar kata-kata menjadi sebuah kalimat.

def toSentence(list_words): # Mengubah daftar kata menjadi kalimat
    sentence = ' '.join(word for word in list_words)
    return sentence

Dengan menggunakan fungsi-fungsi ini, Anda dapat membersihkan, memproses, dan mempersiapkan teks sebelum melakukan analisis sentimen atau tugas analisis teks lainnya. Pastikan untuk memanggil fungsi-fungsi ini dengan benar sesuai dengan langkah-langkah preprocessing teks yang Anda butuhkan dalam proyek Anda.

Selanjutnya adalah penghapus kumpulan slang words atau kata-kata informal yang sering digunakan dalam percakapan sehari-hari, terutama pada media sosial atau obrolan online. Setiap kata slang memiliki padanan atau substitusi dengan kata formal atau baku. Misalnya, "abis" merupakan singkatan dari "habis", "wtb" merupakan singkatan dari "beli", dan seterusnya. Kamus ini berguna untuk membantu pemahaman dan interpretasi teks yang menggunakan bahasa informal atau slang.

slangwords = {"@": "di", "abis": "habis", "wtb": "beli", "masi": "masih", "wts": "jual", "wtt": "tukar", "bgt": "banget", "maks": "maksimal" …}
def fix_slangwords(text):
    words = text.split()
    fixed_words = []
 
    for word in words:
        if word.lower() in slangwords:
            fixed_words.append(slangwords[word.lower()])
        else:
            fixed_words.append(word)
 
    fixed_text = ' '.join(fixed_words)
    return fixed_text


Setelah semua langkah preprocessing telah ditetapkan, langkah berikutnya adalah menerapkannya.


clean_df['text_tokenizingText'] = clean_df['text_slangwords'].apply(tokenizingText)

....



Pelabelan:
Sebelum masuk ke tahap pemodelan, langkah yang dilakukan adalah pelabelan. Pelabelan adalah proses pemberian kategori atau label pada setiap entri data berdasarkan informasi yang tersedia. Dalam konteks ini, setiap entri dataset diberikan label sentimen berdasarkan analisis teksnya. Dengan demikian, tahapan pelabelan menjadi dasar untuk proses selanjutnya dalam membangun model klasifikasi sentimen.

from io import StringIO
 
# Membaca data kamus kata-kata positif dari GitHub
lexicon_positive = dict()
 
response = requests.get('https://raw.githubusercontent.com/angelmetanosaa/dataset/main/lexicon_positive.csv')
# Mengirim permintaan HTTP untuk mendapatkan file CSV dari GitHub
 
if response.status_code == 200:
    # Jika permintaan berhasil
    reader = csv.reader(StringIO(response.text), delimiter=',')
    # Membaca teks respons sebagai file CSV menggunakan pembaca CSV dengan pemisah koma
 
    for row in reader:
        # Mengulangi setiap baris dalam file CSV
        lexicon_positive[row[0]] = int(row[1])
        # Menambahkan kata-kata positif dan skornya ke dalam kamus lexicon_positive
else:
    print("Failed to fetch positive lexicon data")
 
# Membaca data kamus kata-kata negatif dari GitHub
lexicon_negative = dict()
 
response = requests.get('https://raw.githubusercontent.com/angelmetanosaa/dataset/main/lexicon_negative.csv')
# Mengirim permintaan HTTP untuk mendapatkan file CSV dari GitHub
 
if response.status_code == 200:
    # Jika permintaan berhasil
    reader = csv.reader(StringIO(response.text), delimiter=',')
    # Membaca teks respons sebagai file CSV menggunakan pembaca CSV dengan pemisah koma
 
    for row in reader:
        # Mengulangi setiap baris dalam file CSV
        lexicon_negative[row[0]] = int(row[1])
        # Menambahkan kata-kata negatif dan skornya dalam kamus lexicon_negative
else:
    print("Failed to fetch negative lexicon data")



Kode di atas untuk melakukan analisis sentimen pada teks berbahasa Indonesia menggunakan kamus kata-kata positif dan negatif. Pertama, kita mengirim permintaan HTTP untuk mengambil data kamus kata-kata positif dan negatif dari GitHub. 

Jika permintaan berhasil, kita membaca teks respons sebagai file CSV menggunakan pembaca CSV dengan pemisah koma. Kemudian, setiap baris dalam file CSV dibaca dan kata-kata beserta skornya dimasukkan ke kamus yang sesuai (lexicon_positive untuk kata-kata positif dan lexicon_negative untuk kata-kata negatif).

def sentiment_analysis_lexicon_indonesia(text):
    #for word in text:
 
    score = 0
    # Inisialisasi skor sentimen ke 0
 
    for word in text:
        # Mengulangi setiap kata dalam teks
 
        if (word in lexicon_positive):
            score = score + lexicon_positive[word]
            # Jika kata ada dalam kamus positif, tambahkan skornya ke skor sentimen
 
    for word in text:
        # Mengulangi setiap kata dalam teks (sekali lagi)
 
        if (word in lexicon_negative):
            score = score + lexicon_negative[word]
            # Jika kata ada dalam kamus negatif, kurangkan skornya dari skor sentimen
 
    polarity=''
    # Inisialisasi variabel polaritas
 
    if (score >= 0):
        polarity = 'positive'
        # Jika skor sentimen lebih besar atau sama dengan 0, maka polaritas adalah positif
    elif (score < 0):
        polarity = 'negative'
        # Jika skor sentimen kurang dari 0, maka polaritas adalah negatif
 
    # else:
    #     polarity = 'neutral'
    # Ini adalah bagian yang bisa digunakan untuk menentukan polaritas netral jika diperlukan
 
    return score, polarity
    # Mengembalikan skor sentimen dan polaritas teks

Setelah kamus-kamus tersebut terisi, kita mendefinisikan fungsi sentiment_analysis_lexicon_indonesia yang akan menerima teks sebagai input. Dalam fungsi ini, setiap kata pada teks akan diperiksa, ada dalam kamus positif atau negatif. Jika ada, skor sentimen akan ditambahkan atau dikurangkan sesuai dengan skor kata tersebut dalam kamus. Setelah semua kata diperiksa, skor sentimen akan digunakan untuk menentukan polaritas teks, positif atau negatif. 

results = clean_df['text_stopword'].apply(sentiment_analysis_lexicon_indonesia)
results = list(zip(*results))
clean_df['polarity_score'] = results[0]
clean_df['polarity'] = results[1]
print(clean_df['polarity'].value_counts())

Kemudian, skor sentimen dan polaritas dipisahkan dari hasil tersebut menggunakan fungsi zip serta disimpan dalam kolom 'polarity_score' dan 'polarity' secara berturut-turut. Terakhir, jumlah kemunculan setiap polaritas sentimen dicetak dengan value_counts() untuk mengetahui distribusinya.

Pada DataFrame clean_df, ada 423 teks dengan polaritas negatif dan 173 teks dengan polaritas positif. Nah, setelah mendapatkan jumlah pasti dari penyebaran label dataset, selanjutnya kita akan melakukan eksplorasi jumlahnya. Dari hasil analisis, 71% dari teks memiliki polaritas negatif, sementara 29% memiliki polaritas positif. 


Eksplorasi Label:
Teman-teman, bagaimana pendapat Anda terkait visualisasi berikut? Berikut adalah visualisasi dari kata-kata yang sering muncul pada dataset review aplikasi By.U. Visualisasi ini menggunakan WorldCloud. WordCloud adalah representasi visual dari kata-kata yang muncul dalam teks, ketika ukurannya menunjukkan seberapa sering kata tersebut muncul. 

Nah, bagaimana jika Anda menulis pendapat dan analisis pada Forum Diskusi? Mungkin saja kita bisa melihat persepsi teman-teman yang lainnya. Kue ape kue cucur, meluncuuurr!

Di bawah ini, ada tiga visualisasi, yaitu (1) WordCloud secara general; (2) WordCloud untuk Positive Tweets Data; dan (3) Word Cloud untuk Negative Tweets Data. Berikan analisis Anda terhadap tiga gambar tersebut secara lengkap dan komprehensif! Kami tunggu di Forum Diskusi, ya. ????


Data Splitting dan Ekstraksi Fitur dengan TF-IDF
Nah, setelah berhasil menetapkan label untuk setiap entri dalam dataset, sekarang kita dapat melangkah ke tahap berikutnya, yaitu ekstraksi fitur dan pemisahan data. Apakah teman-teman masih ingat tentang ekstraksi fitur yang telah kita pelajari sebelumnya? Pada contoh kali ini, kita akan menggunakan metode ekstraksi fitur yang disebut TF-IDF.


X = clean_df['text_akhir']
y = clean_df['polarity']
 
# Ekstraksi fitur dengan TF-IDF
tfidf = TfidfVectorizer(max_features=200, min_df=17, max_df=0.8 )
X_tfidf = tfidf.fit_transform(X)
 
# Konversi hasil ekstraksi fitur menjadi dataframe
features_df = pd.DataFrame(X_tfidf.toarray(), columns=tfidf.get_feature_names_out())
 
# Menampilkan hasil ekstraksi fitur
features_df
 
# Bagi data menjadi data latih dan data uji
X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.2, random_state=42)


Di sini, data dipisahkan menjadi fitur (tweet) dan label (sentimen). Fitur tweet direpresentasikan oleh variabel X dan label sentimen direpresentasikan oleh variabel y. Kemudian, fitur tersebut diekstraksi menggunakan metode TF-IDF.

Pengaturan spesifik untuk TF-IDF termasuk jumlah fitur maksimum (max_features), frekuensi minimum kata (min_df), dan maksimum dokumen yang memuat kata tersebut (max_df). Hasil ekstraksi fitur kemudian dikonversi menjadi DataFrame untuk memudahkan analisis lebih lanjut. 

Selanjutnya, data dibagi menjadi data latih dan data uji menggunakan train_test_split dengan proporsi data latih sebesar 80% serta data uji sebesar 20%. Tahapan ini akan memungkinkan kita untuk melatih dan menguji model klasifikasi sentimen menggunakan fitur-fitur yang telah diekstraksi.


Modeling:
Setelah proses ekstraksi fitur dan pemisahan data, tahapan terakhir adalah melakukan pemodelan. Pada contoh ini, kita menggunakan empat algoritma yang berbeda: Naive Bayes (NB), Random Forest (RF), Logistic Regression (LR), dan Decision Tree (DT). 

from sklearn.ensemble import RandomForestClassifier
 
# Membuat objek model Random Forest
random_forest = RandomForestClassifier()
 
# Melatih model Random Forest pada data pelatihan
random_forest.fit(X_train.toarray(), y_train)
 
# Prediksi sentimen pada data pelatihan dan data uji
y_pred_train_rf = random_forest.predict(X_train.toarray())
y_pred_test_rf = random_forest.predict(X_test.toarray())
 
# Evaluasi akurasi model Random Forest
accuracy_train_rf = accuracy_score(y_pred_train_rf, y_train)
accuracy_test_rf = accuracy_score(y_pred_test_rf, y_test)
 
# Menampilkan akurasi
print('Random Forest - accuracy_train:', accuracy_train_rf)
print('Random Forest - accuracy_test:', accuracy_test_rf)

Setelah melatih model dengan data latih, kita mengukur akurasinya menggunakan data uji. Hasilnya berikut: Naive Bayes memiliki akurasi sebesar 83,33%, Logistic Regression sebesar 82,50%, Random Forest sebesar 81,67%, dan Decision Tree sebesar 73,33%. Dengan demikian, kita dapat menentukan bahwa Naive Bayes adalah model terbaik dalam kasus ini, diikuti oleh Logistic Regression, Random Forest, dan Decision Tree.



Rangkuman:

Gated Recurrent Unit (GRU)

GRU adalah varian RNN yang lebih sederhana dan efisien secara komputasi dibandingkan LSTM, dengan hanya dua gate (update gate dan reset gate).

Komponen GRU:
- Reset Gate: Mengontrol seberapa banyak informasi dari status tersembunyi sebelumnya yang akan disimpan.
- Update Gate: Mengontrol seberapa banyak informasi baru yang akan ditambahkan ke hidden state.


Gunakan sudut pandang seorang yang mahir dan berpengalaman dalam NLP. Berikut adalah yang bukan tantangan dalam pembangunan implementasi NLP, yaitu?
a. Ketergantungan Bahasa (benar)

Apa nama kelas stemmer NLTK yang umumnya dipilih untuk melakukan stemming dalam bahasa selain Bahasa Inggris
a. Rootstemmer
b. Snowballstemmer (betul)
c. linguistikstemmer
d. basestemmer

Berikut adalah yang bukan contoh tentang klasifikasi teks.
A. Mendeteksi bahasa dari sebuah teks.
B. Menerjemahkan sebuah kalimat dari Bahasa Inggris ke Bahasa Italia.
C. Mengklasifikasikan sebuah tweet sebagai mengandung atau tidak mengandung teks bully.
D. Memberi label kepada isu Git sebagai "Usulan Peningkatan". (salah)


Bagaimana "Named Entity Recognition" (NER) dapat digunakan dalam aplikasi dunia nya
a. Untuk mengidentifikasi dan mengekstraksi informasi penting seperti nama orang, tempat, atau organisasi dari teks.

Apa perbedaan utama antara "stemming" dan "lemmatization" dalam NLP?

A. Stemming mengubah kata ke bentuk dasarnya, sedangkan lemmatization mempertahankan akar kata yang sebenarnya.
B. Stemming lebih kompleks daripada lemmatization.
C. Stemming mempertahankan makna kata, sedangkan lemmatization mengubah kata ke bentuk dasarnya. (salah)
D. Stemming hanya berlaku untuk kata-kata dalam bahasa Inggris, sedangkan lemmatization berlaku untuk semua bahasa.



---------------------------------------------------------------------------
                        Proyek Analisis Sentimen
---------------------------------------------------------------------------

1. Topiknya Analisis Sentimen Tapi Bebas Studi Kasusnya (Lihat HuggingFace)

Kriteria 1: Data merupakan hasil scraping secara mandiri

Anda diberi kebebasan untuk mengambil data atau scraping menggunakan bahasa pemrograman Python dari berbagai sumber, seperti platform PlayStore, X, Instagram, komentar pada penilaian barang di e-commerce, dan lain-lain. Jumlah dataset minimal yang harus diperoleh adalah 3.000 sampel. 


Kriteria 2: Melakukan tahapan ekstraksi fitur dan pelabelan data

Metode yang digunakan bebas sesuai dengan preferensi masing-masing peserta. Tahapan ini penting untuk mempersiapkan data sehingga dapat diolah lebih lanjut dalam proses pelatihan model.


Kriteria 3: Menggunakan algoritma pelatihan machine learning

Pilihan algoritma pelatihan ini haruslah sesuai dengan tujuan analisis sentimen yang ingin dicapai.


Kriteria 4: Akurasi testing set yang didapatkan untuk setiap skema pelatihan minimal harus mencapai 85%

Hal ini menunjukkan bahwa model yang dikembangkan memiliki kinerja yang baik dalam mengklasifikasikan sentimen dari data yang 4diberikan.
Dengan memperhatikan semua kriteria ini, Anda diharapkan dapat menghasilkan model analisis sentimen yang berkualitas tinggi dan bisa dipertanggungjawabkan.



Penilaian:
1. Menggunakan algoritma deep learning di luar dari contoh latihan (Selain LSTM, GRU, RNN?)
2. Akurasi pada training set dan testing set di atas 92%. 
3. Dataset memiliki minimal tiga kelas.
4. Memiliki jumlah data minimal 10.000 sampel data.
5. Melakukan 3 percobaan skema pelatihan yang berbeda. Skema ini dapat dibedakan dari variasi algoritma pelatihan, metode ekstraksi fitur, pelabelan, dan pembagian data dengan memilih minimal 2 kombinasi.
6. Melakukan inference atau testing dalam file .ipynb atau .py yang menghasilkan output berupa kelas kategorikal (contoh: negatif, netral, dan positif).


⭐⭐⭐ = Semua kriteria utama terpenuhi, tetapi tidak terdapat saran yang terpenuhi.

⭐⭐⭐⭐ = Semua kriteria utama terpenuhi dan menerapkan minimal 3 dari seluruh saran yang ada di atas.

⭐⭐⭐⭐⭐ = Semua kriteria utama terpenuhi dan menerapkan semua saran yang ada di atas.





---------------------------------------------------------------------------
                        Materi Materi Materi Materi
---------------------------------------------------------------------------

1. Pada modul Latihan Pra-Pemrosesan Teks sepertinya output yang didapat pada bagian Tokenizing khususnya Word tokenization dan Sentence tokenization belum sesuai dengan apa yang ada pada code kak. 

2. Juga pada bagian Ekstraksi Fitur pada saat menjelaskan GloVe itu sepertinya belum selesai dijelaskan seluruhnya. Karena pada kalimat terakhir 'GloVe cenderung lebih fokus pada kem??' 

3. Terus kak kenapa pada beberapa materi ada tanda tanya seperti ini ???? (????)

---------------------------------------------------------------------------
                        Materi Materi Materi Materi
---------------------------------------------------------------------------



---------------------------------------------------------------------------
                        Materi Materi Materi Materi
---------------------------------------------------------------------------


? = pertanyaan
* / ** = penting